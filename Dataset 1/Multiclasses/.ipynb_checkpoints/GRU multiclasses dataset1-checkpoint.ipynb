{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.011651,
     "end_time": "2021-08-22T04:42:25.359283",
     "exception": false,
     "start_time": "2021-08-22T04:42:25.347632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "path0 ='../'\n",
    "df = pd.read_csv(path0+\"Dataset_S022Final.csv\")\n",
    "\n",
    "a=time.time()\n",
    "df=df[['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','packet_type','droppedPKWrongPort','sentPK','size','channel','DataQueueLen','passedUpPk','rcvdPKFromHL','rcvdPKFromLL','sentDownPK','DropPKByQueue','snir','throughput','label']]         \n",
    "\n",
    "\n",
    "df_Normal=df[0:76064].copy()\n",
    "#print(df_Normal)\n",
    "df_UDP=df[76064:214037].copy()\n",
    "#print(df_UDP)\n",
    "df_pluies=df[214037:426866].copy()\n",
    "\n",
    "#print(df_Normal2)\n",
    "df_jam=df[462481:-1].copy()\n",
    "\n",
    "X=df_Normal.drop(columns = ['label']).copy()\n",
    "y=df_Normal[['label']].copy()\n",
    "#print(y.columns)\n",
    "X1=df_UDP.drop(columns = ['label']).copy()\n",
    "y1=df_UDP[['label']].copy()\n",
    "X2=df_pluies.drop(columns = ['label']).copy()\n",
    "y2=df_pluies[['label']].copy()\n",
    "X3=df_jam.drop(columns = ['label']).copy()\n",
    "y3=df_jam[['label']].copy()\n",
    "\n",
    "train=0.4\n",
    "\n",
    "\n",
    "xcol=X.columns\n",
    "ycol=y.columns\n",
    "\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=train,shuffle=False)\n",
    "X_train1, X_rem1, y_train1, y_rem1 = train_test_split(X1,y1, train_size=train,shuffle=False)\n",
    "X_train2, X_rem2, y_train2, y_rem2 = train_test_split(X2,y2, train_size=train,shuffle=False)\n",
    "X_train3, X_rem3, y_train3, y_rem3 = train_test_split(X3,y3, train_size=train,shuffle=False)\n",
    "\n",
    "validation=0.5\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5,shuffle=False)\n",
    "X_valid1, X_test1, y_valid1, y_test1 = train_test_split(X_rem1,y_rem1, test_size=0.5,shuffle=False)\n",
    "X_valid2, X_test2, y_valid2, y_test2 = train_test_split(X_rem2,y_rem2, test_size=0.5,shuffle=False)\n",
    "X_valid3, X_test3, y_valid3, y_test3 = train_test_split(X_rem3,y_rem3, test_size=0.5,shuffle=False)\n",
    "\n",
    "\n",
    "X_train=np.concatenate((X_train, X_train1, X_train2,X_train3))\n",
    "X_valid=np.concatenate((X_valid, X_valid1, X_valid2,X_valid3))\n",
    "X_test=np.concatenate((X_test, X_test1, X_test2,X_test3))\n",
    "\n",
    "y_train=np.concatenate((y_train, y_train1, y_train2,y_train3))\n",
    "y_valid=np.concatenate((y_valid, y_valid1, y_valid2,y_valid3))\n",
    "y_test=np.concatenate((y_test, y_test1, y_test2,y_test3))\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns= xcol)\n",
    "X_valid = pd.DataFrame(X_valid, columns= xcol)\n",
    "X_test = pd.DataFrame(X_test, columns= xcol)\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns= ycol)\n",
    "y_valid = pd.DataFrame(y_valid, columns= ycol)\n",
    "y_test = pd.DataFrame(y_test, columns= ycol)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a-time.time()\n",
    "X_train1=X_train.values\n",
    "y_train1=y_train.values\n",
    "#X_train1=X_train1.astype(float)\n",
    "\n",
    "torch_tensor = torch.tensor(X_train1)\n",
    "#torch_tensor[torch_tensor != torch_tensor]=float(0)\n",
    "t=[]\n",
    "test=0\n",
    "test0=0\n",
    "\n",
    "for j in y_train1:\n",
    "    i=j[0]\n",
    "#    print(i)\n",
    "    if (i=='Normal'):\n",
    "        t.append(int(0))\n",
    "        test0+=1\n",
    "    #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        t.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        t.append(int(2))\n",
    "        test+=1\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        t.append(int(3))\n",
    "\n",
    "labels = torch.LongTensor(t)\n",
    "\n",
    "bigX = X_valid\n",
    "#[['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','droppedPKWrongPort','sentPK','size','throughput']]  ,'channel'       \n",
    "bigY = y_valid['label']\n",
    "#print(bigY)\n",
    "X_valid1=bigX.values\n",
    "y_valid1=bigY.values\n",
    "\n",
    "vtorch_tensor = torch.tensor(X_valid1)\n",
    "v=[]\n",
    "\n",
    "for i in y_valid1:\n",
    "    #i=j[0]\n",
    "    #print(i)\n",
    "    if (i=='Normal'):\n",
    "        v.append(int(0))\n",
    "        #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        v.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        v.append(int(2))\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        v.append(int(3))\n",
    "\n",
    "vlabels = torch.LongTensor(v)\n",
    "\n",
    "\n",
    "X_test1=X_test.values\n",
    "y_test1=y_test.values\n",
    "\n",
    "#X_train1=X_train1.astype(float)\n",
    "\n",
    "ttorch_tensor = torch.tensor(X_test1)\n",
    "t=[]\n",
    "\n",
    "for j in y_test1:\n",
    "    i=j[0]\n",
    "#    print(i)\n",
    "    if (i=='Normal'):\n",
    "        t.append(int(0))\n",
    "        #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        t.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        t.append(int(2))\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        t.append(int(3))\n",
    "\n",
    "tlabels = torch.LongTensor(t)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T04:42:35.870279Z",
     "iopub.status.busy": "2021-08-22T04:42:35.869400Z",
     "iopub.status.idle": "2021-08-22T04:42:35.900807Z",
     "shell.execute_reply": "2021-08-22T04:42:35.900210Z",
     "shell.execute_reply.started": "2021-08-21T21:19:30.98512Z"
    },
    "papermill": {
     "duration": 0.052161,
     "end_time": "2021-08-22T04:42:35.900949",
     "exception": false,
     "start_time": "2021-08-22T04:42:35.848788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "# input_size = 784 # 28x28\n",
    "num_classes = 4\n",
    "#num_epochs = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "input_size = 19\n",
    "sequence_length = 28\n",
    "hidden_size = 132\n",
    "num_layers = 2\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers,batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        x=x.view(-1,1,19)\n",
    "        #print(x.shape)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        #out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        #out = self.fc(out)\n",
    "        x = F.log_softmax(self.fc(out), dim=1)\n",
    "        # out: (n, 10)\n",
    "        return x\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "    \n",
    "Fonction_de_perte = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "model.load_state_dict(torch.load('models\\\\dataset1_1_50_GRU.pth'))\n",
    "\n",
    "#indices = [0,3, 299:303]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T04:42:36.014146Z",
     "iopub.status.busy": "2021-08-22T04:42:36.013426Z",
     "iopub.status.idle": "2021-08-22T10:00:17.179402Z",
     "shell.execute_reply": "2021-08-22T10:00:17.180001Z"
    },
    "papermill": {
     "duration": 19061.198051,
     "end_time": "2021-08-22T10:00:17.180439",
     "exception": false,
     "start_time": "2021-08-22T04:42:35.982388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#labels1=labels[loc]\n",
    "a=time.time()\n",
    "inputs=torch_tensor[:,:] \n",
    "labels1=labels[:]\n",
    "print(len(inputs))\n",
    "print(len(labels1))\n",
    "vinputs =vtorch_tensor[:,:] \n",
    "vlabels1=vlabels[:]\n",
    "\n",
    "epochs = 50\n",
    "valid_loss=0\n",
    "accuracy=0\n",
    "valid_loss_min=np.Inf\n",
    "\n",
    "b=time.time()\n",
    "with torch.no_grad():\n",
    "    output = torch.exp(model(vinputs))\n",
    "    valid_loss=Fonction_de_perte(output, vlabels1)\n",
    "top_p , top_c = output.topk(1, dim=1)\n",
    "propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "equals = top_c==vlabels1.view(*top_c.shape)\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "#print ('valid accuracy :{0:.8f} with total prob : {3:.8f} and  valid loss : {2:.8f} ,  time {1:.6f} '.format(accuracy*100 , time.time()-b ,test_loss,propabilities))\n",
    "print ('Epoch  : valid accuracy :{0:.8f} with total prob : {1:.8f} '.format(accuracy*100  ,propabilities))\n",
    "\n",
    "####\n",
    "#print(inputs.shape[0])\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "for e in range(epochs):\n",
    "    c=time.time()\n",
    "    running_loss = 0\n",
    "    for i, x in enumerate(inputs):\n",
    "        optimizer.zero_grad()\n",
    "        x2=x[None,:]\n",
    "        output = model.forward(x2)\n",
    "        l2=labels1[i][None]\n",
    "        #print(l2)\n",
    "        loss = Fonction_de_perte(output, l2)\n",
    "      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "        accuracy=0\n",
    "        #print(l2)\n",
    "  \n",
    "        b=time.time()\n",
    "        with torch.no_grad():\n",
    "            ##\n",
    "            #with torch.no_grad():\n",
    "            #for i, x in enumerate(vinputs):\n",
    "             #   x2=x[None,:]\n",
    "              #  output = torch.exp(model(x2))\n",
    "               # l2=vlabels1[i][None]\n",
    "                #valid_loss1+=Fonction_de_perte(output, l2)\n",
    "            #test_loss=test_loss/len(vinputs)\n",
    "            ##\n",
    "                \n",
    "            model.eval()\n",
    "            output = torch.exp(model(vinputs))\n",
    "            valid_loss=Fonction_de_perte(output, vlabels1)\n",
    "        top_p , top_c = output.topk(1, dim=1)\n",
    "        propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "        equals = top_c==vlabels1.view(*top_c.shape)\n",
    "        accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        print('Epoch {2} : Training loss {0:.8f} and valid loss : {1:.8f} :  '.format(running_loss/len(inputs),valid_loss, e))\n",
    "   \n",
    "        print ('Epoch {2} : valid accuracy :{0:.8f} with total prob : {1:.8f} '.format(accuracy*100  ,propabilities, e))\n",
    "        print(time.time()-c)\n",
    "        #print('validation loss with iterations {0}'.format(valid_loss1/len(vinputs) ) )   \n",
    "        train_losses.append(running_loss/len(inputs))\n",
    "        valid_losses.append(valid_loss)\n",
    "        if (valid_loss<valid_loss_min):\n",
    "            print('validation loss decreased , saving model ({:.8f} ==> {:.8f})'.format(valid_loss_min,valid_loss))\n",
    "            torch.save(model.state_dict(),'dataset1_1_50_GRU.pth')\n",
    "            valid_loss_min=valid_loss\n",
    "print(time.time()-a)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "papermill": {
     "duration": 0.027207,
     "end_time": "2021-08-22T10:00:17.235480",
     "exception": false,
     "start_time": "2021-08-22T10:00:17.208273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "Tr=np.array(train_losses)\n",
    "Te=np.array(valid_losses)\n",
    "\n",
    "Tr=np.reshape(Tr, (len(Tr),1))\n",
    "Te=np.reshape(Te, (len(Te),1))\n",
    "\n",
    "# fit on training data column\n",
    "scale = StandardScaler().fit(Tr)\n",
    "tain_losses = scale.transform(Tr)\n",
    "scale = StandardScaler().fit(Te)\n",
    "test_losses = scale.transform(Te)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "#test_losses_scaled = scaler.fit_transform(test_losses)\n",
    "plt.ylim([-1,1])\n",
    "\n",
    "plt.plot(tain_losses, label='Train loss')\n",
    "plt.plot(test_losses, label='valid loss')\n",
    "plt.legend(frameon=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T10:00:17.311175Z",
     "iopub.status.busy": "2021-08-22T10:00:17.305891Z",
     "iopub.status.idle": "2021-08-22T10:02:27.288894Z",
     "shell.execute_reply": "2021-08-22T10:02:27.289379Z"
    },
    "papermill": {
     "duration": 130.026259,
     "end_time": "2021-08-22T10:02:27.289557",
     "exception": false,
     "start_time": "2021-08-22T10:00:17.263298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "test accuracy :98.69104767 with total prob : 99.81935883 and  test loss : -0.98680211 ,  time 2.00548649 \n",
      "la précision de detection globale: 98.68021137313796 \n",
      "detection des communication normal: 97.69914301 with accuracy 97.67856480 ( 78894/80769 )\n",
      "details normal classed udp :0.03343, pluies 2.24467 , jam 0.04333 (ou 1.44000,96.69333,1.86667) \n",
      "detection du deni de service par udp flood 99.99936151 with accuracy 100.00000000 ( 35775/35775 )\n",
      "detection udp flood 100\n",
      "detection du deni naturel : pluies et orages : 99.86152915 with accuracy 100.00000000 ( 22793/22793 )\n",
      "detection pluies et orages 100\n",
      "detection du deni naturel : jam : 99.99107348 with accuracy 100.00000000 ( 3907/3907 )\n",
      "detection brouillage 100\n",
      "=====================\n",
      "<================================>\n",
      "Total_time\n",
      "84.26630687713623\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "a=time.time()\n",
    "summm=[]\n",
    "sum1=[]\n",
    "sum2=[]\n",
    "sum3=[]\n",
    "sum4=[]\n",
    "\n",
    "tinputs =ttorch_tensor[:,:] \n",
    "tlabels1=tlabels[:]\n",
    "\n",
    "\n",
    "#tinputs =torch_tensor[:,:] \n",
    "#tlabels1=labels[:]\n",
    "\n",
    "print('=====================')\n",
    "b=time.time()\n",
    "output = torch.exp(model(tinputs))\n",
    "test_loss=Fonction_de_perte(output, tlabels1)\n",
    "top_p , top_c = output.topk(1, dim=1)\n",
    "propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "equals = top_c==tlabels1.view(*top_c.shape)\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "model.train()\n",
    "print ('test accuracy :{0:.8f} with total prob : {3:.8f} and  test loss : {2:.8f} ,  time {1:.8f} '.format(accuracy*100 , time.time()-b ,test_loss ,propabilities))\n",
    "\n",
    "class_correct = list(0. for i in range (4))\n",
    "class_total = list(0. for i in range (4))\n",
    "\n",
    "C_n_udp=0\n",
    "C_n_pluies=0\n",
    "C_n_jam=0\n",
    "C_n_total=0\n",
    "\n",
    "\n",
    "C_u_normal=0\n",
    "C_u_jam=0\n",
    "C_u_pluies=0\n",
    "C_u_total=0\n",
    "\n",
    "C_p_normal=0\n",
    "C_p_udp=0\n",
    "C_p_jam=0\n",
    "C_p_total=0\n",
    "\n",
    "C_j_normal=0\n",
    "C_j_udp=0\n",
    "C_j_pluies=0\n",
    "C_j_total=0\n",
    "\n",
    "for i, x in enumerate(tinputs):\n",
    "    optimizer.zero_grad()\n",
    "    x2=x[None,:]\n",
    "    with torch.no_grad():\n",
    "        output = torch.exp(model(x2))\n",
    "    out=output.detach().numpy()*100\n",
    " \n",
    "    l3=tlabels1[i].item()\n",
    "  \n",
    "    if(l3==0):\n",
    "        summm.append(out[0][0])\n",
    "        sum1.append(out[0][0])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_n_udp+=1\n",
    "            if(top_c[i][0]==2):\n",
    "                C_n_pluies+=1\n",
    "            if(top_c[i][0]==3):\n",
    "                C_n_jam +=1\n",
    "            \n",
    "            C_n_total +=1\n",
    "            \n",
    "    if(l3==1):\n",
    "        summm.append(out[0][1])\n",
    "        sum2.append(out[0][1])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==3):\n",
    "                C_u_jam+=1\n",
    "            if(top_c[i][0]==2):\n",
    "                C_u_pluies+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_u_normal +=1\n",
    "            \n",
    "            C_u_total +=1\n",
    "\n",
    "    if(l3==2):\n",
    "        summm.append(out[0][2])\n",
    "        sum3.append(out[0][2])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_p_udp+=1\n",
    "            if(top_c[i][0]==3):\n",
    "                C_p_jam+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_p_normal +=1\n",
    "            \n",
    "            C_p_total +=1\n",
    "\n",
    "    if(l3==3):\n",
    "        summm.append(out[0][3])\n",
    "        sum4.append(out[0][3])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_j_udp+=1\n",
    "            if(top_c[i][0]==2):\n",
    "                C_j_pluies+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_j_normal +=1\n",
    "            \n",
    "            C_j_total +=1\n",
    "\n",
    "    class_total[l3]+=1\n",
    "    class_correct[l3]+=equals[i][0]\n",
    "    #print(output)\n",
    "print('la précision de detection globale: {0} '.format(mean(summm)))\n",
    "print('detection des communication normal: {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum1), class_correct[0]*100/class_total[0] ,class_correct[0] ,class_total[0]))\n",
    "if(C_n_total!=0):\n",
    "    print('details normal classed udp :{0:.5f}, pluies {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_n_udp*100/class_total[0],C_n_pluies*100/class_total[0],C_n_jam*100/class_total[0],C_n_udp*100/C_n_total,C_n_pluies*100/C_n_total,C_n_jam*100/C_n_total))\n",
    "else:\n",
    "    print('detection normal 100')\n",
    "\n",
    "print('detection du deni de service par udp flood {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum2), class_correct[1]*100/class_total[1] ,class_correct[1] ,class_total[1]))\n",
    "if(C_u_total!=0):\n",
    "    print('details udp classed normal :{0:.5f}, pluies {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_u_normal*100/class_total[1],C_u_pluies*100/class_total[1],C_u_jam*100/class_total[1],C_u_normal*100/C_u_total,C_u_pluies*100/C_u_total,C_u_jam*100/C_u_total))\n",
    "else:\n",
    "    print('detection udp flood 100')\n",
    "    \n",
    "print('detection du deni naturel : pluies et orages : {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum3), class_correct[2]*100/class_total[2] ,class_correct[2] ,class_total[2]))\n",
    "if(C_p_total!=0):\n",
    "    print('details pluies classed udp :{0:.5f}, normal {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_p_udp*100/class_total[2],C_p_normal*100/class_total[2],C_p_jam*100/class_total[2],C_p_udp*100/C_p_total,C_p_normal*100/C_p_total,C_p_jam*100/C_p_total))\n",
    "else:\n",
    "    print('detection pluies et orages 100')\n",
    "\n",
    "print('detection du deni naturel : jam : {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum4), class_correct[3]*100/class_total[3] ,class_correct[3] ,class_total[3]))\n",
    "if(C_j_total!=0):\n",
    "    print('details jam classed udp :{0:.5f}, pluies {1:.5f} , normal {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_j_udp*100/class_total[3],C_j_pluies*100/class_total[3],C_j_normal*100/class_total[3],C_j_udp*100/C_j_total,C_j_pluies*100/C_j_total,C_j_normal*100/C_j_total))\n",
    "else:\n",
    "    print('detection brouillage 100')\n",
    "print('=====================')\n",
    "\n",
    "print('<================================>')\n",
    "print('Total_time')\n",
    "print(time.time()-a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "def Test():\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Hyper-parameters \n",
    "    # input_size = 784 # 28x28\n",
    "    num_classes = 4\n",
    "    #num_epochs = 2\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    input_size = 19\n",
    "    sequence_length = 28\n",
    "    hidden_size = 132\n",
    "    num_layers = 2\n",
    "\n",
    "    class RNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "            super(RNN, self).__init__()\n",
    "            self.num_layers = num_layers\n",
    "            self.hidden_size = hidden_size\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers,batch_first=True)\n",
    "            # -> x needs to be: (batch_size, seq, input_size)\n",
    "\n",
    "            # or:\n",
    "            #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Set initial hidden states (and cell states for LSTM)\n",
    "            x=x.view(-1,1,19)\n",
    "            #print(x.shape)\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "\n",
    "            out, _ = self.rnn(x, h0)  \n",
    "            # or:\n",
    "            #out, _ = self.lstm(x, (h0,c0))  \n",
    "\n",
    "            # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "            # out: (n, 28, 128)\n",
    "\n",
    "            # Decode the hidden state of the last time step\n",
    "            out = out[:, -1, :]\n",
    "            # out: (n, 128)\n",
    "\n",
    "            #out = self.fc(out)\n",
    "            x = F.log_softmax(self.fc(out), dim=1)\n",
    "            # out: (n, 10)\n",
    "            return x\n",
    "\n",
    "    model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "    Fonction_de_perte = nn.NLLLoss()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "    model.load_state_dict(torch.load('models\\\\dataset1_1_50_GRU.pth'))\n",
    "\n",
    "\n",
    "    path0 ='D:\\\\share\\\\Data_file\\\\Senarios\\\\new_test\\\\satellite00\\\\Senario_test_data_normalized.csv' \n",
    "    df = pd.read_csv(path0)\n",
    "    df1=df[['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','packet_type','droppedPKWrongPort','sentPK','size','channel','DataQueueLen','passedUpPk','rcvdPKFromHL','rcvdPKFromLL','sentDownPK','DropPKByQueue','snir','throughput']]         \n",
    "\n",
    "    #df1=df[['channel','Next_Current_diff','Next_Pre_diff','size','packet_type','SNext_Current_diff','SNext_Pre_diff','snir','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P']]     \n",
    "    np_data=df.to_numpy()\n",
    "    df1=df1.to_numpy()\n",
    "    ttorch_tensor = torch.tensor(df1)\n",
    "    tinputs =ttorch_tensor[:,:] \n",
    "    output = torch.exp(model(tinputs))\n",
    "    #test_loss=Fonction_de_perte(output, tlabels1)\n",
    "    top_p , top_c = output.topk(1, dim=1)\n",
    "    node=set()\n",
    "    node1=set()\n",
    "    node2=dict()\n",
    "    node10=dict()\n",
    "    node11=set()\n",
    "\n",
    "\n",
    "    #with open(location+\"results/index.txt\", \"w\") as f:\n",
    "     #   #f.write(str(C_index+1))\n",
    "      #  f.close()\n",
    "    name='Pluies'\n",
    "    if(name=='Pluies'):\n",
    "        print('pluies')\n",
    "        print(len(top_c))\n",
    "        top_c = top_c.cpu().detach().numpy()\n",
    "        for index,i in enumerate(top_c):\n",
    "            i=i[0]\n",
    "            i1=np_data[index][15]+' '+np_data[index][17]\n",
    "\n",
    "            if (i1 in node11):\n",
    "                    pass\n",
    "                    node10[i1]+=1\n",
    "                    #print(node2[i]) \n",
    "            else:\n",
    "                    node11.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node10[i1]=1\n",
    "            if(i==2):\n",
    "                #print('udp attack from '+str(np_data[index][5])+' and with probability of '+str(top_p[index]))\n",
    "                i1=np_data[index][15]+' '+np_data[index][17]\n",
    "                if (i1 in node):\n",
    "                    pass\n",
    "                    node2[i1][1]+=1\n",
    "                    #print(node2[i]) \n",
    "                else:\n",
    "                    node.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node2[i1]=list([np_data[index][1],1])\n",
    "\n",
    "        somme=0\n",
    "        max1=0\n",
    "        for i in node2:\n",
    "            node2[i].append(node2[i][1]*100/len(top_c))\n",
    "            somme+=node2[i][1]*100/len(top_c)\n",
    "            if(max1<(node2[i][1]*100/len(top_c))):\n",
    "                max1=node2[i][1]*100/len(top_c)\n",
    "        if(somme>40 or 1):\n",
    "\n",
    "            print('=========================================================================================================')\n",
    "            print('                                   Pluies et Orages (Brouillage Naturel)')\n",
    "            print('=========================================================================================================')\n",
    "            #now = datetime.now()\n",
    "            #print(' Detection entre :'+str(dt_0)+' et ' + str(now.strftime(\" %H:%M:%S\")))\n",
    "            for i in node2:\n",
    "                print('Flux entre '+str(i.replace(' ', ' et '))+' avec une probabilite de '+str(\"{:.2f}\".format(node2[i][1]*100/node10[i])) + '% ( '+ str(\"{:.2f}\".format(node2[i][2])) +' % de flux capture )'  )\n",
    "    node=set()\n",
    "    node1=set()\n",
    "    node2=dict()\n",
    "    node10=dict()\n",
    "    node11=set()\n",
    "\n",
    "    #name='Jam'\n",
    "    if(name=='Jam'):\n",
    "        #print(location)\n",
    "        #print(np_data[0])\n",
    "        for index,i in enumerate(top_c):\n",
    "            i=i[0]\n",
    "            i1=np_data[index][5]+' '+np_data[index][6]\n",
    "            if (i1 in node11):\n",
    "                    pass\n",
    "                    node10[i1]+=1\n",
    "                    #print(node2[i]) \n",
    "            else:\n",
    "                    node11.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node10[i1]=1\n",
    "            if(i==3):\n",
    "\n",
    "                #print('udp attack from '+str(np_data[index][5])+' and with probability of '+str(top_p[index]))\n",
    "                i1=np_data[index][5]+' '+np_data[index][6]\n",
    "                if (i1 in node):\n",
    "                    pass\n",
    "                    node2[i1][1]+=1\n",
    "                    #print(node2[i]) \n",
    "                else:\n",
    "                    node.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node2[i1]=list([np_data[index][1],1])\n",
    "        somme=0\n",
    "        max1=0\n",
    "        for i in node2:\n",
    "            node2[i].append(node2[i][1]*100/len(top_c))\n",
    "            somme+=node2[i][1]*100/len(top_c)\n",
    "            if(max1<(node2[i][1]*100/len(top_c))):\n",
    "                max1=node2[i][1]*100/len(top_c)\n",
    "        if(somme>35 or 1):\n",
    "\n",
    "            print('=========================================================================================================')\n",
    "            print('                                              Jamming')\n",
    "            print('=========================================================================================================')\n",
    "           # now = datetime.now()\n",
    "            #print(' Detection entre :'+str(dt_0)+' et ' + str(now.strftime(\" %H:%M:%S\")))\n",
    "\n",
    "            for i in node2:\n",
    "                print('Flux entre '+str(i.replace(' ', ' et '))+' avec une probabilite de '+str(\"{:.2f}\".format(node2[i][1]*100/node10[i])) + ' % ( '+ str(\"{:.2f}\".format(node2[i][2])) +' % de flux capture )'  )\n",
    "    node=set()\n",
    "    node1=set()\n",
    "    node2=dict()\n",
    "    node10=dict()\n",
    "    node11=set()\n",
    "\n",
    "    #name='Flood'\n",
    "    if(name=='Flood'):\n",
    "        #print('flood')\n",
    "        #top_c = top_c.cpu().detach().numpy()\n",
    "        for index,i in enumerate(top_c):\n",
    "            i=i[0]\n",
    "            i1=np_data[index][5]+' '+np_data[index][6]\n",
    "\n",
    "            if (i1 in node11):\n",
    "                    pass\n",
    "                    node10[i1]+=1\n",
    "                    #print(node2[i]) \n",
    "            else:\n",
    "                    node11.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node10[i1]=1\n",
    "            if(i==1):\n",
    "                #print('udp attack from '+str(np_data[index][5])+' and with probability of '+str(top_p[index]))\n",
    "                i1=np_data[index][5]+' '+np_data[index][6]\n",
    "                if (i1 in node):\n",
    "                    pass\n",
    "                    node2[i1][1]+=1\n",
    "                    #print(node2[i]) \n",
    "                else:\n",
    "                    node.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node2[i1]=list([np_data[index][1],1])\n",
    "\n",
    "        somme=0\n",
    "        max1=0\n",
    "        for i in node2:\n",
    "            node2[i].append(node2[i][1]*100/len(top_c))\n",
    "            somme+=node2[i][1]*100/len(top_c)\n",
    "            if(max1<(node2[i][1]*100/len(top_c))):\n",
    "                max1=node2[i][1]*100/len(top_c)\n",
    "        del(top_c)\n",
    "        if(somme>20 or 1):\n",
    "\n",
    "            print('=========================================================================================================')\n",
    "            print('                                      Attaque par Flood')\n",
    "            print('=========================================================================================================')\n",
    "\n",
    "            #now = datetime.now()\n",
    "            #print(' Detection entre :'+str(dt_0)+' et ' + str(now.strftime(\" %H:%M:%S\")))\n",
    "\n",
    "            for i in node2:\n",
    "                print('Flux entre '+str(i.replace(' ', ' et '))+' avec une probabilite de '+str(\"{:.2f}\".format(node2[i][1]*100/node10[i])) + '% ( '+ str(\"{:.2f}\".format(node2[i][2])) +' % de flux capture )'  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pluies\n",
      "7379\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7b08819e076e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-5aadf6a82f37>\u001b[0m in \u001b[0;36mTest\u001b[1;34m()\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mi1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnp_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19212.246824,
   "end_time": "2021-08-22T10:02:29.459810",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-22T04:42:17.212986",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.014009,
     "end_time": "2021-09-02T21:49:09.002083",
     "exception": false,
     "start_time": "2021-09-02T21:49:08.988074",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T21:49:09.028173Z",
     "iopub.status.busy": "2021-09-02T21:49:09.026254Z",
     "iopub.status.idle": "2021-09-02T21:49:45.887464Z",
     "shell.execute_reply": "2021-09-02T21:49:45.888063Z",
     "shell.execute_reply.started": "2021-09-02T21:05:49.247332Z"
    },
    "papermill": {
     "duration": 36.875212,
     "end_time": "2021-09-02T21:49:45.888461",
     "exception": false,
     "start_time": "2021-09-02T21:49:09.013249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import os\n",
    "\n",
    "path0 ='../' \n",
    "df = pd.read_csv(path0+\"FinaldatasetN1.csv\")\n",
    "\n",
    "a=time.time()\n",
    "#df=df[['channel','Next_Current_diff','Next_Pre_diff','size', 'packet_type','DataQueueLen','lossRate','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "#df=df[['channel','Next_Current_diff','Next_Pre_diff','size', 'packet_type','lossRate','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "df=df[['channel','Next_Current_diff','Next_Pre_diff','size','packet_type','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "#df=df[['channel','Next_Current_diff','Next_Pre_diff','size','packet_type','SNext_Current_diff','SNext_Pre_diff','snir','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "\n",
    "\n",
    "#df=df.to_numpy()\n",
    "#df_Normal=df[200000:354961].copy()\n",
    "df_Normal=df[0:354961].copy()\n",
    "\n",
    "#print(df_Normal[0][1])\n",
    "#print(df_Normal[-1][1])\n",
    "df_UDP1=df[354961:763730].copy()\n",
    "#print(df_UDP1[0][1])\n",
    "#print(df_UDP1[-1][1])\n",
    "df_UDP2=df[763730:1573000].copy()\n",
    "#print(df_UDP2[0][1])\n",
    "#print(df_UDP2[-1][1])\n",
    "#df_jam1=df[1573000:1696992].copy()\n",
    "df_jam1=df[1573000:1696992].copy()\n",
    "\n",
    "#print(df_jam1[0][1])\n",
    "#print(df_jam1[-1][1])\n",
    "df_jam2=df[1696992:2714727].copy()\n",
    "#print(df_jam2[0][1])\n",
    "#print(df_jam2[-1][1])\n",
    "df_pluies=df[2714727:-1].copy()\n",
    "#print(df_pluies[0][1])\n",
    "#print(df_pluies[-1][1])\n",
    "del(df)\n",
    "\n",
    "X=df_Normal.drop(columns = ['label']).copy()\n",
    "y=df_Normal[['label']].copy()\n",
    "#print(y.columns)\n",
    "X1=df_UDP1.drop(columns = ['label']).copy()\n",
    "y1=df_UDP1[['label']].copy()\n",
    "X2=df_UDP2.drop(columns = ['label']).copy()\n",
    "y2=df_UDP2[['label']].copy()\n",
    "X3=df_jam1.drop(columns = ['label']).copy()\n",
    "y3=df_jam1[['label']].copy()\n",
    "X4=df_jam2.drop(columns = ['label']).copy()\n",
    "y4=df_jam2[['label']].copy()\n",
    "X5=df_pluies.drop(columns = ['label']).copy()\n",
    "y5=df_pluies[['label']].copy()\n",
    "\n",
    "train=0.7\n",
    "#train=0.01\n",
    "\n",
    "del(df_Normal)\n",
    "del(df_UDP1)\n",
    "del(df_UDP2)\n",
    "del(df_jam1)\n",
    "del(df_jam2)\n",
    "del(df_pluies)\n",
    "\n",
    "\n",
    "xcol=X.columns\n",
    "ycol=y.columns\n",
    "\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=train,shuffle=False)\n",
    "X_train1, X_rem1, y_train1, y_rem1 = train_test_split(X1,y1, train_size=train,shuffle=False)\n",
    "X_train2, X_rem2, y_train2, y_rem2 = train_test_split(X2,y2, train_size=train,shuffle=False)\n",
    "X_train3, X_rem3, y_train3, y_rem3 = train_test_split(X3,y3, train_size=train,shuffle=False)\n",
    "X_train4, X_rem4, y_train4, y_rem4 = train_test_split(X4,y4, train_size=train,shuffle=False)\n",
    "X_train5, X_rem5, y_train5, y_rem5 = train_test_split(X5,y5, train_size=train,shuffle=False)\n",
    "\n",
    "#validation=0.5\n",
    "#X_valid, X_rem, y_valid, y_rem = train_test_split(X_rem,y_rem, test_size=validation,shuffle=False)\n",
    "#X_valid1, X_rem1, y_valid1, y_rem1 = train_test_split(X_rem1,y_rem1, test_size=validation,shuffle=False)\n",
    "#X_valid2, X_rem2, y_valid2, y_rem2 = train_test_split(X_rem2,y_rem2, test_size=validation,shuffle=False)\n",
    "#X_valid3, X_rem3, y_valid3, y_rem3 = train_test_split(X_rem3,y_rem3, test_size=validation,shuffle=False)\n",
    "#X_valid4, X_rem4, y_valid4, y_rem4= train_test_split(X_rem4,y_rem4, test_size=validation,shuffle=False)\n",
    "#X_valid5, X_rem5, y_valid5, y_rem5 = train_test_split(X_rem5,y_rem5, test_size=validation,shuffle=False)\n",
    "\n",
    "#validation=0.65\n",
    "#X_valid, X_rem, y_valid, y_rem = train_test_split(X_rem,y_rem, test_size=validation,shuffle=False)\n",
    "#X_valid1, X_rem1, y_valid1, y_rem1 = train_test_split(X_rem1,y_rem1, test_size=validation,shuffle=False)\n",
    "#X_valid2, X_rem2, y_valid2, y_rem2 = train_test_split(X_rem2,y_rem2, test_size=validation,shuffle=False)\n",
    "#X_valid3, X_rem3, y_valid3, y_rem3 = train_test_split(X_rem3,y_rem3, test_size=validation,shuffle=False)\n",
    "#X_valid4, X_rem4, y_valid4, y_rem4= train_test_split(X_rem4,y_rem4, test_size=validation,shuffle=False)\n",
    "#X_valid5, X_rem5, y_valid5, y_rem5 = train_test_split(X_rem5,y_rem5, test_size=validation,shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.57,shuffle=False)\n",
    "X_valid1, X_test1, y_valid1, y_test1 = train_test_split(X_rem1,y_rem1, test_size=0.57,shuffle=False)\n",
    "X_valid2, X_test2, y_valid2, y_test2 = train_test_split(X_rem2,y_rem2, test_size=0.57,shuffle=False)\n",
    "X_valid3, X_test3, y_valid3, y_test3 = train_test_split(X_rem3,y_rem3, test_size=0.57,shuffle=False)\n",
    "X_valid4, X_test4, y_valid4, y_test4 = train_test_split(X_rem4,y_rem4, test_size=0.57,shuffle=False)\n",
    "X_valid5, X_test5, y_valid5, y_test5 = train_test_split(X_rem5,y_rem5, test_size=0.57,shuffle=False)\n",
    "\n",
    "\n",
    "X_train=np.concatenate((X_train, X_train1, X_train2,X_train3, X_train4,X_train5))\n",
    "X_valid=np.concatenate((X_valid, X_valid1, X_valid2,X_valid3, X_valid4,X_valid5))\n",
    "X_test=np.concatenate((X_test, X_test1, X_test2,X_test3, X_test4,X_test5))\n",
    "\n",
    "y_train=np.concatenate((y_train, y_train1, y_train2,y_train3, y_train4,y_train5))\n",
    "y_valid=np.concatenate((y_valid, y_valid1, y_valid2,y_valid3, y_valid4,y_valid5))\n",
    "y_test=np.concatenate((y_test, y_test1, y_test2,y_test3, y_test4,y_test5))\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns= xcol)\n",
    "X_valid = pd.DataFrame(X_valid, columns= xcol)\n",
    "X_test = pd.DataFrame(X_test, columns= xcol)\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns= ycol)\n",
    "y_valid = pd.DataFrame(y_valid, columns= ycol)\n",
    "y_test = pd.DataFrame(y_test, columns= ycol)\n",
    "\n",
    "#########333\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T21:49:45.909617Z",
     "iopub.status.busy": "2021-09-02T21:49:45.908832Z",
     "iopub.status.idle": "2021-09-02T21:50:16.313814Z",
     "shell.execute_reply": "2021-09-02T21:50:16.313147Z",
     "shell.execute_reply.started": "2021-09-02T21:06:18.637224Z"
    },
    "papermill": {
     "duration": 30.416719,
     "end_time": "2021-09-02T21:50:16.313986",
     "exception": false,
     "start_time": "2021-09-02T21:49:45.897267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263116\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "2381331\n",
      "438845\n",
      "581729\n",
      "tensor(1598574)\n",
      "tensor(289624)\n",
      "tensor(388668)\n",
      "38.34583830833435\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "a-time.time()\n",
    "X_train1=X_train.values\n",
    "y_train1=y_train.values\n",
    "#X_train1=X_train1.astype(float)\n",
    "\n",
    "torch_tensor = torch.tensor(X_train1)\n",
    "#torch_tensor[torch_tensor != torch_tensor]=float(0)\n",
    "t=[]\n",
    "test=0\n",
    "test0=0\n",
    "\n",
    "for j in y_train1:\n",
    "    i=j[0]\n",
    "#    print(i)\n",
    "    if (i=='Normal'):\n",
    "        t.append(int(0))\n",
    "        test0+=1\n",
    "    #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        t.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        t.append(int(2))\n",
    "        test+=1\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        t.append(int(3))\n",
    "print(test)\n",
    "labels = torch.LongTensor(t)\n",
    "#print(torch_tensor[-1])\n",
    "#print(labels[-1])\n",
    "print(labels)\n",
    "#print(df_label)\n",
    "#print(labels.shape)\n",
    "\n",
    "#df00 = pd.read_csv(path0+\"small/testbed3/Dataset_S03Filtrer.csv\")\n",
    "#bigX = df00[['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','droppedPKWrongPort','sentPK','size','channel','throughput']]         \n",
    "#bigY = df00['label']\n",
    "bigX = X_valid\n",
    "#[['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','droppedPKWrongPort','sentPK','size','throughput']]  ,'channel'       \n",
    "bigY = y_valid['label']\n",
    "#print(bigY)\n",
    "X_valid1=bigX.values\n",
    "y_valid1=bigY.values\n",
    "\n",
    "#X_valid1=X_test.values\n",
    "#y_valid1=y_test.values\n",
    "#X_train1=X_train1.astype(float)\n",
    "\n",
    "vtorch_tensor = torch.tensor(X_valid1)\n",
    "v=[]\n",
    "\n",
    "for i in y_valid1:\n",
    "    #i=j[0]\n",
    "    #print(i)\n",
    "    if (i=='Normal'):\n",
    "        v.append(int(0))\n",
    "        #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        v.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        v.append(int(2))\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        v.append(int(3))\n",
    "\n",
    "vlabels = torch.LongTensor(v)\n",
    "\n",
    "\n",
    "X_test1=X_test.values\n",
    "y_test1=y_test.values\n",
    "\n",
    "#X_train1=X_train1.astype(float)\n",
    "\n",
    "ttorch_tensor = torch.tensor(X_test1)\n",
    "\n",
    "#torch_tensor[torch_tensor != torch_tensor]=float(0)\n",
    "t=[]\n",
    "\n",
    "for j in y_test1:\n",
    "    i=j[0]\n",
    "#    print(i)\n",
    "    if (i=='Normal'):\n",
    "        t.append(int(0))\n",
    "        #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        t.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        t.append(int(2))\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        t.append(int(3))\n",
    "\n",
    "tlabels = torch.LongTensor(t)\n",
    "#print(torch_tensor[-1])\n",
    "#print(labels[-1])\n",
    "print(len(labels))\n",
    "print(len(vlabels))\n",
    "print(len(tlabels))\n",
    "\n",
    "del(X)\n",
    "del(y)\n",
    "del(X1)\n",
    "del(y1)\n",
    "del(X2)\n",
    "del(y2)\n",
    "del(X3)\n",
    "del(y3)\n",
    "del(X4)\n",
    "del(y4)\n",
    "del(X5)\n",
    "del(y5)\n",
    "del(X_train1); del(X_train2);del(X_train3); del(X_train4);del(X_train5)\n",
    "del(y_train1); del(y_train2);del(y_train3); del(y_train4);del(y_train5)\n",
    "del(y_rem);del(y_rem1); del(y_rem2);del(y_rem3); del(y_rem4);del(y_rem5)\n",
    "del(X_rem);del(X_rem1); del(X_rem2);del(X_rem3); del(X_rem4);del(X_rem5)\n",
    "\n",
    "#X_rem1, y_train1, y_rem1\n",
    "del(X_train)\n",
    "del(X_valid)\n",
    "del(X_valid1); del(X_valid2);del(X_valid3); del(X_valid4);del(X_valid5)\n",
    "del(y_valid1); del(y_valid2);del(y_valid3); del(y_valid4);del(y_valid5)\n",
    "\n",
    "\n",
    "del(X_test)\n",
    "del(X_test1); del(X_test2);del(X_test3); del(X_test4);del(X_test5)\n",
    "del(y_test1); del(y_test2);del(y_test3); del(y_test4);del(y_test5)\n",
    "\n",
    "del(y_train)\n",
    "del(y_valid)\n",
    "del(y_test)\n",
    "del(t)\n",
    "del(v)\n",
    "\n",
    "inputs=torch_tensor[:,:] \n",
    "labels1=labels[:]\n",
    "del(torch_tensor)\n",
    "del(labels)\n",
    "#print(len(inputs))\n",
    "#print(len(labels1))\n",
    "vinputs =vtorch_tensor[:,:]\n",
    "vlabels1=vlabels[:]\n",
    "del(vtorch_tensor)\n",
    "del(vlabels)\n",
    "tinputs =ttorch_tensor[:,:] \n",
    "tlabels1=tlabels[:]\n",
    "del(ttorch_tensor)\n",
    "del(tlabels)\n",
    "\n",
    "print(sum(labels1))\n",
    "print(sum(vlabels1))\n",
    "print(sum(tlabels1))\n",
    "\n",
    " \n",
    "print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T21:50:16.340997Z",
     "iopub.status.busy": "2021-09-02T21:50:16.340291Z",
     "iopub.status.idle": "2021-09-02T21:50:16.364834Z",
     "shell.execute_reply": "2021-09-02T21:50:16.365460Z",
     "shell.execute_reply.started": "2021-09-02T21:06:46.013312Z"
    },
    "papermill": {
     "duration": 0.040208,
     "end_time": "2021-09-02T21:50:16.365679",
     "exception": false,
     "start_time": "2021-09-02T21:50:16.325471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "# input_size = 784 # 28x28\n",
    "num_classes = 4\n",
    "#num_epochs = 2\n",
    "batch_size = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "input_size = 14\n",
    "sequence_length = 28\n",
    "hidden_size = 132\n",
    "num_layers = 2\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers,batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        x=x.view(-1,1,14)\n",
    "        #print(x.shape)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        #out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        #out = self.fc(out)\n",
    "        x = F.log_softmax(self.fc(out), dim=1)\n",
    "        # out: (n, 10)\n",
    "        return x\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    " \n",
    "       \n",
    "Fonction_de_perte = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('models\\\\dataset2_1000_200_GRU.pth'))\n",
    "#indices = [0,3, 299:303]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T21:50:16.394602Z",
     "iopub.status.busy": "2021-09-02T21:50:16.393801Z",
     "iopub.status.idle": "2021-09-03T03:38:32.154187Z",
     "shell.execute_reply": "2021-09-03T03:38:32.154775Z",
     "shell.execute_reply.started": "2021-09-02T21:06:46.039903Z"
    },
    "papermill": {
     "duration": 20895.776567,
     "end_time": "2021-09-03T03:38:32.155072",
     "exception": false,
     "start_time": "2021-09-02T21:50:16.378505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#labels1=labels[loc]\n",
    "a=time.time()\n",
    "#inputs=torch_tensor[:,:].to(device) \n",
    "##labels1=labels[:].to(device)\n",
    "#print(len(inputs))\n",
    "#print(len(labels1))\n",
    "#vinputs =vtorch_tensor[:,:].to(device)\n",
    "#vlabels1=vlabels[:].to(device)\n",
    "no_learning=0\n",
    "epochs = 200\n",
    "valid_loss=0\n",
    "accuracy=0\n",
    "valid_loss_min=np.Inf\n",
    "batch_size = 500\n",
    "\n",
    "####33  !!!!!!!!!! This shit cost alot of time ----> do not use it \n",
    "#b=time.time()\n",
    "#with torch.no_grad():\n",
    "   # for i, x in enumerate(vinputs):\n",
    "       # x2=x[None,:]\n",
    "       # output = torch.exp(model(x2))\n",
    "       # l2=vlabels1[i][None]\n",
    "       # test_loss+=Fonction_de_perte(output, l2)\n",
    "     #   output = torch.exp(output)\n",
    "      #  top_p , top_c = output.topk(1, dim=1)\n",
    "     #   equals = top_c==vlabels1[i].view(*top_c.shape)\n",
    "    #    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "   # tain_losses.append(running_loss/len(inputs))\n",
    "   # test_losses.append(test_loss/len(vinputs))\n",
    "   # print ('test accuracy :{0}, test loss : {2} ,  time {1} '.format(accuracy*100/len(vinputs) , time.time()-b ,test_loss/len(vinputs) ))\n",
    "\n",
    "######## ---> use this :) \n",
    "b=time.time()\n",
    "with torch.no_grad():\n",
    "    output = torch.exp(model(vinputs))\n",
    "    valid_loss=Fonction_de_perte(output, vlabels1)\n",
    "top_p , top_c = output.topk(1, dim=1)\n",
    "del(output)\n",
    "propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "equals = top_c==vlabels1.view(*top_c.shape)\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "#print ('valid accuracy :{0:.8f} with total prob : {3:.8f} and  valid loss : {2:.8f} ,  time {1:.6f} '.format(accuracy*100 , time.time()-b ,test_loss,propabilities))\n",
    "print ('Epoch  : valid accuracy :{0:.8f} with total prob : {1:.8f} '.format(accuracy*100  ,propabilities))\n",
    "\n",
    "####\n",
    "#print(inputs.shape[0])\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "for e in range(epochs):\n",
    "    c=time.time()\n",
    "    running_loss = 0\n",
    "    #for i, x in enumerate(inputs):\n",
    "    for i in range(0,inputs.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #indices = [i:i+batch_size]\n",
    "        #print(i+batch_size)\n",
    "        batch_x, batch_y = inputs[i:i+batch_size], labels1[i:i+batch_size]\n",
    "        \n",
    "        #x2=x[None,:]\n",
    "        output = model.forward(batch_x)\n",
    "        \n",
    "        #output = model.forward(x2)\n",
    "        #l2=labels1[i][None]\n",
    "        #print(l2)\n",
    "        #loss = Fonction_de_perte(output, l2)\n",
    "        loss = Fonction_de_perte(output, batch_y)\n",
    "      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "        accuracy=0\n",
    "        #print(l2)\n",
    "       \n",
    "        b=time.time()\n",
    "        with torch.no_grad():\n",
    "            ##\n",
    "            #with torch.no_grad():\n",
    "            #for i, x in enumerate(vinputs):\n",
    "             #   x2=x[None,:]\n",
    "              #  output = torch.exp(model(x2))\n",
    "               # l2=vlabels1[i][None]\n",
    "                #valid_loss1+=Fonction_de_perte(output, l2)\n",
    "            #test_loss=test_loss/len(vinputs)\n",
    "            ##\n",
    "                \n",
    "            model.eval()\n",
    "            output = torch.exp(model(vinputs))\n",
    "            valid_loss=Fonction_de_perte(output, vlabels1)\n",
    "        top_p , top_c = output.topk(1, dim=1)\n",
    "        del(output)\n",
    "        propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "        equals = top_c==vlabels1.view(*top_c.shape)\n",
    "        accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        print('Epoch {2} : Training loss {0:.8f} and valid loss : {1:.8f} :  '.format(running_loss/len(inputs),valid_loss, e))\n",
    "   \n",
    "        print ('Epoch {2} : valid accuracy :{0:.8f} with total prob : {1:.8f} '.format(accuracy*100  ,propabilities, e))\n",
    "        print(time.time()-c)\n",
    "    #print('validation loss with iterations {0}'.format(valid_loss1/len(vinputs) ) )   \n",
    "        train_losses.append(running_loss/len(inputs))\n",
    "        valid_losses.append(valid_loss)\n",
    "        if (valid_loss<valid_loss_min):\n",
    "            print('validation loss decreased , saving model ({:.8f} ==> {:.8f})'.format(valid_loss_min,valid_loss))\n",
    "            torch.save(model.state_dict(),'14mod_gru_adam.pth')\n",
    "            no_learning=0\n",
    "            valid_loss_min=valid_loss\n",
    "        else:\n",
    "            no_learning+=1\n",
    "        if(no_learning==10):\n",
    "            no_learning=0\n",
    "            #model.load_state_dict(torch.load('3mod_temp_5_mlp_b.pth'))\n",
    "\n",
    "print(time.time()-a)\n",
    "torch.save(model.state_dict(),'14mod_gru_adam#2.pth')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T03:38:32.318583Z",
     "iopub.status.busy": "2021-09-03T03:38:32.317686Z",
     "iopub.status.idle": "2021-09-03T03:38:32.718554Z",
     "shell.execute_reply": "2021-09-03T03:38:32.719116Z",
     "shell.execute_reply.started": "2021-09-02T21:21:27.612532Z"
    },
    "papermill": {
     "duration": 0.491275,
     "end_time": "2021-09-03T03:38:32.719330",
     "exception": false,
     "start_time": "2021-09-03T03:38:32.228055",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "#test_losses_scaled = scaler.fit_transform(test_losses)\n",
    "\n",
    "\n",
    "\n",
    "Tr=np.array(train_losses)\n",
    "Te=np.array(valid_losses)\n",
    "\n",
    "Tr=np.reshape(Tr, (len(Tr),1))\n",
    "Te=np.reshape(Te, (len(Te),1))\n",
    "\n",
    "# fit on training data column\n",
    "scale = StandardScaler().fit(Tr)\n",
    "tain_losses = scale.transform(Tr)\n",
    "scale = StandardScaler().fit(Te)\n",
    "test_losses = scale.transform(Te)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "#test_losses_scaled = scaler.fit_transform(test_losses)\n",
    "plt.ylim([-1,1])\n",
    "\n",
    "plt.plot(tain_losses, label='Train loss')\n",
    "plt.plot(test_losses, label='valid loss')\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T21:21:27.844302Z",
     "iopub.status.busy": "2021-09-02T21:21:27.844052Z"
    },
    "papermill": {
     "duration": 0.074717,
     "end_time": "2021-09-03T03:38:32.869913",
     "exception": false,
     "start_time": "2021-09-03T03:38:32.795196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "=====================\n",
      "test accuracy :95.89997864 with total prob : 99.68845367 and  test loss : -0.95767213 ,  time 34.39602947 \n",
      "la précision de detection globale: 95.76721314894914 \n",
      "detection des communication normal: 92.65354103 with accuracy 92.49048322 ( 293747/317597 )\n",
      "details normal classed udp :4.09009, pluies 0.00000 , jam 3.41943 (ou 54.46541,0.00000,45.53459) \n",
      "detection du deni de service par udp flood 99.25854067 with accuracy 100.00000000 ( 170159/170159 )\n",
      "detection udp flood 100\n",
      "detection du deni naturel : pluies et orages : 99.99841825 with accuracy 99.99842296 ( 63409/63410 )\n",
      "details pluies classed udp :0.00000, normal 0.00158 , jam 0.00000 (ou 0.00000,100.00000,0.00000) \n",
      "detection du deni naturel : jam : 99.90657804 with accuracy 100.00000000 ( 30563/30563 )\n",
      "detection brouillage 100\n",
      "=====================\n",
      "<================================>\n",
      "Total_time\n",
      "395.333860874176\n"
     ]
    }
   ],
   "source": [
    "a=time.time()\n",
    "summm=[]\n",
    "sum1=[]\n",
    "sum2=[]\n",
    "sum3=[]\n",
    "sum4=[]\n",
    "\n",
    "#tinputs =ttorch_tensor[:,:] \n",
    "#tlabels1=tlabels[:]\n",
    "\n",
    "\n",
    "#tinputs =torch_tensor[:,:] \n",
    "#tlabels1=labels[:]\n",
    "\n",
    "print('=====================')\n",
    "print('=====================')\n",
    "b=time.time()\n",
    "output = torch.exp(model(tinputs))\n",
    "test_loss=Fonction_de_perte(output, tlabels1)\n",
    "top_p , top_c = output.topk(1, dim=1)\n",
    "propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "equals = top_c==tlabels1.view(*top_c.shape)\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "model.train()\n",
    "print ('test accuracy :{0:.8f} with total prob : {3:.8f} and  test loss : {2:.8f} ,  time {1:.8f} '.format(accuracy*100 , time.time()-b ,test_loss ,propabilities))\n",
    "\n",
    "class_correct = list(0. for i in range (4))\n",
    "class_total = list(0. for i in range (4))\n",
    "\n",
    "C_n_udp=0\n",
    "C_n_pluies=0\n",
    "C_n_jam=0\n",
    "C_n_total=0\n",
    "\n",
    "\n",
    "C_u_normal=0\n",
    "C_u_jam=0\n",
    "C_u_pluies=0\n",
    "C_u_total=0\n",
    "\n",
    "C_p_normal=0\n",
    "C_p_udp=0\n",
    "C_p_jam=0\n",
    "C_p_total=0\n",
    "\n",
    "C_j_normal=0\n",
    "C_j_udp=0\n",
    "C_j_pluies=0\n",
    "C_j_total=0\n",
    "\n",
    "for i, x in enumerate(tinputs):\n",
    "    optimizer.zero_grad()\n",
    "    x2=x[None,:]\n",
    "    with torch.no_grad():\n",
    "        output = torch.exp(model(x2))\n",
    "    out=output.detach().numpy()*100\n",
    " \n",
    "    l3=tlabels1[i].item()\n",
    "  \n",
    "    if(l3==0):\n",
    "        summm.append(out[0][0])\n",
    "        sum1.append(out[0][0])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_n_udp+=1\n",
    "            if(top_c[i][0]==2):\n",
    "                C_n_pluies+=1\n",
    "            if(top_c[i][0]==3):\n",
    "                C_n_jam +=1\n",
    "            \n",
    "            C_n_total +=1\n",
    "            \n",
    "    if(l3==1):\n",
    "        summm.append(out[0][1])\n",
    "        sum2.append(out[0][1])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==3):\n",
    "                C_u_jam+=1\n",
    "            if(top_c[i][0]==2):\n",
    "                C_u_pluies+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_u_normal +=1\n",
    "            \n",
    "            C_u_total +=1\n",
    "\n",
    "    if(l3==2):\n",
    "        summm.append(out[0][2])\n",
    "        sum3.append(out[0][2])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_p_udp+=1\n",
    "            if(top_c[i][0]==3):\n",
    "                C_p_jam+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_p_normal +=1\n",
    "            \n",
    "            C_p_total +=1\n",
    "\n",
    "    if(l3==3):\n",
    "        summm.append(out[0][3])\n",
    "        sum4.append(out[0][3])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_j_udp+=1\n",
    "            if(top_c[i][0]==2):\n",
    "                C_j_pluies+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_j_normal +=1\n",
    "            \n",
    "            C_j_total +=1\n",
    "\n",
    "    class_total[l3]+=1\n",
    "    class_correct[l3]+=equals[i][0]\n",
    "    #print(output)\n",
    "print('la précision de detection globale: {0} '.format(mean(summm)))\n",
    "print('detection des communication normal: {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum1), class_correct[0]*100/class_total[0] ,class_correct[0] ,class_total[0]))\n",
    "if(C_n_total!=0):\n",
    "    print('details normal classed udp :{0:.5f}, pluies {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_n_udp*100/class_total[0],C_n_pluies*100/class_total[0],C_n_jam*100/class_total[0],C_n_udp*100/C_n_total,C_n_pluies*100/C_n_total,C_n_jam*100/C_n_total))\n",
    "else:\n",
    "    print('detection normal 100')\n",
    "\n",
    "print('detection du deni de service par udp flood {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum2), class_correct[1]*100/class_total[1] ,class_correct[1] ,class_total[1]))\n",
    "if(C_u_total!=0):\n",
    "    print('details udp classed normal :{0:.5f}, pluies {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_u_normal*100/class_total[1],C_u_pluies*100/class_total[1],C_u_jam*100/class_total[1],C_u_normal*100/C_u_total,C_u_pluies*100/C_u_total,C_u_jam*100/C_u_total))\n",
    "else:\n",
    "    print('detection udp flood 100')\n",
    "    \n",
    "print('detection du deni naturel : pluies et orages : {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum3), class_correct[2]*100/class_total[2] ,class_correct[2] ,class_total[2]))\n",
    "if(C_p_total!=0):\n",
    "    print('details pluies classed udp :{0:.5f}, normal {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_p_udp*100/class_total[2],C_p_normal*100/class_total[2],C_p_jam*100/class_total[2],C_p_udp*100/C_p_total,C_p_normal*100/C_p_total,C_p_jam*100/C_p_total))\n",
    "else:\n",
    "    print('detection pluies et orages 100')\n",
    "\n",
    "print('detection du deni naturel : jam : {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum4), class_correct[3]*100/class_total[3] ,class_correct[3] ,class_total[3]))\n",
    "if(C_j_total!=0):\n",
    "    print('details jam classed udp :{0:.5f}, pluies {1:.5f} , normal {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_j_udp*100/class_total[3],C_j_pluies*100/class_total[3],C_j_normal*100/class_total[3],C_j_udp*100/C_j_total,C_j_pluies*100/C_j_total,C_j_normal*100/C_j_total))\n",
    "else:\n",
    "    print('detection brouillage 100')\n",
    "print('=====================')\n",
    "\n",
    "print('<================================>')\n",
    "print('Total_time')\n",
    "print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import os\n",
    "\n",
    "\n",
    "def Test():  \n",
    "\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "\n",
    "    from torch import nn, optim\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Hyper-parameters \n",
    "    # input_size = 784 # 28x28\n",
    "    num_classes = 4\n",
    "    #num_epochs = 2\n",
    "    batch_size = 500\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    input_size = 14\n",
    "    sequence_length = 28\n",
    "    hidden_size = 132\n",
    "    num_layers = 2\n",
    "\n",
    "    class RNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "            super(RNN, self).__init__()\n",
    "            self.num_layers = num_layers\n",
    "            self.hidden_size = hidden_size\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers,batch_first=True)\n",
    "            # -> x needs to be: (batch_size, seq, input_size)\n",
    "\n",
    "            # or:\n",
    "            #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Set initial hidden states (and cell states for LSTM)\n",
    "            x=x.view(-1,1,14)\n",
    "            #print(x.shape)\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "            #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "\n",
    "            # x: (n, 28, 28), h0: (2, n, 128)\n",
    "\n",
    "            # Forward propagate RNN\n",
    "            out, _ = self.rnn(x, h0)  \n",
    "            # or:\n",
    "            #out, _ = self.lstm(x, (h0,c0))  \n",
    "\n",
    "            # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "            # out: (n, 28, 128)\n",
    "\n",
    "            # Decode the hidden state of the last time step\n",
    "            out = out[:, -1, :]\n",
    "            # out: (n, 128)\n",
    "\n",
    "            #out = self.fc(out)\n",
    "            x = F.log_softmax(self.fc(out), dim=1)\n",
    "            # out: (n, 10)\n",
    "            return x\n",
    "\n",
    "    model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "\n",
    "    Fonction_de_perte = nn.NLLLoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "\n",
    "    model.load_state_dict(torch.load('models\\\\dataset2_1000_200_GRU.pth'))\n",
    "    #indices = [0,3, 299:303]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    path0 ='D:\\\\share\\\\Data_file\\\\Senarios\\\\new_test\\\\dataset2.csv' \n",
    "    df = pd.read_csv(path0)\n",
    "    df1=df[['channel','Next_Current_diff','Next_Pre_diff','size','packet_type','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P']]     \n",
    "    np_data=df.to_numpy()\n",
    "    df1=df1.to_numpy()\n",
    "    ttorch_tensor = torch.tensor(df1)\n",
    "    tinputs =ttorch_tensor[:,:] \n",
    "    output = torch.exp(model(tinputs))\n",
    "    #test_loss=Fonction_de_perte(output, tlabels1)\n",
    "    top_p , top_c = output.topk(1, dim=1)\n",
    "    node=set()\n",
    "    node1=set()\n",
    "    node2=dict()\n",
    "    node10=dict()\n",
    "    node11=set()\n",
    "\n",
    "\n",
    "    #with open(location+\"results/index.txt\", \"w\") as f:\n",
    "     #   #f.write(str(C_index+1))\n",
    "      #  f.close()\n",
    "    name='Pluies'\n",
    "    if(name=='Pluies'):\n",
    "        print('pluies')\n",
    "        print(len(top_c))\n",
    "        top_c = top_c.cpu().detach().numpy()\n",
    "        for index,i in enumerate(top_c):\n",
    "            i=i[0]\n",
    "            i1=np_data[index][5]+' '+np_data[index][6]\n",
    "\n",
    "            if (i1 in node11):\n",
    "                    pass\n",
    "                    node10[i1]+=1\n",
    "                    #print(node2[i]) \n",
    "            else:\n",
    "                    node11.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node10[i1]=1\n",
    "            if(i==2):\n",
    "                #print('udp attack from '+str(np_data[index][5])+' and with probability of '+str(top_p[index]))\n",
    "                i1=np_data[index][5]+' '+np_data[index][6]\n",
    "                if (i1 in node):\n",
    "                    pass\n",
    "                    node2[i1][1]+=1\n",
    "                    #print(node2[i]) \n",
    "                else:\n",
    "                    node.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node2[i1]=list([np_data[index][1],1])\n",
    "\n",
    "        somme=0\n",
    "        max1=0\n",
    "        for i in node2:\n",
    "            node2[i].append(node2[i][1]*100/len(top_c))\n",
    "            somme+=node2[i][1]*100/len(top_c)\n",
    "            if(max1<(node2[i][1]*100/len(top_c))):\n",
    "                max1=node2[i][1]*100/len(top_c)\n",
    "        if(somme>40 or 1):\n",
    "\n",
    "            print('=========================================================================================================')\n",
    "            print('                                   Pluies et Orages (Brouillage Naturel)')\n",
    "            print('=========================================================================================================')\n",
    "            #now = datetime.now()\n",
    "            #print(' Detection entre :'+str(dt_0)+' et ' + str(now.strftime(\" %H:%M:%S\")))\n",
    "            for i in node2:\n",
    "                print('Flux entre '+str(i.replace(' ', ' et '))+' avec une probabilite de '+str(\"{:.2f}\".format(node2[i][1]*100/node10[i])) + '% ( '+ str(\"{:.2f}\".format(node2[i][2])) +' % de flux capture )'  )\n",
    "    node=set()\n",
    "    node1=set()\n",
    "    node2=dict()\n",
    "    node10=dict()\n",
    "    node11=set()\n",
    "\n",
    "    name='Jam'\n",
    "    if(name=='Jam'):\n",
    "        #print(location)\n",
    "        #print(np_data[0])\n",
    "        for index,i in enumerate(top_c):\n",
    "            i=i[0]\n",
    "            i1=np_data[index][5]+' '+np_data[index][6]\n",
    "            if (i1 in node11):\n",
    "                    pass\n",
    "                    node10[i1]+=1\n",
    "                    #print(node2[i]) \n",
    "            else:\n",
    "                    node11.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node10[i1]=1\n",
    "            if(i==3):\n",
    "\n",
    "                #print('udp attack from '+str(np_data[index][5])+' and with probability of '+str(top_p[index]))\n",
    "                i1=np_data[index][5]+' '+np_data[index][6]\n",
    "                if (i1 in node):\n",
    "                    pass\n",
    "                    node2[i1][1]+=1\n",
    "                    #print(node2[i]) \n",
    "                else:\n",
    "                    node.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node2[i1]=list([np_data[index][1],1])\n",
    "        somme=0\n",
    "        max1=0\n",
    "        for i in node2:\n",
    "            node2[i].append(node2[i][1]*100/len(top_c))\n",
    "            somme+=node2[i][1]*100/len(top_c)\n",
    "            if(max1<(node2[i][1]*100/len(top_c))):\n",
    "                max1=node2[i][1]*100/len(top_c)\n",
    "        if(somme>35 or 1):\n",
    "\n",
    "            print('=========================================================================================================')\n",
    "            print('                                              Jamming')\n",
    "            print('=========================================================================================================')\n",
    "           # now = datetime.now()\n",
    "            #print(' Detection entre :'+str(dt_0)+' et ' + str(now.strftime(\" %H:%M:%S\")))\n",
    "\n",
    "            for i in node2:\n",
    "                print('Flux entre '+str(i.replace(' ', ' et '))+' avec une probabilite de '+str(\"{:.2f}\".format(node2[i][1]*100/node10[i])) + ' % ( '+ str(\"{:.2f}\".format(node2[i][2])) +' % de flux capture )'  )\n",
    "    node=set()\n",
    "    node1=set()\n",
    "    node2=dict()\n",
    "    node10=dict()\n",
    "    node11=set()\n",
    "\n",
    "    name='Flood'\n",
    "    if(name=='Flood'):\n",
    "        #print('flood')\n",
    "        #top_c = top_c.cpu().detach().numpy()\n",
    "        for index,i in enumerate(top_c):\n",
    "            i=i[0]\n",
    "            i1=np_data[index][5]+' '+np_data[index][6]\n",
    "\n",
    "            if (i1 in node11):\n",
    "                    pass\n",
    "                    node10[i1]+=1\n",
    "                    #print(node2[i]) \n",
    "            else:\n",
    "                    node11.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node10[i1]=1\n",
    "            if(i==1):\n",
    "                #print('udp attack from '+str(np_data[index][5])+' and with probability of '+str(top_p[index]))\n",
    "                i1=np_data[index][5]+' '+np_data[index][6]\n",
    "                if (i1 in node):\n",
    "                    pass\n",
    "                    node2[i1][1]+=1\n",
    "                    #print(node2[i]) \n",
    "                else:\n",
    "                    node.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node2[i1]=list([np_data[index][1],1])\n",
    "\n",
    "        somme=0\n",
    "        max1=0\n",
    "        for i in node2:\n",
    "            node2[i].append(node2[i][1]*100/len(top_c))\n",
    "            somme+=node2[i][1]*100/len(top_c)\n",
    "            if(max1<(node2[i][1]*100/len(top_c))):\n",
    "                max1=node2[i][1]*100/len(top_c)\n",
    "        del(top_c)\n",
    "        if(somme>20 or 1):\n",
    "\n",
    "            print('=========================================================================================================')\n",
    "            print('                                      Attaque par Flood')\n",
    "            print('=========================================================================================================')\n",
    "\n",
    "            #now = datetime.now()\n",
    "            #print(' Detection entre :'+str(dt_0)+' et ' + str(now.strftime(\" %H:%M:%S\")))\n",
    "\n",
    "            for i in node2:\n",
    "                print('Flux entre '+str(i.replace(' ', ' et '))+' avec une probabilite de '+str(\"{:.2f}\".format(node2[i][1]*100/node10[i])) + '% ( '+ str(\"{:.2f}\".format(node2[i][2])) +' % de flux capture )'  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GRU multiclasses dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pluies\n",
      "38100\n",
      "=========================================================================================================\n",
      "                                   Pluies et Orages (Brouillage Naturel)\n",
      "=========================================================================================================\n",
      "Flux entre 19.10.1.104 et 19.10.1.107 avec une probabilite de 100.00% ( 4.62 % de flux capture )\n",
      "Flux entre 19.10.1.103 et 19.11.20.100 avec une probabilite de 100.00% ( 6.51 % de flux capture )\n",
      "Flux entre 19.10.1.104 et 19.10.1.106 avec une probabilite de 100.00% ( 1.50 % de flux capture )\n",
      "=========================================================================================================\n",
      "                                              Jamming\n",
      "=========================================================================================================\n",
      "Flux entre 19.10.1.101 et 19.11.19.100 avec une probabilite de 100.00 % ( 0.78 % de flux capture )\n",
      "Flux entre 19.10.1.102 et 19.11.16.100 avec une probabilite de 100.00 % ( 0.80 % de flux capture )\n",
      "Flux entre 19.10.1.102 et 19.11.14.100 avec une probabilite de 100.00 % ( 0.80 % de flux capture )\n",
      "Flux entre 19.10.1.101 et 19.11.16.100 avec une probabilite de 100.00 % ( 3.62 % de flux capture )\n",
      "Flux entre 19.10.1.102 et 19.10.1.105 avec une probabilite de 100.00 % ( 1.53 % de flux capture )\n",
      "Flux entre 19.10.1.100 et 19.10.1.108 avec une probabilite de 100.00 % ( 1.54 % de flux capture )\n",
      "=========================================================================================================\n",
      "                                      Attaque par Flood\n",
      "=========================================================================================================\n",
      "Flux entre 19.10.1.105 et 19.10.1.254 avec une probabilite de 100.00% ( 12.16 % de flux capture )\n",
      "Flux entre 19.10.1.106 et 19.10.1.254 avec une probabilite de 100.00% ( 12.10 % de flux capture )\n",
      "Flux entre 19.10.1.254 et 19.10.1.105 avec une probabilite de 100.00% ( 4.34 % de flux capture )\n",
      "Flux entre 19.10.1.254 et 19.10.1.106 avec une probabilite de 100.00% ( 4.41 % de flux capture )\n"
     ]
    }
   ],
   "source": [
    "Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20975.357492,
   "end_time": "2021-09-03T03:38:35.612540",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-02T21:49:00.255048",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

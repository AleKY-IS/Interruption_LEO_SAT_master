{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improt time 9.768874168395996\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import os\n",
    "\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    " #   for filename in filenames:\n",
    "  #      print(os.path.join(dirname, filename))\n",
    "a=time.time()\n",
    "path0 ='D:\\\\share\\\\with weather\\\\'\n",
    "df = pd.read_csv(path0+\"FinaldatasetN1.csv\")\n",
    "print('improt time '+str(time.time()-a))\n",
    "\n",
    "a=time.time()\n",
    "#df=df[['channel','Next_Current_diff','Next_Pre_diff','size', 'packet_type','DataQueueLen','lossRate','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "#df=df[['channel','Next_Current_diff','Next_Pre_diff','size', 'packet_type','lossRate','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "#df=df[['channel','Next_Current_diff','Next_Pre_diff','size','snir','throughput','packet_type','SNext_Current_diff','SNext_Pre_diff','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "df=df[['channel','Next_Current_diff','Next_Pre_diff','size','packet_type','SNext_Current_diff','SNext_Pre_diff','snir','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "\n",
    "\n",
    "#df=df.to_numpy()\n",
    "#df_Normal=df[200000:354961].copy()\n",
    "df_Normal=df[0:354961].copy()\n",
    "\n",
    "#print(df_Normal[0][1])\n",
    "#print(df_Normal[-1][1])\n",
    "df_UDP1=df[354961:763730].copy()\n",
    "#print(df_UDP1[0][1])\n",
    "#print(df_UDP1[-1][1])\n",
    "df_UDP2=df[763730:1573000].copy()\n",
    "#print(df_UDP2[0][1])\n",
    "#print(df_UDP2[-1][1])\n",
    "#df_jam1=df[1573000:1696992].copy()\n",
    "df_jam1=df[1573000:1696992].copy()\n",
    "\n",
    "#print(df_jam1[0][1])\n",
    "#print(df_jam1[-1][1])\n",
    "df_jam2=df[1696992:2714727].copy()\n",
    "#print(df_jam2[0][1])\n",
    "#print(df_jam2[-1][1])\n",
    "df_pluies=df[2714727:-1].copy()\n",
    "#print(df_pluies[0][1])\n",
    "#print(df_pluies[-1][1])\n",
    "del(df)\n",
    "\n",
    "X=df_Normal.drop(columns = ['label']).copy()\n",
    "y=df_Normal[['label']].copy()\n",
    "#print(y.columns)\n",
    "X1=df_UDP1.drop(columns = ['label']).copy()\n",
    "y1=df_UDP1[['label']].copy()\n",
    "X2=df_UDP2.drop(columns = ['label']).copy()\n",
    "y2=df_UDP2[['label']].copy()\n",
    "X3=df_jam1.drop(columns = ['label']).copy()\n",
    "y3=df_jam1[['label']].copy()\n",
    "X4=df_jam2.drop(columns = ['label']).copy()\n",
    "y4=df_jam2[['label']].copy()\n",
    "X5=df_pluies.drop(columns = ['label']).copy()\n",
    "y5=df_pluies[['label']].copy()\n",
    "\n",
    "train=0.7\n",
    "#train=0.01\n",
    "\n",
    "del(df_Normal)\n",
    "del(df_UDP1)\n",
    "del(df_UDP2)\n",
    "del(df_jam1)\n",
    "del(df_jam2)\n",
    "del(df_pluies)\n",
    "\n",
    "\n",
    "xcol=X.columns\n",
    "ycol=y.columns\n",
    "\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=train,shuffle=False)\n",
    "X_train1, X_rem1, y_train1, y_rem1 = train_test_split(X1,y1, train_size=train,shuffle=False)\n",
    "X_train2, X_rem2, y_train2, y_rem2 = train_test_split(X2,y2, train_size=train,shuffle=False)\n",
    "X_train3, X_rem3, y_train3, y_rem3 = train_test_split(X3,y3, train_size=train,shuffle=False)\n",
    "X_train4, X_rem4, y_train4, y_rem4 = train_test_split(X4,y4, train_size=train,shuffle=False)\n",
    "X_train5, X_rem5, y_train5, y_rem5 = train_test_split(X5,y5, train_size=train,shuffle=False)\n",
    "\n",
    "#validation=0.5\n",
    "#X_valid, X_rem, y_valid, y_rem = train_test_split(X_rem,y_rem, test_size=validation,shuffle=False)\n",
    "#X_valid1, X_rem1, y_valid1, y_rem1 = train_test_split(X_rem1,y_rem1, test_size=validation,shuffle=False)\n",
    "#X_valid2, X_rem2, y_valid2, y_rem2 = train_test_split(X_rem2,y_rem2, test_size=validation,shuffle=False)\n",
    "#X_valid3, X_rem3, y_valid3, y_rem3 = train_test_split(X_rem3,y_rem3, test_size=validation,shuffle=False)\n",
    "#X_valid4, X_rem4, y_valid4, y_rem4= train_test_split(X_rem4,y_rem4, test_size=validation,shuffle=False)\n",
    "#X_valid5, X_rem5, y_valid5, y_rem5 = train_test_split(X_rem5,y_rem5, test_size=validation,shuffle=False)\n",
    "\n",
    "#validation=0.65\n",
    "#X_valid, X_rem, y_valid, y_rem = train_test_split(X_rem,y_rem, test_size=validation,shuffle=False)\n",
    "#X_valid1, X_rem1, y_valid1, y_rem1 = train_test_split(X_rem1,y_rem1, test_size=validation,shuffle=False)\n",
    "#X_valid2, X_rem2, y_valid2, y_rem2 = train_test_split(X_rem2,y_rem2, test_size=validation,shuffle=False)\n",
    "#X_valid3, X_rem3, y_valid3, y_rem3 = train_test_split(X_rem3,y_rem3, test_size=validation,shuffle=False)\n",
    "#X_valid4, X_rem4, y_valid4, y_rem4= train_test_split(X_rem4,y_rem4, test_size=validation,shuffle=False)\n",
    "#X_valid5, X_rem5, y_valid5, y_rem5 = train_test_split(X_rem5,y_rem5, test_size=validation,shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.57,shuffle=False)\n",
    "X_valid1, X_test1, y_valid1, y_test1 = train_test_split(X_rem1,y_rem1, test_size=0.57,shuffle=False)\n",
    "X_valid2, X_test2, y_valid2, y_test2 = train_test_split(X_rem2,y_rem2, test_size=0.57,shuffle=False)\n",
    "X_valid3, X_test3, y_valid3, y_test3 = train_test_split(X_rem3,y_rem3, test_size=0.57,shuffle=False)\n",
    "X_valid4, X_test4, y_valid4, y_test4 = train_test_split(X_rem4,y_rem4, test_size=0.57,shuffle=False)\n",
    "X_valid5, X_test5, y_valid5, y_test5 = train_test_split(X_rem5,y_rem5, test_size=0.57,shuffle=False)\n",
    "\n",
    "\n",
    "X_train=np.concatenate((X_train, X_train1, X_train2,X_train3, X_train4,X_train5))\n",
    "X_valid=np.concatenate((X_valid, X_valid1, X_valid2,X_valid3, X_valid4,X_valid5))\n",
    "X_test=np.concatenate((X_test, X_test1, X_test2,X_test3, X_test4,X_test5))\n",
    "\n",
    "y_train=np.concatenate((y_train, y_train1, y_train2,y_train3, y_train4,y_train5))\n",
    "y_valid=np.concatenate((y_valid, y_valid1, y_valid2,y_valid3, y_valid4,y_valid5))\n",
    "y_test=np.concatenate((y_test, y_test1, y_test2,y_test3, y_test4,y_test5))\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns= xcol)\n",
    "X_valid = pd.DataFrame(X_valid, columns= xcol)\n",
    "X_test = pd.DataFrame(X_test, columns= xcol)\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns= ycol)\n",
    "y_valid = pd.DataFrame(y_valid, columns= ycol)\n",
    "y_test = pd.DataFrame(y_test, columns= ycol)\n",
    "\n",
    "#########333\n",
    "n=0\n",
    "u=0\n",
    "p=0\n",
    "j=0\n",
    "t=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Normal']\n",
      " ['Normal']\n",
      " ['Normal']\n",
      " ...\n",
      " ['Normal']\n",
      " ['Normal']\n",
      " ['Normal']]\n",
      "2381331\n",
      "-------------------------\n",
      "normal 54.43875714883819 and of 34.5121278814243\n",
      "udp 29.252590253097953 and of 29.252590253097953\n",
      "pluies 11.049114969737513 and of 11.049114969737513\n",
      "jam 5.259537628326344 and of 5.259537628326344\n",
      "438845\n",
      "-------------------------\n",
      "normal 54.98524535997904 and of 34.5310986794882\n",
      "udp 29.281864895350296 and of 29.281864895350296\n",
      "pluies 10.483655960532761 and of 10.483655960532761\n",
      "jam 5.249233784137908 and of 5.249233784137908\n",
      "581729\n",
      "-------------------------\n",
      "normal 54.595352818924276 and of 34.50438262489922\n",
      "udp 29.250561687658685 and of 29.250561687658685\n",
      "pluies 10.900264556176502 and of 10.900264556176502\n",
      "jam 5.253820937240536 and of 5.253820937240536\n",
      "3401905\n",
      "creating input data time 3.05771803855896\n",
      "263116\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "2381331\n",
      "438845\n",
      "581729\n",
      "tensor(1598574)\n",
      "tensor(289624)\n",
      "tensor(388668)\n",
      "19.030394077301025\n"
     ]
    }
   ],
   "source": [
    "y_train2=y_train.to_numpy()\n",
    "y_train22=y_valid.to_numpy()\n",
    "y_train222=y_test.to_numpy()\n",
    "print(y_train2)\n",
    "\n",
    "print(len(y_train2))\n",
    "print('-------------------------')\n",
    "for x ,i in enumerate(y_train2):\n",
    "    \n",
    "    if(i[-1]=='Normal'):\n",
    "        n+=1\n",
    "    if(i[-1]=='DDOS_UDP_FLOOD'):\n",
    "        u+=1\n",
    "        t+=1\n",
    "    if(i[-1]=='PLUIES_ET_ORAGES'):\n",
    "        p+=1 \n",
    "        #n+=1\n",
    "        #t+=1\n",
    "    if(i[-1]=='BROUILLAGE_Trafic'):\n",
    "        #u+=1\n",
    "        j+=1\n",
    "        t+=1\n",
    "\n",
    "print('normal {0} and of {1}'.format(n*100/len(y_train),t*100/len(y_train)))\n",
    "print('udp {0} and of {1}'.format(u*100/len(y_train),u*100/len(y_train)))\n",
    "print('pluies {0} and of {1}'.format(p*100/len(y_train),p*100/len(y_train)))\n",
    "print('jam {0} and of {1}'.format(j*100/len(y_train),j*100/len(y_train)))\n",
    "\n",
    "\n",
    "n=0;u=0;p=0;j=0;t=0\n",
    "\n",
    "print(len(y_train22))\n",
    "print('-------------------------')\n",
    "for x ,i in enumerate(y_train22):\n",
    "    \n",
    "    if(i[-1]=='Normal'):\n",
    "        n+=1\n",
    "    if(i[-1]=='DDOS_UDP_FLOOD'):\n",
    "        u+=1\n",
    "        t+=1\n",
    "    if(i[-1]=='PLUIES_ET_ORAGES'):\n",
    "        p+=1 \n",
    "        #n+=1\n",
    "        #t+=1\n",
    "    if(i[-1]=='BROUILLAGE_Trafic'):\n",
    "        #u+=1\n",
    "        j+=1\n",
    "        t+=1\n",
    "\n",
    "print('normal {0} and of {1}'.format(n*100/len(y_train22),t*100/len(y_train22)))\n",
    "print('udp {0} and of {1}'.format(u*100/len(y_train22),u*100/len(y_train22)))\n",
    "print('pluies {0} and of {1}'.format(p*100/len(y_train22),p*100/len(y_train22)))\n",
    "print('jam {0} and of {1}'.format(j*100/len(y_train22),j*100/len(y_train22)))\n",
    "\n",
    "n=0;u=0;p=0;j=0;t=0\n",
    "\n",
    "print(len(y_train222))\n",
    "print('-------------------------')\n",
    "for x ,i in enumerate(y_train222):\n",
    "    \n",
    "    if(i[-1]=='Normal'):\n",
    "        n+=1\n",
    "    if(i[-1]=='DDOS_UDP_FLOOD'):\n",
    "        u+=1\n",
    "        t+=1\n",
    "    if(i[-1]=='PLUIES_ET_ORAGES'):\n",
    "        p+=1 \n",
    "        #n+=1\n",
    "        #t+=1\n",
    "    if(i[-1]=='BROUILLAGE_Trafic'):\n",
    "        #u+=1\n",
    "        j+=1\n",
    "        t+=1\n",
    "\n",
    "print('normal {0} and of {1}'.format(n*100/len(y_train222),t*100/len(y_train222)))\n",
    "print('udp {0} and of {1}'.format(u*100/len(y_train222),u*100/len(y_train222)))\n",
    "print('pluies {0} and of {1}'.format(p*100/len(y_train222),p*100/len(y_train222)))\n",
    "print('jam {0} and of {1}'.format(j*100/len(y_train222),j*100/len(y_train222)))\n",
    "\n",
    "########\n",
    "\n",
    "print(int(len(y_train))+int(len(y_valid))+int(len(y_test)))\n",
    "print('creating input data time '+str(time.time()-a))\n",
    "\n",
    "\n",
    "\n",
    "a-time.time()\n",
    "X_train1=X_train.values\n",
    "y_train1=y_train.values\n",
    "#X_train1=X_train1.astype(float)\n",
    "\n",
    "torch_tensor = torch.tensor(X_train1)\n",
    "#torch_tensor[torch_tensor != torch_tensor]=float(0)\n",
    "t=[]\n",
    "test=0\n",
    "test0=0\n",
    "\n",
    "for j in y_train1:\n",
    "    i=j[0]\n",
    "#    print(i)\n",
    "    if (i=='Normal'):\n",
    "        t.append(int(0))\n",
    "        test0+=1\n",
    "    #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        t.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        t.append(int(2))\n",
    "        test+=1\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        t.append(int(3))\n",
    "print(test)\n",
    "labels = torch.LongTensor(t)\n",
    "#print(torch_tensor[-1])\n",
    "#print(labels[-1])\n",
    "print(labels)\n",
    "#print(df_label)\n",
    "#print(labels.shape)\n",
    "\n",
    "#df00 = pd.read_csv(path0+\"small/testbed3/Dataset_S03Filtrer.csv\")\n",
    "#bigX = df00[['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','droppedPKWrongPort','sentPK','size','channel','throughput']]         \n",
    "#bigY = df00['label']\n",
    "bigX = X_valid\n",
    "#[['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','droppedPKWrongPort','sentPK','size','throughput']]  ,'channel'       \n",
    "bigY = y_valid['label']\n",
    "#print(bigY)\n",
    "X_valid1=bigX.values\n",
    "y_valid1=bigY.values\n",
    "\n",
    "#X_valid1=X_test.values\n",
    "#y_valid1=y_test.values\n",
    "#X_train1=X_train1.astype(float)\n",
    "\n",
    "vtorch_tensor = torch.tensor(X_valid1)\n",
    "v=[]\n",
    "\n",
    "for i in y_valid1:\n",
    "    #i=j[0]\n",
    "    #print(i)\n",
    "    if (i=='Normal'):\n",
    "        v.append(int(0))\n",
    "        #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        v.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        v.append(int(2))\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        v.append(int(3))\n",
    "\n",
    "vlabels = torch.LongTensor(v)\n",
    "\n",
    "\n",
    "X_test1=X_test.values\n",
    "y_test1=y_test.values\n",
    "\n",
    "#X_train1=X_train1.astype(float)\n",
    "\n",
    "ttorch_tensor = torch.tensor(X_test1)\n",
    "\n",
    "#torch_tensor[torch_tensor != torch_tensor]=float(0)\n",
    "t=[]\n",
    "\n",
    "for j in y_test1:\n",
    "    i=j[0]\n",
    "#    print(i)\n",
    "    if (i=='Normal'):\n",
    "        t.append(int(0))\n",
    "        #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        t.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        t.append(int(2))\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        t.append(int(3))\n",
    "\n",
    "tlabels = torch.LongTensor(t)\n",
    "#print(torch_tensor[-1])\n",
    "#print(labels[-1])\n",
    "print(len(labels))\n",
    "print(len(vlabels))\n",
    "print(len(tlabels))\n",
    "\n",
    "del(X)\n",
    "del(y)\n",
    "del(X1)\n",
    "del(y1)\n",
    "del(X2)\n",
    "del(y2)\n",
    "del(X3)\n",
    "del(y3)\n",
    "del(X4)\n",
    "del(y4)\n",
    "del(X5)\n",
    "del(y5)\n",
    "del(X_train1); del(X_train2);del(X_train3); del(X_train4);del(X_train5)\n",
    "del(y_train1); del(y_train2);del(y_train3); del(y_train4);del(y_train5)\n",
    "del(y_rem);del(y_rem1); del(y_rem2);del(y_rem3); del(y_rem4);del(y_rem5)\n",
    "del(X_rem);del(X_rem1); del(X_rem2);del(X_rem3); del(X_rem4);del(X_rem5)\n",
    "\n",
    "#X_rem1, y_train1, y_rem1\n",
    "del(X_train)\n",
    "del(X_valid)\n",
    "del(X_valid1); del(X_valid2);del(X_valid3); del(X_valid4);del(X_valid5)\n",
    "del(y_valid1); del(y_valid2);del(y_valid3); del(y_valid4);del(y_valid5)\n",
    "\n",
    "\n",
    "del(X_test)\n",
    "del(X_test1); del(X_test2);del(X_test3); del(X_test4);del(X_test5)\n",
    "del(y_test1); del(y_test2);del(y_test3); del(y_test4);del(y_test5)\n",
    "\n",
    "del(y_train)\n",
    "del(y_valid)\n",
    "del(y_test)\n",
    "del(t)\n",
    "del(v)\n",
    "\n",
    "inputs=torch_tensor[:,:] \n",
    "labels1=labels[:]\n",
    "del(torch_tensor)\n",
    "del(labels)\n",
    "#print(len(inputs))\n",
    "#print(len(labels1))\n",
    "vinputs =vtorch_tensor[:,:]\n",
    "vlabels1=vlabels[:]\n",
    "del(vtorch_tensor)\n",
    "del(vlabels)\n",
    "tinputs =ttorch_tensor[:,:] \n",
    "tlabels1=tlabels[:]\n",
    "del(ttorch_tensor)\n",
    "del(tlabels)\n",
    "\n",
    "print(sum(labels1))\n",
    "print(sum(vlabels1))\n",
    "print(sum(tlabels1))\n",
    "\n",
    " \n",
    "print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "USE_GPU = True\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "# input_size = 784 # 28x28\n",
    "num_classes = 4\n",
    "#num_epochs = 2\n",
    "batch_size = 1\n",
    "\n",
    "input_size = 13\n",
    "sequence_length = 28\n",
    "hidden_size = 132\n",
    "num_layers = 2\n",
    "\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(13, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 140),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(140, 200),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(200, 32),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(32, 4),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "#criterion = nn.NLLLoss()\n",
    "Fonction_de_perte = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "#indices = [0,3, 299:303]\n",
    "\n",
    "#criterion = nn.NLLLoss()\n",
    "#Fonction_de_perte = nn.NLLLoss()\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.006)\n",
    "\n",
    "#indices = [0,3, 299:303]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  : valid accuracy :5.24923372 with total prob : 26.56209564 \n",
      "Epoch 0 : Training loss 0.00032674 and valid loss : 1.14141771 :  \n",
      "Epoch 0 : valid accuracy :60.14765930 with total prob : 98.47805786 \n",
      "194.97366404533386\n",
      "validation loss decreased , saving model (inf ==> 1.14141771)\n"
     ]
    }
   ],
   "source": [
    "#labels1=labels[loc]\n",
    "a=time.time()\n",
    "#inputs=torch_tensor[:,:].to(device) \n",
    "##labels1=labels[:].to(device)\n",
    "#print(len(inputs))\n",
    "#print(len(labels1))\n",
    "#vinputs =vtorch_tensor[:,:].to(device)\n",
    "#vlabels1=vlabels[:].to(device)\n",
    "\n",
    "epochs = 200\n",
    "valid_loss=0\n",
    "accuracy=0\n",
    "valid_loss_min=np.Inf\n",
    "batch_size = 50\n",
    "\n",
    "####33  !!!!!!!!!! This shit cost alot of time ----> do not use it \n",
    "#b=time.time()\n",
    "#with torch.no_grad():\n",
    "   # for i, x in enumerate(vinputs):\n",
    "       # x2=x[None,:]\n",
    "       # output = torch.exp(model(x2))\n",
    "       # l2=vlabels1[i][None]\n",
    "       # test_loss+=Fonction_de_perte(output, l2)\n",
    "     #   output = torch.exp(output)\n",
    "      #  top_p , top_c = output.topk(1, dim=1)\n",
    "     #   equals = top_c==vlabels1[i].view(*top_c.shape)\n",
    "    #    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "   # tain_losses.append(running_loss/len(inputs))\n",
    "   # test_losses.append(test_loss/len(vinputs))\n",
    "   # print ('test accuracy :{0}, test loss : {2} ,  time {1} '.format(accuracy*100/len(vinputs) , time.time()-b ,test_loss/len(vinputs) ))\n",
    "\n",
    "######## ---> use this :) \n",
    "b=time.time()\n",
    "with torch.no_grad():\n",
    "    output = torch.exp(model(vinputs))\n",
    "    valid_loss=Fonction_de_perte(output, vlabels1)\n",
    "top_p , top_c = output.topk(1, dim=1)\n",
    "del(output)\n",
    "propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "equals = top_c==vlabels1.view(*top_c.shape)\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "#print ('valid accuracy :{0:.8f} with total prob : {3:.8f} and  valid loss : {2:.8f} ,  time {1:.6f} '.format(accuracy*100 , time.time()-b ,test_loss,propabilities))\n",
    "print ('Epoch  : valid accuracy :{0:.8f} with total prob : {1:.8f} '.format(accuracy*100  ,propabilities))\n",
    "\n",
    "####\n",
    "#print(inputs.shape[0])\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "for e in range(epochs):\n",
    "    c=time.time()\n",
    "    running_loss = 0\n",
    "    #for i, x in enumerate(inputs):\n",
    "    for i in range(0,inputs.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #indices = [i:i+batch_size]\n",
    "        #print(i+batch_size)\n",
    "        batch_x, batch_y = inputs[i:i+batch_size], labels1[i:i+batch_size]\n",
    "        \n",
    "        #x2=x[None,:]\n",
    "        output = model.forward(batch_x)\n",
    "        \n",
    "        #output = model.forward(x2)\n",
    "        #l2=labels1[i][None]\n",
    "        #print(l2)\n",
    "        #loss = Fonction_de_perte(output, l2)\n",
    "        loss = Fonction_de_perte(output, batch_y)\n",
    "      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "        accuracy=0\n",
    "        #print(l2)\n",
    "  \n",
    "        b=time.time()\n",
    "        with torch.no_grad():\n",
    "            ##\n",
    "            #with torch.no_grad():\n",
    "            #for i, x in enumerate(vinputs):\n",
    "             #   x2=x[None,:]\n",
    "              #  output = torch.exp(model(x2))\n",
    "               # l2=vlabels1[i][None]\n",
    "                #valid_loss1+=Fonction_de_perte(output, l2)\n",
    "            #test_loss=test_loss/len(vinputs)\n",
    "            ##\n",
    "                \n",
    "            model.eval()\n",
    "            output = torch.exp(model(vinputs))\n",
    "            valid_loss=Fonction_de_perte(output, vlabels1)\n",
    "        top_p , top_c = output.topk(1, dim=1)\n",
    "        del(output)\n",
    "        propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "        equals = top_c==vlabels1.view(*top_c.shape)\n",
    "        accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        print('Epoch {2} : Training loss {0:.8f} and valid loss : {1:.8f} :  '.format(running_loss/len(inputs),valid_loss, e))\n",
    "   \n",
    "        print ('Epoch {2} : valid accuracy :{0:.8f} with total prob : {1:.8f} '.format(accuracy*100  ,propabilities, e))\n",
    "        print(time.time()-c)\n",
    "    #print('validation loss with iterations {0}'.format(valid_loss1/len(vinputs) ) )   \n",
    "        train_losses.append(running_loss/len(inputs))\n",
    "        valid_losses.append(valid_loss)\n",
    "        if (valid_loss<valid_loss_min):\n",
    "            print('validation loss decreased , saving model ({:.8f} ==> {:.8f})'.format(valid_loss_min,valid_loss))\n",
    "            torch.save(model.state_dict(),'3mod_temp_5_mlp.pth')\n",
    "            valid_loss_min=valid_loss\n",
    "print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "#test_losses_scaled = scaler.fit_transform(test_losses)\n",
    "\n",
    "\n",
    "\n",
    "Tr=np.array(train_losses)\n",
    "Te=np.array(valid_losses)\n",
    "\n",
    "Tr=np.reshape(Tr, (len(Tr),1))\n",
    "Te=np.reshape(Te, (len(Te),1))\n",
    "\n",
    "# fit on training data column\n",
    "scale = StandardScaler().fit(Tr)\n",
    "tain_losses = scale.transform(Tr)\n",
    "scale = StandardScaler().fit(Te)\n",
    "test_losses = scale.transform(Te)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "#test_losses_scaled = scaler.fit_transform(test_losses)\n",
    "plt.ylim([-1,1])\n",
    "\n",
    "plt.plot(tain_losses, label='Train loss')\n",
    "plt.plot(test_losses, label='valid loss')\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4d6fab8d841a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msummm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msum1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msum2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msum3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "a=time.time()\n",
    "summm=[]\n",
    "sum1=[]\n",
    "sum2=[]\n",
    "sum3=[]\n",
    "sum4=[]\n",
    "\n",
    "\n",
    "#tinputs =torch_tensor[:,:] \n",
    "#tlabels1=labels[:]\n",
    "#del(output);\n",
    "#del(top_p);del(top_c);del(propabilities)\n",
    "#del(equals)\n",
    "print('=====================')\n",
    "b=time.time()\n",
    "model.eval()\n",
    "output = torch.exp(model(tinputs))\n",
    "test_loss=Fonction_de_perte(output, tlabels1)\n",
    "top_p , top_c = output.topk(1, dim=1)\n",
    "del(output)\n",
    "\n",
    "#####\n",
    "######\n",
    "propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "equals = top_c==tlabels1.view(*top_c.shape)\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "model.train()\n",
    "\n",
    "####\n",
    "###\n",
    "print ('test accuracy :{0:.8f} with total prob : {3:.8f} and  test loss : {2:.8f} ,  time {1:.8f} '.format(accuracy*100 , time.time()-b ,test_loss ,propabilities))\n",
    "class_correct = list(0. for i in range (4))\n",
    "class_total = list(0. for i in range (4))\n",
    "\n",
    "C_n_udp=0\n",
    "C_n_pluies=0\n",
    "C_n_jam=0\n",
    "C_n_total=0\n",
    "\n",
    "\n",
    "C_u_normal=0\n",
    "C_u_jam=0\n",
    "C_u_pluies=0\n",
    "C_u_total=0\n",
    "\n",
    "C_p_normal=0\n",
    "C_p_udp=0\n",
    "C_p_jam=0\n",
    "C_p_total=0\n",
    "\n",
    "C_j_normal=0\n",
    "C_j_udp=0\n",
    "C_j_pluies=0\n",
    "C_j_total=0\n",
    "\n",
    "for i, x in enumerate(tinputs):\n",
    "    optimizer.zero_grad()\n",
    "    x2=x[None,:]\n",
    "    with torch.no_grad():\n",
    "        output = torch.exp(model(x2))\n",
    "    out=output.detach().numpy()*100\n",
    " \n",
    "    l3=tlabels1[i].item()\n",
    "  \n",
    "    if(l3==0):\n",
    "        summm.append(out[0][0])\n",
    "        sum1.append(out[0][0])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_n_udp+=1\n",
    "            if(top_c[i][0]==2):\n",
    "                C_n_pluies+=1\n",
    "            if(top_c[i][0]==3):\n",
    "                C_n_jam +=1\n",
    "            \n",
    "            C_n_total +=1\n",
    "            \n",
    "    if(l3==1):\n",
    "        summm.append(out[0][1])\n",
    "        sum2.append(out[0][1])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==3):\n",
    "                C_u_jam+=1\n",
    "            if(top_c[i][0]==2):\n",
    "                C_u_pluies+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_u_normal +=1\n",
    "            \n",
    "            C_u_total +=1\n",
    "\n",
    "    if(l3==2):\n",
    "        summm.append(out[0][2])\n",
    "        sum3.append(out[0][2])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_p_udp+=1\n",
    "            if(top_c[i][0]==3):\n",
    "                C_p_jam+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_p_normal +=1\n",
    "            \n",
    "            C_p_total +=1\n",
    "\n",
    "    if(l3==3):\n",
    "        summm.append(out[0][3])\n",
    "        sum4.append(out[0][3])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_j_udp+=1\n",
    "            if(top_c[i][0]==2):\n",
    "                C_j_pluies+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_j_normal +=1\n",
    "            \n",
    "            C_j_total +=1\n",
    "\n",
    "    class_total[l3]+=1\n",
    "    class_correct[l3]+=equals[i][0]\n",
    "    #print(output)\n",
    "print('la précision de detection globale: {0} '.format(mean(summm)))\n",
    "print('detection des communication normal: {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum1), class_correct[0]*100/class_total[0] ,class_correct[0] ,class_total[0]))\n",
    "if(C_n_total!=0):\n",
    "    print('details normal classed udp :{0:.5f}, pluies {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_n_udp*100/class_total[0],C_n_pluies*100/class_total[0],C_n_jam*100/class_total[0],C_n_udp*100/C_n_total,C_n_pluies*100/C_n_total,C_n_jam*100/C_n_total))\n",
    "else:\n",
    "    print('detection normal 100')\n",
    "\n",
    "print('detection du deni de service par udp flood {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum2), class_correct[1]*100/class_total[1] ,class_correct[1] ,class_total[1]))\n",
    "if(C_u_total!=0):\n",
    "    print('details udp classed normal :{0:.5f}, pluies {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_u_normal*100/class_total[1],C_u_pluies*100/class_total[1],C_u_jam*100/class_total[1],C_u_normal*100/C_u_total,C_u_pluies*100/C_u_total,C_u_jam*100/C_u_total))\n",
    "else:\n",
    "    print('detection udp flood 100')\n",
    "    \n",
    "print('detection du deni naturel : pluies et orages : {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum3), class_correct[2]*100/class_total[2] ,class_correct[2] ,class_total[2]))\n",
    "if(C_p_total!=0):\n",
    "    print('details pluies classed udp :{0:.5f}, normal {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_p_udp*100/class_total[2],C_p_normal*100/class_total[2],C_p_jam*100/class_total[2],C_p_udp*100/C_p_total,C_p_normal*100/C_p_total,C_p_jam*100/C_p_total))\n",
    "else:\n",
    "    print('detection pluies et orages 100')\n",
    "\n",
    "print('detection du deni naturel : jam : {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum4), class_correct[3]*100/class_total[3] ,class_correct[3] ,class_total[3]))\n",
    "if(C_j_total!=0):\n",
    "    print('details jam classed udp :{0:.5f}, pluies {1:.5f} , normal {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_j_udp*100/class_total[3],C_j_pluies*100/class_total[3],C_j_normal*100/class_total[3],C_j_udp*100/C_j_total,C_j_pluies*100/C_j_total,C_j_normal*100/C_j_total))\n",
    "else:\n",
    "    print('detection brouillage 100')\n",
    "print('=====================')\n",
    "\n",
    "print('<================================>')\n",
    "print('Total_time')\n",
    "print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0485f8cdad47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath0\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"FinaldatasetN.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'improt time '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#df=df[['channel','Next_Current_diff','Next_Pre_diff','size', 'packet_type','DataQueueLen','lossRate','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(path0+\"FinaldatasetN.csv\")\n",
    "print('improt time '+str(time.time()-a))\n",
    "\n",
    "a=time.time()\n",
    "#df=df[['channel','Next_Current_diff','Next_Pre_diff','size', 'packet_type','DataQueueLen','lossRate','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "#df=df[['channel','Next_Current_diff','Next_Pre_diff','size', 'packet_type','lossRate','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "df=df[['channel','Next_Current_diff','Next_Pre_diff','size','snir','throughput','packet_type','SNext_Current_diff','SNext_Pre_diff','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "\n",
    "\n",
    "#df=df.to_numpy()\n",
    "df_Normal=df[0:1000000].copy()\n",
    "#print(df_Normal[0][1])\n",
    "#print(df_Normal[-1][1])\n",
    "df_UDP1=df[1000000:2000000].copy()\n",
    "#print(df_UDP1[0][1])\n",
    "#print(df_UDP1[-1][1])\n",
    "df_UDP2=df[2000000:3000000].copy()\n",
    "#print(df_UDP2[0][1])\n",
    "#print(df_UDP2[-1][1])\n",
    "df_jam1=df[3000000:4000000].copy()\n",
    "#print(df_jam1[0][1])\n",
    "#print(df_jam1[-1][1])\n",
    "df_jam2=df[2000000:2500000].copy()\n",
    "#print(df_jam2[0][1])\n",
    "#print(df_jam2[-1][1])\n",
    "df_pluies=df[2500000:3000000].copy()\n",
    "#print(df_pluies[0][1])\n",
    "#print(df_pluies[-1][1])\n",
    "del(df)\n",
    "\n",
    "X=df_Normal.drop(columns = ['label']).copy()\n",
    "y=df_Normal[['label']].copy()\n",
    "del(df_Normal)\n",
    "X=X.values\n",
    "y=y.values\n",
    "X = torch.tensor(X)\n",
    "#torch_tensor[torch_tensor != torch_tensor]=float(0)\n",
    "def mttest(X,y):\n",
    "    t=[]\n",
    "    for j in y:\n",
    "        i=j[0]\n",
    "    #    print(i)\n",
    "        if (i=='Normal'):\n",
    "            t.append(int(0))\n",
    "        #t.append(int(1))\n",
    "        if (i=='DDOS_UDP_FLOOD'):\n",
    "            t.append(int(1))\n",
    "        if (i=='PLUIES_ET_ORAGES'):\n",
    "            t.append(int(0))\n",
    "        if (i=='BROUILLAGE_Trafic'):\n",
    "            t.append(int(1))\n",
    "    y = torch.LongTensor(t)\n",
    "    model.eval()\n",
    "    output = torch.exp(model(X))\n",
    "\n",
    "    test_loss=Fonction_de_perte(output, y)\n",
    "    top_p , top_c = output.topk(1, dim=1)\n",
    "    del(output)\n",
    "    model.eval()\n",
    "    propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "    equals = top_c==y.view(*top_c.shape)\n",
    "    accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "    print ('test accuracy :{0:.8f} with total prob : {3:.8f} and  test loss : {2:.8f} ,  time {1:.8f} '.format(accuracy*100 , time.time()-b ,test_loss ,propabilities))\n",
    "    del(X)\n",
    "    del(y)\n",
    "####\n",
    "\n",
    "mttest(X,y)\n",
    "del(X);del(y)\n",
    "X1=df_UDP1.drop(columns = ['label']).copy()\n",
    "y1=df_UDP1[['label']].copy()\n",
    "del(df_UDP1)\n",
    "X1=X1.values\n",
    "y1=y1.values\n",
    "X1 = torch.tensor(X1)\n",
    "mttest(X1,y1)\n",
    "del(X1);del(y1)\n",
    "\n",
    "X2=df_UDP2.drop(columns = ['label']).copy()\n",
    "y2=df_UDP2[['label']].copy()\n",
    "del(df_UDP2)\n",
    "X2=X2.values\n",
    "y2=y2.values\n",
    "X2 = torch.tensor(X2)\n",
    "mttest(X2,y2)\n",
    "del(X2);del(y2)\n",
    "\n",
    "X3=df_jam1.drop(columns = ['label']).copy()\n",
    "y3=df_jam1[['label']].copy()\n",
    "del(df_jam1)\n",
    "X3=X3.values\n",
    "y3=y3.values\n",
    "X3 = torch.tensor(X3)\n",
    "mttest(X3,y3)\n",
    "del(X3);del(y3)\n",
    "\n",
    "#X4=df_jam2.drop(columns = ['label']).copy()\n",
    "#y4=df_jam2[['label']].copy()\n",
    "#X5=df_pluies.drop(columns = ['label']).copy()\n",
    "#y5=df_pluies[['label']].copy()\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104082, 18)\n",
      "(104082, 1)\n",
      "(34694, 18)\n",
      "(34694, 1)\n",
      "(34694, 18)\n",
      "(34694, 1)\n",
      "end\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####starting agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2228171946511422\n",
      "Training loss: 0.10734075619802586\n",
      "Training loss: 0.061076861022234794\n",
      "Training loss: 0.04849367666824681\n",
      "Training loss: 0.047760097768268985\n",
      "Training loss: 0.04205877098648349\n",
      "Training loss: 0.04427055889282852\n",
      "Training loss: 0.04313506641758868\n",
      "Training loss: 0.04226361174081555\n",
      "Training loss: 0.04112569277969371\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la précision de detection globale: \n",
      "96.7123281899267\n",
      "detection des communication normal:\n",
      "98.89724781642047\n",
      "detection du deni de service par udp flood :\n",
      "99.99732103069512\n",
      "detection du deni naturel : pluies et orages :\n",
      "89.48570418509557\n",
      "detection du deni naturel : jam :\n",
      "86.53281766175175\n",
      "=====================\n",
      "time\n",
      "765.0322959423065\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pourcentge du trafic normal :59.453586205955666\n",
      "pourcentge du trafic udp_flood :23.12923048737943\n",
      "pourcentge du trafic dans un mauvais temps :14.837523947993631\n",
      "pourcentge du trafic vicitme d' brouillage :2.579659358671269\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-70-b734b4a253c9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-70-b734b4a253c9>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pourcentge du trafic normal :21.765924563089197\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280.000421818582\n",
      "487.2262257424459\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280.000421818582\n",
      "694.4520296663097\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003650631698\n",
      "694.4520296663097\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

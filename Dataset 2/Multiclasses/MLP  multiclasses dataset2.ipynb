{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-01T13:12:14.871083Z",
     "iopub.status.busy": "2021-09-01T13:12:14.870537Z",
     "iopub.status.idle": "2021-09-01T13:12:49.245583Z",
     "shell.execute_reply": "2021-09-01T13:12:49.24409Z",
     "shell.execute_reply.started": "2021-09-01T13:12:14.870937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improt time 15.502544164657593\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import os\n",
    "\n",
    "a=time.time()\n",
    "\n",
    "path0 ='../' \n",
    "df = pd.read_csv(path0+\"FinaldatasetN1.csv\")\n",
    "print('improt time '+str(time.time()-a))\n",
    "\n",
    "a=time.time()\n",
    "#df=df[['channel','Next_Current_diff','Next_Pre_diff','size', 'packet_type','DataQueueLen','lossRate','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "#df=df[['channel','Next_Current_diff','Next_Pre_diff','size', 'packet_type','lossRate','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "df=df[['channel','Next_Current_diff','Next_Pre_diff','size','packet_type','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "#df=df[['channel','Next_Current_diff','Next_Pre_diff','size','packet_type','SNext_Current_diff','SNext_Pre_diff','snir','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "#df1=df[['channel','Next_Current_diff','Next_Pre_diff','size','packet_type','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']].copy()     \n",
    "\n",
    "\n",
    "#df=df.to_numpy()\n",
    "#df_Normal=df[200000:354961].copy()\n",
    "df_Normal=df[0:354961].copy()\n",
    "\n",
    "#print(df_Normal[0][1])\n",
    "#print(df_Normal[-1][1])\n",
    "df_UDP1=df[354961:763730].copy()\n",
    "#print(df_UDP1[0][1])\n",
    "#print(df_UDP1[-1][1])\n",
    "df_UDP2=df[763730:1573000].copy()\n",
    "#print(df_UDP2[0][1])\n",
    "#print(df_UDP2[-1][1])\n",
    "#df_jam1=df[1573000:1696992].copy()\n",
    "df_jam1=df[1573000:1696992].copy()\n",
    "\n",
    "#print(df_jam1[0][1])\n",
    "#print(df_jam1[-1][1])\n",
    "df_jam2=df[1696992:2714727].copy()\n",
    "#print(df_jam2[0][1])\n",
    "#print(df_jam2[-1][1])\n",
    "df_pluies=df[2714727:-1].copy()\n",
    "#print(df_pluies[0][1])\n",
    "#print(df_pluies[-1][1])\n",
    "\n",
    "X=df_Normal.drop(columns = ['label']).copy()\n",
    "y=df_Normal[['label']].copy()\n",
    "#print(y.columns)\n",
    "X1=df_UDP1.drop(columns = ['label']).copy()\n",
    "y1=df_UDP1[['label']].copy()\n",
    "X2=df_UDP2.drop(columns = ['label']).copy()\n",
    "y2=df_UDP2[['label']].copy()\n",
    "X3=df_jam1.drop(columns = ['label']).copy()\n",
    "y3=df_jam1[['label']].copy()\n",
    "X4=df_jam2.drop(columns = ['label']).copy()\n",
    "y4=df_jam2[['label']].copy()\n",
    "X5=df_pluies.drop(columns = ['label']).copy()\n",
    "y5=df_pluies[['label']].copy()\n",
    "\n",
    "train=0.7\n",
    "#train=0.01\n",
    "\n",
    "del(df_Normal)\n",
    "del(df_UDP1)\n",
    "del(df_UDP2)\n",
    "del(df_jam1)\n",
    "del(df_jam2)\n",
    "del(df_pluies)\n",
    "\n",
    "\n",
    "xcol=X.columns\n",
    "ycol=y.columns\n",
    "\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=train,shuffle=False)\n",
    "X_train1, X_rem1, y_train1, y_rem1 = train_test_split(X1,y1, train_size=train,shuffle=False)\n",
    "X_train2, X_rem2, y_train2, y_rem2 = train_test_split(X2,y2, train_size=train,shuffle=False)\n",
    "X_train3, X_rem3, y_train3, y_rem3 = train_test_split(X3,y3, train_size=train,shuffle=False)\n",
    "X_train4, X_rem4, y_train4, y_rem4 = train_test_split(X4,y4, train_size=train,shuffle=False)\n",
    "X_train5, X_rem5, y_train5, y_rem5 = train_test_split(X5,y5, train_size=train,shuffle=False)\n",
    "\n",
    "#validation=0.5\n",
    "#X_valid, X_rem, y_valid, y_rem = train_test_split(X_rem,y_rem, test_size=validation,shuffle=False)\n",
    "#X_valid1, X_rem1, y_valid1, y_rem1 = train_test_split(X_rem1,y_rem1, test_size=validation,shuffle=False)\n",
    "#X_valid2, X_rem2, y_valid2, y_rem2 = train_test_split(X_rem2,y_rem2, test_size=validation,shuffle=False)\n",
    "#X_valid3, X_rem3, y_valid3, y_rem3 = train_test_split(X_rem3,y_rem3, test_size=validation,shuffle=False)\n",
    "#X_valid4, X_rem4, y_valid4, y_rem4= train_test_split(X_rem4,y_rem4, test_size=validation,shuffle=False)\n",
    "#X_valid5, X_rem5, y_valid5, y_rem5 = train_test_split(X_rem5,y_rem5, test_size=validation,shuffle=False)\n",
    "\n",
    "#validation=0.65\n",
    "#X_valid, X_rem, y_valid, y_rem = train_test_split(X_rem,y_rem, test_size=validation,shuffle=False)\n",
    "#X_valid1, X_rem1, y_valid1, y_rem1 = train_test_split(X_rem1,y_rem1, test_size=validation,shuffle=False)\n",
    "#X_valid2, X_rem2, y_valid2, y_rem2 = train_test_split(X_rem2,y_rem2, test_size=validation,shuffle=False)\n",
    "#X_valid3, X_rem3, y_valid3, y_rem3 = train_test_split(X_rem3,y_rem3, test_size=validation,shuffle=False)\n",
    "#X_valid4, X_rem4, y_valid4, y_rem4= train_test_split(X_rem4,y_rem4, test_size=validation,shuffle=False)\n",
    "#X_valid5, X_rem5, y_valid5, y_rem5 = train_test_split(X_rem5,y_rem5, test_size=validation,shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.57,shuffle=False)\n",
    "X_valid1, X_test1, y_valid1, y_test1 = train_test_split(X_rem1,y_rem1, test_size=0.57,shuffle=False)\n",
    "X_valid2, X_test2, y_valid2, y_test2 = train_test_split(X_rem2,y_rem2, test_size=0.57,shuffle=False)\n",
    "X_valid3, X_test3, y_valid3, y_test3 = train_test_split(X_rem3,y_rem3, test_size=0.57,shuffle=False)\n",
    "X_valid4, X_test4, y_valid4, y_test4 = train_test_split(X_rem4,y_rem4, test_size=0.57,shuffle=False)\n",
    "X_valid5, X_test5, y_valid5, y_test5 = train_test_split(X_rem5,y_rem5, test_size=0.57,shuffle=False)\n",
    "\n",
    "\n",
    "X_train=np.concatenate((X_train, X_train1, X_train2,X_train3, X_train4,X_train5))\n",
    "X_valid=np.concatenate((X_valid, X_valid1, X_valid2,X_valid3, X_valid4,X_valid5))\n",
    "X_test=np.concatenate((X_test, X_test1, X_test2,X_test3, X_test4,X_test5))\n",
    "\n",
    "y_train=np.concatenate((y_train, y_train1, y_train2,y_train3, y_train4,y_train5))\n",
    "y_valid=np.concatenate((y_valid, y_valid1, y_valid2,y_valid3, y_valid4,y_valid5))\n",
    "y_test=np.concatenate((y_test, y_test1, y_test2,y_test3, y_test4,y_test5))\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns= xcol)\n",
    "X_valid = pd.DataFrame(X_valid, columns= xcol)\n",
    "X_test = pd.DataFrame(X_test, columns= xcol)\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns= ycol)\n",
    "y_valid = pd.DataFrame(y_valid, columns= ycol)\n",
    "y_test = pd.DataFrame(y_test, columns= ycol)\n",
    "\n",
    "#########333\n",
    "n=0\n",
    "u=0\n",
    "p=0\n",
    "j=0\n",
    "t=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T13:12:49.248158Z",
     "iopub.status.busy": "2021-09-01T13:12:49.247782Z",
     "iopub.status.idle": "2021-09-01T13:13:19.200522Z",
     "shell.execute_reply": "2021-09-01T13:13:19.199168Z",
     "shell.execute_reply.started": "2021-09-01T13:12:49.248118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Normal']\n",
      " ['Normal']\n",
      " ['Normal']\n",
      " ...\n",
      " ['Normal']\n",
      " ['Normal']\n",
      " ['Normal']]\n",
      "2381331\n",
      "-------------------------\n",
      "normal 54.43875714883819 and of 34.5121278814243\n",
      "udp 29.252590253097953 and of 29.252590253097953\n",
      "pluies 11.049114969737513 and of 11.049114969737513\n",
      "jam 5.259537628326344 and of 5.259537628326344\n",
      "438845\n",
      "-------------------------\n",
      "normal 54.98524535997904 and of 34.5310986794882\n",
      "udp 29.281864895350296 and of 29.281864895350296\n",
      "pluies 10.483655960532761 and of 10.483655960532761\n",
      "jam 5.249233784137908 and of 5.249233784137908\n",
      "581729\n",
      "-------------------------\n",
      "normal 54.595352818924276 and of 34.50438262489922\n",
      "udp 29.250561687658685 and of 29.250561687658685\n",
      "pluies 10.900264556176502 and of 10.900264556176502\n",
      "jam 5.253820937240536 and of 5.253820937240536\n",
      "3401905\n",
      "creating input data time 6.6892359256744385\n",
      "263116\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "581729\n",
      "581729\n",
      "2381331\n",
      "438845\n",
      "581729\n",
      "tensor(1598574)\n",
      "tensor(289624)\n",
      "tensor(388668)\n",
      "59.418376445770264\n"
     ]
    }
   ],
   "source": [
    "y_train2=y_train.to_numpy()\n",
    "y_train22=y_valid.to_numpy()\n",
    "y_train222=y_test.to_numpy()\n",
    "print(y_train2)\n",
    "\n",
    "print(len(y_train2))\n",
    "print('-------------------------')\n",
    "for x ,i in enumerate(y_train2):\n",
    "    \n",
    "    if(i[-1]=='Normal'):\n",
    "        n+=1\n",
    "    if(i[-1]=='DDOS_UDP_FLOOD'):\n",
    "        u+=1\n",
    "        t+=1\n",
    "    if(i[-1]=='PLUIES_ET_ORAGES'):\n",
    "        p+=1 \n",
    "        #n+=1\n",
    "        #t+=1\n",
    "    if(i[-1]=='BROUILLAGE_Trafic'):\n",
    "        #u+=1\n",
    "        j+=1\n",
    "        t+=1\n",
    "\n",
    "print('normal {0} and of {1}'.format(n*100/len(y_train),t*100/len(y_train)))\n",
    "print('udp {0} and of {1}'.format(u*100/len(y_train),u*100/len(y_train)))\n",
    "print('pluies {0} and of {1}'.format(p*100/len(y_train),p*100/len(y_train)))\n",
    "print('jam {0} and of {1}'.format(j*100/len(y_train),j*100/len(y_train)))\n",
    "\n",
    "\n",
    "n=0;u=0;p=0;j=0;t=0\n",
    "\n",
    "print(len(y_train22))\n",
    "print('-------------------------')\n",
    "for x ,i in enumerate(y_train22):\n",
    "    \n",
    "    if(i[-1]=='Normal'):\n",
    "        n+=1\n",
    "    if(i[-1]=='DDOS_UDP_FLOOD'):\n",
    "        u+=1\n",
    "        t+=1\n",
    "    if(i[-1]=='PLUIES_ET_ORAGES'):\n",
    "        p+=1 \n",
    "        #n+=1\n",
    "        #t+=1\n",
    "    if(i[-1]=='BROUILLAGE_Trafic'):\n",
    "        #u+=1\n",
    "        j+=1\n",
    "        t+=1\n",
    "\n",
    "print('normal {0} and of {1}'.format(n*100/len(y_train22),t*100/len(y_train22)))\n",
    "print('udp {0} and of {1}'.format(u*100/len(y_train22),u*100/len(y_train22)))\n",
    "print('pluies {0} and of {1}'.format(p*100/len(y_train22),p*100/len(y_train22)))\n",
    "print('jam {0} and of {1}'.format(j*100/len(y_train22),j*100/len(y_train22)))\n",
    "\n",
    "n=0;u=0;p=0;j=0;t=0\n",
    "\n",
    "print(len(y_train222))\n",
    "print('-------------------------')\n",
    "for x ,i in enumerate(y_train222):\n",
    "    \n",
    "    if(i[-1]=='Normal'):\n",
    "        n+=1\n",
    "    if(i[-1]=='DDOS_UDP_FLOOD'):\n",
    "        u+=1\n",
    "        t+=1\n",
    "    if(i[-1]=='PLUIES_ET_ORAGES'):\n",
    "        p+=1 \n",
    "        #n+=1\n",
    "        #t+=1\n",
    "    if(i[-1]=='BROUILLAGE_Trafic'):\n",
    "        #u+=1\n",
    "        j+=1\n",
    "        t+=1\n",
    "\n",
    "print('normal {0} and of {1}'.format(n*100/len(y_train222),t*100/len(y_train222)))\n",
    "print('udp {0} and of {1}'.format(u*100/len(y_train222),u*100/len(y_train222)))\n",
    "print('pluies {0} and of {1}'.format(p*100/len(y_train222),p*100/len(y_train222)))\n",
    "print('jam {0} and of {1}'.format(j*100/len(y_train222),j*100/len(y_train222)))\n",
    "\n",
    "########\n",
    "\n",
    "print(int(len(y_train))+int(len(y_valid))+int(len(y_test)))\n",
    "print('creating input data time '+str(time.time()-a))\n",
    "\n",
    "\n",
    "\n",
    "a-time.time()\n",
    "X_train1=X_train.values\n",
    "y_train1=y_train.values\n",
    "#X_train1=X_train1.astype(float)\n",
    "\n",
    "torch_tensor = torch.tensor(X_train1)\n",
    "#torch_tensor[torch_tensor != torch_tensor]=float(0)\n",
    "t=[]\n",
    "test=0\n",
    "test0=0\n",
    "\n",
    "for j in y_train1:\n",
    "    i=j[0]\n",
    "#    print(i)\n",
    "    if (i=='Normal'):\n",
    "        t.append(int(0))\n",
    "        test0+=1\n",
    "    #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        t.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        t.append(int(2))\n",
    "        test+=1\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        t.append(int(3))\n",
    "print(test)\n",
    "labels = torch.LongTensor(t)\n",
    "#print(torch_tensor[-1])\n",
    "#print(labels[-1])\n",
    "print(labels)\n",
    "#print(df_label)\n",
    "#print(labels.shape)\n",
    "\n",
    "#df00 = pd.read_csv(path0+\"small/testbed3/Dataset_S03Filtrer.csv\")\n",
    "#bigX = df00[['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','droppedPKWrongPort','sentPK','size','channel','throughput']]         \n",
    "#bigY = df00['label']\n",
    "bigX = X_valid\n",
    "#[['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','droppedPKWrongPort','sentPK','size','throughput']]  ,'channel'       \n",
    "bigY = y_valid['label']\n",
    "#print(bigY)\n",
    "X_valid1=bigX.values\n",
    "y_valid1=bigY.values\n",
    "\n",
    "#X_valid1=X_test.values\n",
    "#y_valid1=y_test.values\n",
    "#X_train1=X_train1.astype(float)\n",
    "\n",
    "vtorch_tensor = torch.tensor(X_valid1)\n",
    "v=[]\n",
    "\n",
    "for i in y_valid1:\n",
    "    #i=j[0]\n",
    "    #print(i)\n",
    "    if (i=='Normal'):\n",
    "        v.append(int(0))\n",
    "        #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        v.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        v.append(int(2))\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        v.append(int(3))\n",
    "\n",
    "vlabels = torch.LongTensor(v)\n",
    "\n",
    "X_test1=X_test.values\n",
    "y_test1=y_test.values\n",
    "#path0 ='D:\\\\share\\\\with weather\\\\'\n",
    "#df = pd.read_csv(path0+\"FinaldatasetN1.csv\")\n",
    "#print('improt time '+str(time.time()-a))\n",
    "\n",
    "#a=time.time()\n",
    "#df=df[['channel','Next_Current_diff','Next_Pre_diff','size', 'packet_type','DataQueueLen','lossRate','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "#df=df[['channel','Next_Current_diff','Next_Pre_diff','size', 'packet_type','lossRate','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "#df=df[['channel','Next_Current_diff','Next_Pre_diff','size','packet_type','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P','label']]     \n",
    "#print('llllll')\n",
    "#X_test1=df.drop(columns = ['label']).values\n",
    "#y_test1=df[['label']].values\n",
    "print(len(y_test1))\n",
    "#X_train1=X_train1.astype(float)\n",
    "\n",
    "ttorch_tensor = torch.tensor(X_test1)\n",
    "\n",
    "#torch_tensor[torch_tensor != torch_tensor]=float(0)\n",
    "t=[]\n",
    "\n",
    "for j in y_test1:\n",
    "    i=j[0]\n",
    "    #print(i)\n",
    "    if (i=='Normal'):\n",
    "        t.append(int(0))\n",
    "        #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        t.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        t.append(int(2))\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        t.append(int(3))\n",
    "print(len(t))\n",
    "tlabels = torch.LongTensor(t)\n",
    "#print(torch_tensor[-1])\n",
    "#print(labels[-1])\n",
    "print(len(labels))\n",
    "print(len(vlabels))\n",
    "print(len(tlabels))\n",
    "\n",
    "del(X)\n",
    "del(y)\n",
    "del(X1)\n",
    "del(y1)\n",
    "del(X2)\n",
    "del(y2)\n",
    "del(X3)\n",
    "del(y3)\n",
    "del(X4)\n",
    "del(y4)\n",
    "del(X5)\n",
    "del(y5)\n",
    "del(X_train1); del(X_train2);del(X_train3); del(X_train4);del(X_train5)\n",
    "del(y_train1); del(y_train2);del(y_train3); del(y_train4);del(y_train5)\n",
    "del(y_rem);del(y_rem1); del(y_rem2);del(y_rem3); del(y_rem4);del(y_rem5)\n",
    "del(X_rem);del(X_rem1); del(X_rem2);del(X_rem3); del(X_rem4);del(X_rem5)\n",
    "\n",
    "#X_rem1, y_train1, y_rem1\n",
    "del(X_train)\n",
    "del(X_valid)\n",
    "del(X_valid1); del(X_valid2);del(X_valid3); del(X_valid4);del(X_valid5)\n",
    "del(y_valid1); del(y_valid2);del(y_valid3); del(y_valid4);del(y_valid5)\n",
    "\n",
    "\n",
    "del(X_test)\n",
    "del(X_test1); del(X_test2);del(X_test3); del(X_test4);del(X_test5)\n",
    "del(y_test1); del(y_test2);del(y_test3); del(y_test4);del(y_test5)\n",
    "\n",
    "del(y_train)\n",
    "del(y_valid)\n",
    "del(y_test)\n",
    "del(t)\n",
    "del(v)\n",
    "\n",
    "inputs=torch_tensor[:,:] \n",
    "labels1=labels[:]\n",
    "del(torch_tensor)\n",
    "del(labels)\n",
    "#print(len(inputs))\n",
    "#print(len(labels1))\n",
    "vinputs =vtorch_tensor[:,:]\n",
    "vlabels1=vlabels[:]\n",
    "del(vtorch_tensor)\n",
    "del(vlabels)\n",
    "tinputs =ttorch_tensor[:,:] \n",
    "tlabels1=tlabels[:]\n",
    "del(ttorch_tensor)\n",
    "del(tlabels)\n",
    "del(df)\n",
    "#del(df1)\n",
    "\n",
    "print(sum(labels1))\n",
    "print(sum(vlabels1))\n",
    "print(sum(tlabels1))\n",
    "\n",
    " \n",
    "print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T13:13:19.244958Z",
     "iopub.status.busy": "2021-09-01T13:13:19.244406Z",
     "iopub.status.idle": "2021-09-01T13:13:19.261803Z",
     "shell.execute_reply": "2021-09-01T13:13:19.260644Z",
     "shell.execute_reply.started": "2021-09-01T13:13:19.244919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "USE_GPU = True\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "# input_size = 784 # 28x28\n",
    "num_classes = 4\n",
    "#num_epochs = 2\n",
    "batch_size = 1\n",
    "\n",
    "input_size = 13\n",
    "sequence_length = 28\n",
    "hidden_size = 132\n",
    "num_layers = 2\n",
    "\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(14, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 140),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(140, 200),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(200, 32),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(32, 4),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "#criterion = nn.NLLLoss()\n",
    "Fonction_de_perte = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "model.load_state_dict(torch.load('models\\\\dataset2_300_200_MLP.pth'))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T13:13:19.264131Z",
     "iopub.status.busy": "2021-09-01T13:13:19.263789Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "epochs = 200\n",
    "valid_loss=0\n",
    "accuracy=0\n",
    "valid_loss_min=np.Inf\n",
    "batch_size =300\n",
    "\n",
    "####33  !!!!!!!!!! This shit cost alot of time ----> do not use it \n",
    "#b=time.time()\n",
    "#with torch.no_grad():\n",
    "   # for i, x in enumerate(vinputs):\n",
    "       # x2=x[None,:]\n",
    "       # output = torch.exp(model(x2))\n",
    "       # l2=vlabels1[i][None]\n",
    "       # test_loss+=Fonction_de_perte(output, l2)\n",
    "     #   output = torch.exp(output)\n",
    "      #  top_p , top_c = output.topk(1, dim=1)\n",
    "     #   equals = top_c==vlabels1[i].view(*top_c.shape)\n",
    "    #    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "   # tain_losses.append(running_loss/len(inputs))\n",
    "   # test_losses.append(test_loss/len(vinputs))\n",
    "   # print ('test accuracy :{0}, test loss : {2} ,  time {1} '.format(accuracy*100/len(vinputs) , time.time()-b ,test_loss/len(vinputs) ))\n",
    "\n",
    "######## ---> use this :) \n",
    "b=time.time()\n",
    "with torch.no_grad():\n",
    "    output = torch.exp(model(vinputs))\n",
    "    valid_loss=Fonction_de_perte(output, vlabels1)\n",
    "top_p , top_c = output.topk(1, dim=1)\n",
    "del(output)\n",
    "propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "equals = top_c==vlabels1.view(*top_c.shape)\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "#print ('valid accuracy :{0:.8f} with total prob : {3:.8f} and  valid loss : {2:.8f} ,  time {1:.6f} '.format(accuracy*100 , time.time()-b ,test_loss,propabilities))\n",
    "print ('Epoch  : valid accuracy :{0:.8f} with total prob : {1:.8f} '.format(accuracy*100  ,propabilities))\n",
    "\n",
    "####\n",
    "#print(inputs.shape[0])\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "for e in range(epochs):\n",
    "    c=time.time()\n",
    "    running_loss = 0\n",
    "    #for i, x in enumerate(inputs):\n",
    "    for i in range(0,inputs.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #indices = [i:i+batch_size]\n",
    "        #print(i+batch_size)\n",
    "        batch_x, batch_y = inputs[i:i+batch_size], labels1[i:i+batch_size]\n",
    "        \n",
    "        #x2=x[None,:]\n",
    "        output = model.forward(batch_x)\n",
    "        \n",
    "        #output = model.forward(x2)\n",
    "        #l2=labels1[i][None]\n",
    "        #print(l2)\n",
    "        #loss = Fonction_de_perte(output, l2)\n",
    "        loss = Fonction_de_perte(output, batch_y)\n",
    "      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "        accuracy=0\n",
    "        #print(l2)\n",
    "  \n",
    "        b=time.time()\n",
    "        with torch.no_grad():\n",
    "            ##\n",
    "            #with torch.no_grad():\n",
    "            #for i, x in enumerate(vinputs):\n",
    "             #   x2=x[None,:]\n",
    "              #  output = torch.exp(model(x2))\n",
    "               # l2=vlabels1[i][None]\n",
    "                #valid_loss1+=Fonction_de_perte(output, l2)\n",
    "            #test_loss=test_loss/len(vinputs)\n",
    "            ##\n",
    "                \n",
    "            model.eval()\n",
    "            output = torch.exp(model(vinputs))\n",
    "            valid_loss=Fonction_de_perte(output, vlabels1)\n",
    "        top_p , top_c = output.topk(1, dim=1)\n",
    "        del(output)\n",
    "        propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "        equals = top_c==vlabels1.view(*top_c.shape)\n",
    "        accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        print('Epoch {2} : Training loss {0:.8f} and valid loss : {1:.8f} :  '.format(running_loss/len(inputs),valid_loss, e))\n",
    "   \n",
    "        print ('Epoch {2} : valid accuracy :{0:.8f} with total prob : {1:.8f} '.format(accuracy*100  ,propabilities, e))\n",
    "        print(time.time()-c)\n",
    "    #print('validation loss with iterations {0}'.format(valid_loss1/len(vinputs) ) )   \n",
    "        train_losses.append(running_loss/len(inputs))\n",
    "        valid_losses.append(valid_loss)\n",
    "        if (valid_loss<valid_loss_min):\n",
    "            print('validation loss decreased , saving model ({:.8f} ==> {:.8f})'.format(valid_loss_min,valid_loss))\n",
    "            torch.save(model.state_dict(),'dataset2_300_200_MLP.pth')\n",
    "            valid_loss_min=valid_loss\n",
    "print(time.time()-a)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "#test_losses_scaled = scaler.fit_transform(test_losses)\n",
    "\n",
    "\n",
    "\n",
    "Tr=np.array(train_losses)\n",
    "Te=np.array(valid_losses)\n",
    "\n",
    "Tr=np.reshape(Tr, (len(Tr),1))\n",
    "Te=np.reshape(Te, (len(Te),1))\n",
    "\n",
    "# fit on training data column\n",
    "scale = StandardScaler().fit(Tr)\n",
    "tain_losses = scale.transform(Tr)\n",
    "scale = StandardScaler().fit(Te)\n",
    "test_losses = scale.transform(Te)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "#test_losses_scaled = scaler.fit_transform(test_losses)\n",
    "plt.ylim([-1,1])\n",
    "\n",
    "plt.plot(tain_losses, label='Train loss')\n",
    "plt.plot(test_losses, label='valid loss')\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "=====================\n",
      "test accuracy :91.74959564 with total prob : 99.66487885 and  test loss : 0.82851001 ,  time 13.98792982 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-685ceaa6f688>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mx2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a=time.time()\n",
    "summm=[]\n",
    "sum1=[]\n",
    "sum2=[]\n",
    "sum3=[]\n",
    "sum4=[]\n",
    "\n",
    "#tinputs =ttorch_tensor[:,:] \n",
    "#tlabels1=tlabels[:]\n",
    "\n",
    "\n",
    "#tinputs =torch_tensor[:,:] \n",
    "#tlabels1=labels[:]\n",
    "\n",
    "print('=====================')\n",
    "print('=====================')\n",
    "b=time.time()\n",
    "output = torch.exp(model(tinputs))\n",
    "test_loss=Fonction_de_perte(output, tlabels1)\n",
    "top_p , top_c = output.topk(1, dim=1)\n",
    "propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "equals = top_c==tlabels1.view(*top_c.shape)\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "model.train()\n",
    "print ('test accuracy :{0:.8f} with total prob : {3:.8f} and  test loss : {2:.8f} ,  time {1:.8f} '.format(accuracy*100 , time.time()-b ,test_loss ,propabilities))\n",
    "\n",
    "class_correct = list(0. for i in range (4))\n",
    "class_total = list(0. for i in range (4))\n",
    "\n",
    "C_n_udp=0\n",
    "C_n_pluies=0\n",
    "C_n_jam=0\n",
    "C_n_total=0\n",
    "\n",
    "\n",
    "C_u_normal=0\n",
    "C_u_jam=0\n",
    "C_u_pluies=0\n",
    "C_u_total=0\n",
    "\n",
    "C_p_normal=0\n",
    "C_p_udp=0\n",
    "C_p_jam=0\n",
    "C_p_total=0\n",
    "\n",
    "C_j_normal=0\n",
    "C_j_udp=0\n",
    "C_j_pluies=0\n",
    "C_j_total=0\n",
    "\n",
    "for i, x in enumerate(tinputs):\n",
    "    optimizer.zero_grad()\n",
    "    x2=x[None,:]\n",
    "    with torch.no_grad():\n",
    "        output = torch.exp(model(x2))\n",
    "    out=output.detach().numpy()*100\n",
    " \n",
    "    l3=tlabels1[i].item()\n",
    "  \n",
    "    if(l3==0):\n",
    "        summm.append(out[0][0])\n",
    "        sum1.append(out[0][0])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_n_udp+=1\n",
    "            if(top_c[i][0]==2):\n",
    "                C_n_pluies+=1\n",
    "            if(top_c[i][0]==3):\n",
    "                C_n_jam +=1\n",
    "            \n",
    "            C_n_total +=1\n",
    "            \n",
    "    if(l3==1):\n",
    "        summm.append(out[0][1])\n",
    "        sum2.append(out[0][1])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==3):\n",
    "                C_u_jam+=1\n",
    "            if(top_c[i][0]==2):\n",
    "                C_u_pluies+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_u_normal +=1\n",
    "            \n",
    "            C_u_total +=1\n",
    "\n",
    "    if(l3==2):\n",
    "        summm.append(out[0][2])\n",
    "        sum3.append(out[0][2])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_p_udp+=1\n",
    "            if(top_c[i][0]==3):\n",
    "                C_p_jam+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_p_normal +=1\n",
    "            \n",
    "            C_p_total +=1\n",
    "\n",
    "    if(l3==3):\n",
    "        summm.append(out[0][3])\n",
    "        sum4.append(out[0][3])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_j_udp+=1\n",
    "            if(top_c[i][0]==2):\n",
    "                C_j_pluies+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_j_normal +=1\n",
    "            \n",
    "            C_j_total +=1\n",
    "\n",
    "    class_total[l3]+=1\n",
    "    class_correct[l3]+=equals[i][0]\n",
    "    #print(output)\n",
    "print('la précision de detection globale: {0} '.format(mean(summm)))\n",
    "print('detection des communication normal: {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum1), class_correct[0]*100/class_total[0] ,class_correct[0] ,class_total[0]))\n",
    "if(C_n_total!=0):\n",
    "    print('details normal classed udp :{0:.5f}, pluies {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_n_udp*100/class_total[0],C_n_pluies*100/class_total[0],C_n_jam*100/class_total[0],C_n_udp*100/C_n_total,C_n_pluies*100/C_n_total,C_n_jam*100/C_n_total))\n",
    "else:\n",
    "    print('detection normal 100')\n",
    "\n",
    "print('detection du deni de service par udp flood {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum2), class_correct[1]*100/class_total[1] ,class_correct[1] ,class_total[1]))\n",
    "if(C_u_total!=0):\n",
    "    print('details udp classed normal :{0:.5f}, pluies {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_u_normal*100/class_total[1],C_u_pluies*100/class_total[1],C_u_jam*100/class_total[1],C_u_normal*100/C_u_total,C_u_pluies*100/C_u_total,C_u_jam*100/C_u_total))\n",
    "else:\n",
    "    print('detection udp flood 100')\n",
    "    \n",
    "print('detection du deni naturel : pluies et orages : {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum3), class_correct[2]*100/class_total[2] ,class_correct[2] ,class_total[2]))\n",
    "if(C_p_total!=0):\n",
    "    print('details pluies classed udp :{0:.5f}, normal {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_p_udp*100/class_total[2],C_p_normal*100/class_total[2],C_p_jam*100/class_total[2],C_p_udp*100/C_p_total,C_p_normal*100/C_p_total,C_p_jam*100/C_p_total))\n",
    "else:\n",
    "    print('detection pluies et orages 100')\n",
    "\n",
    "print('detection du deni naturel : jam : {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum4), class_correct[3]*100/class_total[3] ,class_correct[3] ,class_total[3]))\n",
    "if(C_j_total!=0):\n",
    "    print('details jam classed udp :{0:.5f}, pluies {1:.5f} , normal {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_j_udp*100/class_total[3],C_j_pluies*100/class_total[3],C_j_normal*100/class_total[3],C_j_udp*100/C_j_total,C_j_pluies*100/C_j_total,C_j_normal*100/C_j_total))\n",
    "else:\n",
    "    print('detection brouillage 100')\n",
    "print('=====================')\n",
    "\n",
    "print('<================================>')\n",
    "print('Total_time')\n",
    "print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test():  \n",
    "\n",
    "\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    USE_GPU = True\n",
    "\n",
    "    from torch import nn, optim\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Hyper-parameters \n",
    "    # input_size = 784 # 28x28\n",
    "    num_classes = 4\n",
    "    #num_epochs = 2\n",
    "    batch_size = 1\n",
    "    input_size = 13\n",
    "    sequence_length = 28\n",
    "    hidden_size = 132\n",
    "    num_layers = 2\n",
    "\n",
    "\n",
    "\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "\n",
    "    model = nn.Sequential(nn.Linear(14, 64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(64, 140),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(140, 200),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(200, 32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(32, 4),\n",
    "                          nn.LogSoftmax(dim=1))\n",
    "\n",
    "    #criterion = nn.NLLLoss()\n",
    "    Fonction_de_perte = nn.CrossEntropyLoss()\n",
    "\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "    model.load_state_dict(torch.load('models\\\\dataset2_300_200_MLP.pth'))\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    path0 ='D:\\\\share\\\\Data_file\\\\Senarios\\\\new_test\\\\dataset2.csv' \n",
    "    df = pd.read_csv(path0)\n",
    "    df1=df[['channel','Next_Current_diff','Next_Pre_diff','size','packet_type','SNext_Current_diff','SNext_Pre_diff','snir','throughput','Flow Bytes_s','Flow Packets_s','meanT_b_2P','minT_b_2P','maxT_b_2P']]     \n",
    "    np_data=df.to_numpy()\n",
    "    df1=df1.to_numpy()\n",
    "    ttorch_tensor = torch.tensor(df1)\n",
    "    tinputs =ttorch_tensor[:,:] \n",
    "    output = torch.exp(model(tinputs))\n",
    "    #test_loss=Fonction_de_perte(output, tlabels1)\n",
    "    top_p , top_c = output.topk(1, dim=1)\n",
    "    node=set()\n",
    "    node1=set()\n",
    "    node2=dict()\n",
    "    node10=dict()\n",
    "    node11=set()\n",
    "\n",
    "\n",
    "    #with open(location+\"results/index.txt\", \"w\") as f:\n",
    "     #   #f.write(str(C_index+1))\n",
    "      #  f.close()\n",
    "    name='Pluies'\n",
    "    if(name=='Pluies'):\n",
    "        print('pluies')\n",
    "        print(len(top_c))\n",
    "        top_c = top_c.cpu().detach().numpy()\n",
    "        for index,i in enumerate(top_c):\n",
    "            i=i[0]\n",
    "            i1=np_data[index][5]+' '+np_data[index][6]\n",
    "\n",
    "            if (i1 in node11):\n",
    "                    pass\n",
    "                    node10[i1]+=1\n",
    "                    #print(node2[i]) \n",
    "            else:\n",
    "                    node11.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node10[i1]=1\n",
    "            if(i==2):\n",
    "                #print('udp attack from '+str(np_data[index][5])+' and with probability of '+str(top_p[index]))\n",
    "                i1=np_data[index][5]+' '+np_data[index][6]\n",
    "                if (i1 in node):\n",
    "                    pass\n",
    "                    node2[i1][1]+=1\n",
    "                    #print(node2[i]) \n",
    "                else:\n",
    "                    node.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node2[i1]=list([np_data[index][1],1])\n",
    "\n",
    "        somme=0\n",
    "        max1=0\n",
    "        for i in node2:\n",
    "            node2[i].append(node2[i][1]*100/len(top_c))\n",
    "            somme+=node2[i][1]*100/len(top_c)\n",
    "            if(max1<(node2[i][1]*100/len(top_c))):\n",
    "                max1=node2[i][1]*100/len(top_c)\n",
    "        if(somme>40 or 1):\n",
    "\n",
    "            print('=========================================================================================================')\n",
    "            print('                                   Pluies et Orages (Brouillage Naturel)')\n",
    "            print('=========================================================================================================')\n",
    "            #now = datetime.now()\n",
    "            #print(' Detection entre :'+str(dt_0)+' et ' + str(now.strftime(\" %H:%M:%S\")))\n",
    "            for i in node2:\n",
    "                print('Flux entre '+str(i.replace(' ', ' et '))+' avec une probabilite de '+str(\"{:.2f}\".format(node2[i][1]*100/node10[i])) + '% ( '+ str(\"{:.2f}\".format(node2[i][2])) +' % de flux capture )'  )\n",
    "    node=set()\n",
    "    node1=set()\n",
    "    node2=dict()\n",
    "    node10=dict()\n",
    "    node11=set()\n",
    "\n",
    "    name='Jam'\n",
    "    if(name=='Jam'):\n",
    "        #print(location)\n",
    "        #print(np_data[0])\n",
    "        for index,i in enumerate(top_c):\n",
    "            i=i[0]\n",
    "            i1=np_data[index][5]+' '+np_data[index][6]\n",
    "            if (i1 in node11):\n",
    "                    pass\n",
    "                    node10[i1]+=1\n",
    "                    #print(node2[i]) \n",
    "            else:\n",
    "                    node11.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node10[i1]=1\n",
    "            if(i==3):\n",
    "\n",
    "                #print('udp attack from '+str(np_data[index][5])+' and with probability of '+str(top_p[index]))\n",
    "                i1=np_data[index][5]+' '+np_data[index][6]\n",
    "                if (i1 in node):\n",
    "                    pass\n",
    "                    node2[i1][1]+=1\n",
    "                    #print(node2[i]) \n",
    "                else:\n",
    "                    node.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node2[i1]=list([np_data[index][1],1])\n",
    "        somme=0\n",
    "        max1=0\n",
    "        for i in node2:\n",
    "            node2[i].append(node2[i][1]*100/len(top_c))\n",
    "            somme+=node2[i][1]*100/len(top_c)\n",
    "            if(max1<(node2[i][1]*100/len(top_c))):\n",
    "                max1=node2[i][1]*100/len(top_c)\n",
    "        if(somme>35 or 1):\n",
    "\n",
    "            print('=========================================================================================================')\n",
    "            print('                                              Jamming')\n",
    "            print('=========================================================================================================')\n",
    "           # now = datetime.now()\n",
    "            #print(' Detection entre :'+str(dt_0)+' et ' + str(now.strftime(\" %H:%M:%S\")))\n",
    "\n",
    "            for i in node2:\n",
    "                print('Flux entre '+str(i.replace(' ', ' et '))+' avec une probabilite de '+str(\"{:.2f}\".format(node2[i][1]*100/node10[i])) + ' % ( '+ str(\"{:.2f}\".format(node2[i][2])) +' % de flux capture )'  )\n",
    "    node=set()\n",
    "    node1=set()\n",
    "    node2=dict()\n",
    "    node10=dict()\n",
    "    node11=set()\n",
    "\n",
    "    name='Flood'\n",
    "    if(name=='Flood'):\n",
    "        #print('flood')\n",
    "        #top_c = top_c.cpu().detach().numpy()\n",
    "        for index,i in enumerate(top_c):\n",
    "            i=i[0]\n",
    "            i1=np_data[index][5]+' '+np_data[index][6]\n",
    "\n",
    "            if (i1 in node11):\n",
    "                    pass\n",
    "                    node10[i1]+=1\n",
    "                    #print(node2[i]) \n",
    "            else:\n",
    "                    node11.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node10[i1]=1\n",
    "            if(i==1):\n",
    "                #print('udp attack from '+str(np_data[index][5])+' and with probability of '+str(top_p[index]))\n",
    "                i1=np_data[index][5]+' '+np_data[index][6]\n",
    "                if (i1 in node):\n",
    "                    pass\n",
    "                    node2[i1][1]+=1\n",
    "                    #print(node2[i]) \n",
    "                else:\n",
    "                    node.add(i1)\n",
    "                    #[sentTime , n]\n",
    "                    node2[i1]=list([np_data[index][1],1])\n",
    "\n",
    "        somme=0\n",
    "        max1=0\n",
    "        for i in node2:\n",
    "            node2[i].append(node2[i][1]*100/len(top_c))\n",
    "            somme+=node2[i][1]*100/len(top_c)\n",
    "            if(max1<(node2[i][1]*100/len(top_c))):\n",
    "                max1=node2[i][1]*100/len(top_c)\n",
    "        del(top_c)\n",
    "        if(somme>20 or 1):\n",
    "\n",
    "            print('=========================================================================================================')\n",
    "            print('                                      Attaque par Flood')\n",
    "            print('=========================================================================================================')\n",
    "\n",
    "            #now = datetime.now()\n",
    "            #print(' Detection entre :'+str(dt_0)+' et ' + str(now.strftime(\" %H:%M:%S\")))\n",
    "\n",
    "            for i in node2:\n",
    "                print('Flux entre '+str(i.replace(' ', ' et '))+' avec une probabilite de '+str(\"{:.2f}\".format(node2[i][1]*100/node10[i])) + '% ( '+ str(\"{:.2f}\".format(node2[i][2])) +' % de flux capture )'  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MLP multiclasses dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pluies\n",
      "38100\n",
      "=========================================================================================================\n",
      "                                   Pluies et Orages (Brouillage Naturel)\n",
      "=========================================================================================================\n",
      "Flux entre 19.10.1.104 et 19.10.1.107 avec une probabilite de 100.00% ( 4.62 % de flux capture )\n",
      "Flux entre 19.10.1.103 et 19.11.20.100 avec une probabilite de 100.00% ( 6.51 % de flux capture )\n",
      "Flux entre 19.10.1.104 et 19.10.1.106 avec une probabilite de 100.00% ( 1.50 % de flux capture )\n",
      "=========================================================================================================\n",
      "                                              Jamming\n",
      "=========================================================================================================\n",
      "Flux entre 19.10.1.105 et 19.10.1.108 avec une probabilite de 100.00 % ( 0.76 % de flux capture )\n",
      "Flux entre 19.11.14.100 et 19.10.1.102 avec une probabilite de 100.00 % ( 0.80 % de flux capture )\n",
      "Flux entre 19.10.1.102 et 19.11.14.100 avec une probabilite de 100.00 % ( 0.80 % de flux capture )\n",
      "Flux entre 19.11.12.100 et 19.10.1.109 avec une probabilite de 100.00 % ( 0.83 % de flux capture )\n",
      "Flux entre 19.10.1.101 et 19.11.16.100 avec une probabilite de 100.00 % ( 3.62 % de flux capture )\n",
      "Flux entre 19.10.1.100 et 19.10.1.108 avec une probabilite de 100.00 % ( 1.54 % de flux capture )\n",
      "=========================================================================================================\n",
      "                                      Attaque par Flood\n",
      "=========================================================================================================\n",
      "Flux entre 19.10.1.101 et 19.11.19.100 avec une probabilite de 100.00% ( 0.78 % de flux capture )\n",
      "Flux entre 19.10.1.102 et 19.11.16.100 avec une probabilite de 100.00% ( 0.80 % de flux capture )\n",
      "Flux entre 19.10.1.105 et 19.10.1.254 avec une probabilite de 100.00% ( 12.16 % de flux capture )\n",
      "Flux entre 19.10.1.106 et 19.10.1.254 avec une probabilite de 100.00% ( 12.10 % de flux capture )\n"
     ]
    }
   ],
   "source": [
    "Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-08-22T12:29:55.210161Z",
     "iopub.status.busy": "2021-08-22T12:29:55.206368Z",
     "iopub.status.idle": "2021-08-22T12:30:06.058782Z",
     "shell.execute_reply": "2021-08-22T12:30:06.057726Z",
     "shell.execute_reply.started": "2021-08-22T11:59:59.699434Z"
    },
    "papermill": {
     "duration": 10.980666,
     "end_time": "2021-08-22T12:30:06.058990",
     "exception": false,
     "start_time": "2021-08-22T12:29:55.078324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30497\n",
      "tensor([0, 0, 0,  ..., 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import os\n",
    "\n",
    "a=time.time()\n",
    "path0 ='../'\n",
    "df = pd.read_csv(path0+\"Dataset_S022Final.csv\")\n",
    "df=df[['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','packet_type','droppedPKWrongPort','sentPK','size','channel','DataQueueLen','passedUpPk','rcvdPKFromHL','rcvdPKFromLL','sentDownPK','DropPKByQueue','snir','throughput','label']]         \n",
    "\n",
    "df_Normal=df[0:76064].copy()\n",
    "#print(df_Normal)\n",
    "df_UDP=df[76064:214037].copy()\n",
    "#print(df_UDP)\n",
    "df_pluies=df[214037:426866].copy()\n",
    "#print(df_Normal2)\n",
    "df_jam=df[462481:-1].copy()\n",
    "\n",
    "X=df_Normal.drop(columns = ['label']).copy()\n",
    "y=df_Normal[['label']].copy()\n",
    "X1=df_UDP.drop(columns = ['label']).copy()\n",
    "y1=df_UDP[['label']].copy()\n",
    "X2=df_pluies.drop(columns = ['label']).copy()\n",
    "y2=df_pluies[['label']].copy()\n",
    "X3=df_jam.drop(columns = ['label']).copy()\n",
    "y3=df_jam[['label']].copy()\n",
    "\n",
    "train=0.4\n",
    "xcol=X.columns\n",
    "ycol=y.columns\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=train,shuffle=False)\n",
    "X_train1, X_rem1, y_train1, y_rem1 = train_test_split(X1,y1, train_size=train,shuffle=False)\n",
    "X_train2, X_rem2, y_train2, y_rem2 = train_test_split(X2,y2, train_size=train,shuffle=False)\n",
    "X_train3, X_rem3, y_train3, y_rem3 = train_test_split(X3,y3, train_size=train,shuffle=False)\n",
    "validation=0.5\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5,shuffle=False)\n",
    "X_valid1, X_test1, y_valid1, y_test1 = train_test_split(X_rem1,y_rem1, test_size=0.5,shuffle=False)\n",
    "X_valid2, X_test2, y_valid2, y_test2 = train_test_split(X_rem2,y_rem2, test_size=0.5,shuffle=False)\n",
    "X_valid3, X_test3, y_valid3, y_test3 = train_test_split(X_rem3,y_rem3, test_size=0.5,shuffle=False)\n",
    "\n",
    "\n",
    "X_train=np.concatenate((X_train, X_train1, X_train2,X_train3))\n",
    "X_valid=np.concatenate((X_valid, X_valid1, X_valid2,X_valid3))\n",
    "X_test=np.concatenate((X_test, X_test1, X_test2,X_test3))\n",
    "\n",
    "y_train=np.concatenate((y_train, y_train1, y_train2,y_train3))\n",
    "y_valid=np.concatenate((y_valid, y_valid1, y_valid2,y_valid3))\n",
    "y_test=np.concatenate((y_test, y_test1, y_test2,y_test3))\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns= xcol)\n",
    "X_valid = pd.DataFrame(X_valid, columns= xcol)\n",
    "X_test = pd.DataFrame(X_test, columns= xcol)\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns= ycol)\n",
    "y_valid = pd.DataFrame(y_valid, columns= ycol)\n",
    "y_test = pd.DataFrame(y_test, columns= ycol)\n",
    "#print('creating input data time '+str(time.time()-a))\n",
    "a-time.time()\n",
    "X_train1=X_train.values\n",
    "y_train1=y_train.values\n",
    "#X_train1=X_train1.astype(float)\n",
    "\n",
    "torch_tensor = torch.tensor(X_train1)\n",
    "#torch_tensor[torch_tensor != torch_tensor]=float(0)\n",
    "t=[]\n",
    "test=0\n",
    "test0=0\n",
    "\n",
    "for j in y_train1:\n",
    "    i=j[0]\n",
    "#    print(i)\n",
    "    if (i=='Normal'):\n",
    "        t.append(int(0))\n",
    "        test0+=1\n",
    "    #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        t.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        t.append(int(0))\n",
    "        test+=1\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        t.append(int(1))\n",
    "print(test)\n",
    "labels = torch.LongTensor(t)\n",
    "print(labels)\n",
    "bigX = X_valid\n",
    "bigY = y_valid['label']\n",
    "#print(bigY)\n",
    "X_valid1=bigX.values\n",
    "y_valid1=bigY.values\n",
    "\n",
    "vtorch_tensor = torch.tensor(X_valid1)\n",
    "v=[]\n",
    "\n",
    "for i in y_valid1:\n",
    "    if (i=='Normal'):\n",
    "        v.append(int(0))\n",
    "        #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        v.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        v.append(int(0))\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        v.append(int(1))\n",
    "\n",
    "vlabels = torch.LongTensor(v)\n",
    "X_test1=X_test.values\n",
    "y_test1=y_test.values\n",
    "\n",
    "ttorch_tensor = torch.tensor(X_test1)\n",
    "t=[]\n",
    "\n",
    "for j in y_test1:\n",
    "    i=j[0]\n",
    "    if (i=='Normal'):\n",
    "        t.append(int(0))\n",
    "        #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        t.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        t.append(int(0))\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        t.append(int(1))\n",
    "\n",
    "tlabels = torch.LongTensor(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T12:30:06.090622Z",
     "iopub.status.busy": "2021-08-22T12:30:06.089875Z",
     "iopub.status.idle": "2021-08-22T12:30:06.107226Z",
     "shell.execute_reply": "2021-08-22T12:30:06.106550Z",
     "shell.execute_reply.started": "2021-08-22T12:00:10.250945Z"
    },
    "papermill": {
     "duration": 0.040136,
     "end_time": "2021-08-22T12:30:06.107375",
     "exception": false,
     "start_time": "2021-08-22T12:30:06.067239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_classes = 2\n",
    "learning_rate = 0.01\n",
    "input_size = 19\n",
    "sequence_length = 28\n",
    "hidden_size = 132\n",
    "num_layers = 3\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=x.view(-1,1,19)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        out = out[:, -1, :]\n",
    "        x = F.log_softmax(self.fc(out), dim=1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "Fonction_de_perte = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "model.load_state_dict(torch.load('models\\\\dataset1_1_50_RNN_Bin.pth'))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T12:30:06.140697Z",
     "iopub.status.busy": "2021-08-22T12:30:06.139847Z",
     "iopub.status.idle": "2021-08-22T18:04:29.108414Z",
     "shell.execute_reply": "2021-08-22T18:04:29.108952Z",
     "shell.execute_reply.started": "2021-08-22T12:00:10.280550Z"
    },
    "papermill": {
     "duration": 20062.994069,
     "end_time": "2021-08-22T18:04:29.109159",
     "exception": false,
     "start_time": "2021-08-22T12:30:06.115090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#labels1=labels[loc]\n",
    "a=time.time()\n",
    "inputs=torch_tensor[:,:].to(device) \n",
    "labels1=labels[:].to(device)\n",
    "vinputs =vtorch_tensor[:,:].to(device)\n",
    "vlabels1=vlabels[:].to(device)\n",
    "\n",
    "epochs = 50\n",
    "valid_loss=0\n",
    "accuracy=0\n",
    "valid_loss_min=np.Inf\n",
    "b=time.time()\n",
    "with torch.no_grad():\n",
    "    output = torch.exp(model(vinputs))\n",
    "    valid_loss=Fonction_de_perte(output, vlabels1)\n",
    "top_p , top_c = output.topk(1, dim=1)\n",
    "propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "equals = top_c==vlabels1.view(*top_c.shape)\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "#print ('valid accuracy :{0:.8f} with total prob : {3:.8f} and  valid loss : {2:.8f} ,  time {1:.6f} '.format(accuracy*100 , time.time()-b ,test_loss,propabilities))\n",
    "print ('Epoch  : valid accuracy :{0:.8f} with total prob : {1:.8f} '.format(accuracy*100  ,propabilities))\n",
    "\n",
    "####\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "for e in range(epochs):\n",
    "    c=time.time()\n",
    "    running_loss = 0\n",
    "    for i, x in enumerate(inputs):\n",
    "        optimizer.zero_grad()\n",
    "        x2=x[None,:]\n",
    "        output = model.forward(x2)\n",
    "        l2=labels1[i][None]\n",
    "        loss = Fonction_de_perte(output, l2)      \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        accuracy=0  \n",
    "        b=time.time()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output = torch.exp(model(vinputs))\n",
    "            valid_loss=Fonction_de_perte(output, vlabels1)\n",
    "        top_p , top_c = output.topk(1, dim=1)\n",
    "        propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "        equals = top_c==vlabels1.view(*top_c.shape)\n",
    "        accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        print('Epoch {2} : Training loss {0:.8f} and valid loss : {1:.8f} :  '.format(running_loss/len(inputs),valid_loss, e))   \n",
    "        print ('Epoch {2} : valid accuracy :{0:.8f} with total prob : {1:.8f} '.format(accuracy*100  ,propabilities, e))\n",
    "        print(time.time()-c)\n",
    "        train_losses.append(running_loss/len(inputs))\n",
    "        valid_losses.append(valid_loss)\n",
    "        if (valid_loss<valid_loss_min):\n",
    "            print('validation loss decreased , saving model ({:.8f} ==> {:.8f})'.format(valid_loss_min,valid_loss))\n",
    "            torch.save(model.state_dict(),'dataset1_1_50_RNN_Bin.pth')\n",
    "            valid_loss_min=valid_loss\n",
    "print(time.time()-a)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T18:04:29.165358Z",
     "iopub.status.busy": "2021-08-22T18:04:29.164265Z",
     "iopub.status.idle": "2021-08-22T18:04:29.391207Z",
     "shell.execute_reply": "2021-08-22T18:04:29.390670Z",
     "shell.execute_reply.started": "2021-08-22T12:28:47.242872Z"
    },
    "papermill": {
     "duration": 0.258655,
     "end_time": "2021-08-22T18:04:29.391351",
     "exception": false,
     "start_time": "2021-08-22T18:04:29.132696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "Tr=np.array(train_losses)\n",
    "Te=np.array(valid_losses)\n",
    "\n",
    "Tr=np.reshape(Tr, (len(Tr),1))\n",
    "Te=np.reshape(Te, (len(Te),1))\n",
    "\n",
    "# fit on training data column\n",
    "scale = StandardScaler().fit(Tr)\n",
    "tain_losses = scale.transform(Tr)\n",
    "scale = StandardScaler().fit(Te)\n",
    "test_losses = scale.transform(Te)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "plt.ylim([-1,1])\n",
    "\n",
    "plt.plot(tain_losses, label='Train loss')\n",
    "plt.plot(test_losses, label='valid loss')\n",
    "plt.legend(frameon=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-22T18:04:29.445211Z",
     "iopub.status.busy": "2021-08-22T18:04:29.444469Z",
     "iopub.status.idle": "2021-08-22T18:06:45.925068Z",
     "shell.execute_reply": "2021-08-22T18:06:45.924481Z"
    },
    "papermill": {
     "duration": 136.508998,
     "end_time": "2021-08-22T18:06:45.925220",
     "exception": false,
     "start_time": "2021-08-22T18:04:29.416222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "test accuracy :99.77451324 with total prob : 99.84203339 and  test loss : -0.99753405 ,  time 1.13139462 \n",
      "la précision de detection globale: 99.75340547506752 \n",
      "detection des communication normal: 99.66813110 with accuracy 99.69486877 ( 103246/103562 )\n",
      "details normal classed udp :0.30513, pluies 0.00000 , jam 0.00000 (ou 100.00000,0.00000,0.00000) \n",
      "detection du deni de service par udp flood 99.97595436 with accuracy 99.98235976 ( 39675/39682 )\n",
      "details udp classed normal :0.01764, pluies 0.00000 , jam 0.00000 (ou 100.00000,0.00000,0.00000) \n",
      "=====================\n",
      "<================================>\n",
      "Total_time\n",
      "58.14657115936279\n"
     ]
    }
   ],
   "source": [
    "a=time.time()\n",
    "summm=[]\n",
    "sum1=[]\n",
    "sum2=[]\n",
    "sum3=[]\n",
    "sum4=[]\n",
    "\n",
    "tinputs =ttorch_tensor[:,:] \n",
    "tlabels1=tlabels[:]\n",
    "\n",
    "print('=====================')\n",
    "b=time.time()\n",
    "output = torch.exp(model(tinputs))\n",
    "test_loss=Fonction_de_perte(output, tlabels1)\n",
    "top_p , top_c = output.topk(1, dim=1)\n",
    "propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "equals = top_c==tlabels1.view(*top_c.shape)\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "model.train()\n",
    "print ('test accuracy :{0:.8f} with total prob : {3:.8f} and  test loss : {2:.8f} ,  time {1:.8f} '.format(accuracy*100 , time.time()-b ,test_loss ,propabilities))\n",
    "class_correct = list(0. for i in range (4))\n",
    "class_total = list(0. for i in range (4))\n",
    "\n",
    "C_n_udp=0\n",
    "C_n_pluies=0\n",
    "C_n_jam=0\n",
    "C_n_total=0\n",
    "\n",
    "\n",
    "C_u_normal=0\n",
    "C_u_jam=0\n",
    "C_u_pluies=0\n",
    "C_u_total=0\n",
    "\n",
    "C_p_normal=0\n",
    "C_p_udp=0\n",
    "C_p_jam=0\n",
    "C_p_total=0\n",
    "\n",
    "C_j_normal=0\n",
    "C_j_udp=0\n",
    "C_j_pluies=0\n",
    "C_j_total=0\n",
    "\n",
    "for i, x in enumerate(tinputs):\n",
    "    optimizer.zero_grad()\n",
    "    x2=x[None,:]\n",
    "    with torch.no_grad():\n",
    "        output = torch.exp(model(x2))\n",
    "    out=output.detach().numpy()*100\n",
    " \n",
    "    l3=tlabels1[i].item()\n",
    "  \n",
    "    if(l3==0):\n",
    "        summm.append(out[0][0])\n",
    "        sum1.append(out[0][0])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_n_udp+=1\n",
    "            C_n_total +=1\n",
    "            \n",
    "    if(l3==1):\n",
    "        summm.append(out[0][1])\n",
    "        sum2.append(out[0][1])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==0):\n",
    "                C_u_normal +=1\n",
    "            \n",
    "            C_u_total +=1\n",
    "\n",
    "    if(l3==2):\n",
    "        summm.append(out[0][2])\n",
    "        sum3.append(out[0][2])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_p_udp+=1\n",
    "            if(top_c[i][0]==3):\n",
    "                C_p_jam+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_p_normal +=1\n",
    "            \n",
    "            C_p_total +=1\n",
    "\n",
    "    if(l3==3):\n",
    "        summm.append(out[0][3])\n",
    "        sum4.append(out[0][3])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_j_udp+=1\n",
    "            if(top_c[i][0]==2):\n",
    "                C_j_pluies+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_j_normal +=1\n",
    "            \n",
    "            C_j_total +=1\n",
    "\n",
    "    class_total[l3]+=1\n",
    "    class_correct[l3]+=equals[i][0]\n",
    "    #print(output)\n",
    "print('la précision de detection globale: {0} '.format(mean(summm)))\n",
    "print('detection des communication normal: {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum1), class_correct[0]*100/class_total[0] ,class_correct[0] ,class_total[0]))\n",
    "if(C_n_total!=0):\n",
    "    print('details normal classed udp :{0:.5f}, pluies {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_n_udp*100/class_total[0],C_n_pluies*100/class_total[0],C_n_jam*100/class_total[0],C_n_udp*100/C_n_total,C_n_pluies*100/C_n_total,C_n_jam*100/C_n_total))\n",
    "else:\n",
    "    print('detection normal 100')\n",
    "\n",
    "print('detection du deni de service par udp flood {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum2), class_correct[1]*100/class_total[1] ,class_correct[1] ,class_total[1]))\n",
    "if(C_u_total!=0):\n",
    "    print('details udp classed normal :{0:.5f}, pluies {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_u_normal*100/class_total[1],C_u_pluies*100/class_total[1],C_u_jam*100/class_total[1],C_u_normal*100/C_u_total,C_u_pluies*100/C_u_total,C_u_jam*100/C_u_total))\n",
    "else:\n",
    "    print('detection udp flood 100')\n",
    "print('=====================')\n",
    "print('<================================>')\n",
    "print('Total_time')\n",
    "print(time.time()-a)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20220.443951,
   "end_time": "2021-08-22T18:06:47.303004",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-22T12:29:46.859053",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-28T23:53:15.091514Z",
     "iopub.status.busy": "2021-08-28T23:53:15.091048Z",
     "iopub.status.idle": "2021-08-28T23:53:46.010110Z",
     "shell.execute_reply": "2021-08-28T23:53:46.009024Z",
     "shell.execute_reply.started": "2021-08-28T23:53:15.091415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improt time 10.024703741073608\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "path0 ='../'\n",
    "df = pd.read_csv(path0+\"Dataset_S022Final.csv\")\n",
    "\n",
    "a=time.time()\n",
    "df=df[['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','packet_type','droppedPKWrongPort','sentPK','size','channel','DataQueueLen','passedUpPk','rcvdPKFromHL','rcvdPKFromLL','sentDownPK','DropPKByQueue','snir','throughput','label']]         \n",
    "\n",
    "\n",
    "df_Normal=df[0:76064].copy()\n",
    "#print(df_Normal)\n",
    "df_UDP=df[76064:214037].copy()\n",
    "#print(df_UDP)\n",
    "df_pluies=df[214037:426866].copy()\n",
    "\n",
    "#print(df_Normal2)\n",
    "df_jam=df[462481:-1].copy()\n",
    "\n",
    "X=df_Normal.drop(columns = ['label']).copy()\n",
    "y=df_Normal[['label']].copy()\n",
    "#print(y.columns)\n",
    "X1=df_UDP.drop(columns = ['label']).copy()\n",
    "y1=df_UDP[['label']].copy()\n",
    "X2=df_pluies.drop(columns = ['label']).copy()\n",
    "y2=df_pluies[['label']].copy()\n",
    "X3=df_jam.drop(columns = ['label']).copy()\n",
    "y3=df_jam[['label']].copy()\n",
    "\n",
    "train=0.4\n",
    "\n",
    "\n",
    "xcol=X.columns\n",
    "ycol=y.columns\n",
    "\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=train,shuffle=False)\n",
    "X_train1, X_rem1, y_train1, y_rem1 = train_test_split(X1,y1, train_size=train,shuffle=False)\n",
    "X_train2, X_rem2, y_train2, y_rem2 = train_test_split(X2,y2, train_size=train,shuffle=False)\n",
    "X_train3, X_rem3, y_train3, y_rem3 = train_test_split(X3,y3, train_size=train,shuffle=False)\n",
    "\n",
    "validation=0.5\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5,shuffle=False)\n",
    "X_valid1, X_test1, y_valid1, y_test1 = train_test_split(X_rem1,y_rem1, test_size=0.5,shuffle=False)\n",
    "X_valid2, X_test2, y_valid2, y_test2 = train_test_split(X_rem2,y_rem2, test_size=0.5,shuffle=False)\n",
    "X_valid3, X_test3, y_valid3, y_test3 = train_test_split(X_rem3,y_rem3, test_size=0.5,shuffle=False)\n",
    "\n",
    "\n",
    "X_train=np.concatenate((X_train, X_train1, X_train2,X_train3))\n",
    "X_valid=np.concatenate((X_valid, X_valid1, X_valid2,X_valid3))\n",
    "X_test=np.concatenate((X_test, X_test1, X_test2,X_test3))\n",
    "\n",
    "y_train=np.concatenate((y_train, y_train1, y_train2,y_train3))\n",
    "y_valid=np.concatenate((y_valid, y_valid1, y_valid2,y_valid3))\n",
    "y_test=np.concatenate((y_test, y_test1, y_test2,y_test3))\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns= xcol)\n",
    "X_valid = pd.DataFrame(X_valid, columns= xcol)\n",
    "X_test = pd.DataFrame(X_test, columns= xcol)\n",
    "\n",
    "y_train = pd.DataFrame(y_train, columns= ycol)\n",
    "y_valid = pd.DataFrame(y_valid, columns= ycol)\n",
    "y_test = pd.DataFrame(y_test, columns= ycol)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a-time.time()\n",
    "X_train1=X_train.values\n",
    "y_train1=y_train.values\n",
    "#X_train1=X_train1.astype(float)\n",
    "\n",
    "torch_tensor = torch.tensor(X_train1)\n",
    "#torch_tensor[torch_tensor != torch_tensor]=float(0)\n",
    "t=[]\n",
    "test=0\n",
    "test0=0\n",
    "\n",
    "for j in y_train1:\n",
    "    i=j[0]\n",
    "#    print(i)\n",
    "    if (i=='Normal'):\n",
    "        t.append(int(0))\n",
    "        test0+=1\n",
    "    #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        t.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        t.append(int(2))\n",
    "        test+=1\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        t.append(int(3))\n",
    "\n",
    "labels = torch.LongTensor(t)\n",
    "\n",
    "bigX = X_valid\n",
    "#[['Next_Current_diff','Next_Pre_diff','SNext_Current_diff','SNext_Pre_diff','rcvdPK','duration(ms)','droppedPKWrongPort','sentPK','size','throughput']]  ,'channel'       \n",
    "bigY = y_valid['label']\n",
    "#print(bigY)\n",
    "X_valid1=bigX.values\n",
    "y_valid1=bigY.values\n",
    "\n",
    "vtorch_tensor = torch.tensor(X_valid1)\n",
    "v=[]\n",
    "\n",
    "for i in y_valid1:\n",
    "    #i=j[0]\n",
    "    #print(i)\n",
    "    if (i=='Normal'):\n",
    "        v.append(int(0))\n",
    "        #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        v.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        v.append(int(2))\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        v.append(int(3))\n",
    "\n",
    "vlabels = torch.LongTensor(v)\n",
    "\n",
    "\n",
    "X_test1=X_test.values\n",
    "y_test1=y_test.values\n",
    "\n",
    "#X_train1=X_train1.astype(float)\n",
    "\n",
    "ttorch_tensor = torch.tensor(X_test1)\n",
    "t=[]\n",
    "\n",
    "for j in y_test1:\n",
    "    i=j[0]\n",
    "#    print(i)\n",
    "    if (i=='Normal'):\n",
    "        t.append(int(0))\n",
    "        #t.append(int(1))\n",
    "    if (i=='DDOS_UDP_FLOOD'):\n",
    "        t.append(int(1))\n",
    "    if (i=='PLUIES_ET_ORAGES'):\n",
    "        t.append(int(2))\n",
    "    if (i=='BROUILLAGE_Trafic'):\n",
    "        t.append(int(3))\n",
    "\n",
    "tlabels = torch.LongTensor(t)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-28T23:54:13.427905Z",
     "iopub.status.busy": "2021-08-28T23:54:13.427163Z",
     "iopub.status.idle": "2021-08-28T23:54:13.454733Z",
     "shell.execute_reply": "2021-08-28T23:54:13.453458Z",
     "shell.execute_reply.started": "2021-08-28T23:54:13.427854Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "# input_size = 784 # 28x28\n",
    "num_classes = 2\n",
    "#num_epochs = 2\n",
    "batch_size = 600\n",
    "\n",
    "\n",
    "input_size = 14\n",
    "sequence_length = 28\n",
    "hidden_size = 132\n",
    "num_layers = 2\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers,batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        x=x.view(-1,1,14)\n",
    "        #print(x.shape)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, (h0,c0) )  \n",
    "        # or:\n",
    "        #out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        #out = self.fc(out)\n",
    "        x = F.log_softmax(self.fc(out), dim=1)\n",
    "        # out: (n, 10)\n",
    "        return x\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "Fonction_de_perte = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "#indices = [0,3, 299:303]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-28T23:55:09.743266Z",
     "iopub.status.busy": "2021-08-28T23:55:09.742870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  : valid accuracy :65.46890259 with total prob : 52.87433243 \n",
      "Epoch 0 : Training loss 0.00087322 and valid loss : -0.61771510 :  \n",
      "Epoch 0 : valid accuracy :65.46890259 with total prob : 87.99440002 \n",
      "119.91115093231201\n",
      "validation loss decreased , saving model (inf ==> -0.61771510)\n",
      "Epoch 1 : Training loss 0.00091664 and valid loss : -0.61851034 :  \n",
      "Epoch 1 : valid accuracy :65.46890259 with total prob : 88.13470459 \n",
      "118.7287015914917\n",
      "validation loss decreased , saving model (-0.61771510 ==> -0.61851034)\n",
      "Epoch 2 : Training loss 0.00091376 and valid loss : -0.61919230 :  \n",
      "Epoch 2 : valid accuracy :65.46890259 with total prob : 88.19651794 \n",
      "121.9922194480896\n",
      "validation loss decreased , saving model (-0.61851034 ==> -0.61919230)\n",
      "Epoch 3 : Training loss 0.00090785 and valid loss : -0.62030509 :  \n",
      "Epoch 3 : valid accuracy :65.46890259 with total prob : 88.30999756 \n",
      "131.45133686065674\n",
      "validation loss decreased , saving model (-0.61919230 ==> -0.62030509)\n",
      "Epoch 4 : Training loss 0.00089773 and valid loss : -0.62222138 :  \n",
      "Epoch 4 : valid accuracy :65.46890259 with total prob : 88.49954224 \n",
      "124.03139519691467\n",
      "validation loss decreased , saving model (-0.62030509 ==> -0.62222138)\n",
      "Epoch 5 : Training loss 0.00087920 and valid loss : -0.62576770 :  \n",
      "Epoch 5 : valid accuracy :65.46890259 with total prob : 88.80232239 \n",
      "127.06703090667725\n",
      "validation loss decreased , saving model (-0.62222138 ==> -0.62576770)\n",
      "Epoch 6 : Training loss 0.00084279 and valid loss : -0.63299073 :  \n",
      "Epoch 6 : valid accuracy :65.46890259 with total prob : 89.24128723 \n",
      "135.728440284729\n",
      "validation loss decreased , saving model (-0.62576770 ==> -0.63299073)\n",
      "Epoch 7 : Training loss 0.00076834 and valid loss : -0.64967525 :  \n",
      "Epoch 7 : valid accuracy :65.46890259 with total prob : 89.53988647 \n",
      "145.8119432926178\n",
      "validation loss decreased , saving model (-0.63299073 ==> -0.64967525)\n",
      "Epoch 8 : Training loss 0.00063121 and valid loss : -0.69672709 :  \n",
      "Epoch 8 : valid accuracy :65.46890259 with total prob : 87.41633606 \n",
      "147.8951952457428\n",
      "validation loss decreased , saving model (-0.64967525 ==> -0.69672709)\n",
      "Epoch 9 : Training loss 0.00045727 and valid loss : -0.80760577 :  \n",
      "Epoch 9 : valid accuracy :89.87683868 with total prob : 88.66891479 \n",
      "156.6258499622345\n",
      "validation loss decreased , saving model (-0.69672709 ==> -0.80760577)\n",
      "Epoch 10 : Training loss 0.00032852 and valid loss : -0.87661992 :  \n",
      "Epoch 10 : valid accuracy :89.87683868 with total prob : 93.80065918 \n",
      "130.0971164703369\n",
      "validation loss decreased , saving model (-0.80760577 ==> -0.87661992)\n",
      "Epoch 11 : Training loss 0.00026337 and valid loss : -0.89755905 :  \n",
      "Epoch 11 : valid accuracy :91.58837128 with total prob : 94.85046387 \n",
      "140.62513899803162\n",
      "validation loss decreased , saving model (-0.87661992 ==> -0.89755905)\n",
      "Epoch 12 : Training loss 0.00023092 and valid loss : -0.90610811 :  \n",
      "Epoch 12 : valid accuracy :92.46863556 with total prob : 95.94462585 \n",
      "139.82998394966125\n",
      "validation loss decreased , saving model (-0.89755905 ==> -0.90610811)\n",
      "Epoch 13 : Training loss 0.00020997 and valid loss : -0.91086127 :  \n",
      "Epoch 13 : valid accuracy :92.46863556 with total prob : 96.49499512 \n",
      "144.848735332489\n",
      "validation loss decreased , saving model (-0.90610811 ==> -0.91086127)\n",
      "Epoch 14 : Training loss 0.00019306 and valid loss : -0.91432758 :  \n",
      "Epoch 14 : valid accuracy :92.46863556 with total prob : 96.65675354 \n",
      "152.34891557693481\n",
      "validation loss decreased , saving model (-0.91086127 ==> -0.91432758)\n",
      "Epoch 15 : Training loss 0.00017842 and valid loss : -0.91742760 :  \n",
      "Epoch 15 : valid accuracy :92.47045898 with total prob : 96.61076355 \n",
      "135.7288625240326\n",
      "validation loss decreased , saving model (-0.91432758 ==> -0.91742760)\n",
      "Epoch 16 : Training loss 0.00016575 and valid loss : -0.92043665 :  \n",
      "Epoch 16 : valid accuracy :92.86968994 with total prob : 96.51787567 \n",
      "131.5264117717743\n",
      "validation loss decreased , saving model (-0.91742760 ==> -0.92043665)\n",
      "Epoch 17 : Training loss 0.00015479 and valid loss : -0.92333937 :  \n",
      "Epoch 17 : valid accuracy :93.18483734 with total prob : 96.47631836 \n",
      "139.38308954238892\n",
      "validation loss decreased , saving model (-0.92043665 ==> -0.92333937)\n",
      "Epoch 18 : Training loss 0.00014523 and valid loss : -0.92604559 :  \n",
      "Epoch 18 : valid accuracy :93.44461060 with total prob : 96.48343658 \n",
      "134.78271174430847\n",
      "validation loss decreased , saving model (-0.92333937 ==> -0.92604559)\n",
      "Epoch 19 : Training loss 0.00013680 and valid loss : -0.92847728 :  \n",
      "Epoch 19 : valid accuracy :93.69071198 with total prob : 96.55126953 \n",
      "130.50784587860107\n",
      "validation loss decreased , saving model (-0.92604559 ==> -0.92847728)\n",
      "Epoch 20 : Training loss 0.00012930 and valid loss : -0.93057681 :  \n",
      "Epoch 20 : valid accuracy :93.82765961 with total prob : 96.66111755 \n",
      "127.62274146080017\n",
      "validation loss decreased , saving model (-0.92847728 ==> -0.93057681)\n",
      "Epoch 21 : Training loss 0.00012261 and valid loss : -0.93230633 :  \n",
      "Epoch 21 : valid accuracy :93.88485718 with total prob : 96.79231262 \n",
      "123.55550169944763\n",
      "validation loss decreased , saving model (-0.93057681 ==> -0.93230633)\n",
      "Epoch 22 : Training loss 0.00011666 and valid loss : -0.93365708 :  \n",
      "Epoch 22 : valid accuracy :93.88417816 with total prob : 96.93080902 \n",
      "110.43037581443787\n",
      "validation loss decreased , saving model (-0.93230633 ==> -0.93365708)\n",
      "Epoch 23 : Training loss 0.00011140 and valid loss : -0.93465681 :  \n",
      "Epoch 23 : valid accuracy :93.85636902 with total prob : 97.06754303 \n",
      "112.05250930786133\n",
      "validation loss decreased , saving model (-0.93365708 ==> -0.93465681)\n",
      "Epoch 24 : Training loss 0.00010678 and valid loss : -0.93536514 :  \n",
      "Epoch 24 : valid accuracy :93.84361267 with total prob : 97.19773865 \n",
      "109.82082939147949\n",
      "validation loss decreased , saving model (-0.93465681 ==> -0.93536514)\n",
      "Epoch 25 : Training loss 0.00010279 and valid loss : -0.93585846 :  \n",
      "Epoch 25 : valid accuracy :93.77548218 with total prob : 97.31947327 \n",
      "109.28449940681458\n",
      "validation loss decreased , saving model (-0.93536514 ==> -0.93585846)\n",
      "Epoch 26 : Training loss 0.00009937 and valid loss : -0.93621309 :  \n",
      "Epoch 26 : valid accuracy :93.73287201 with total prob : 97.43250275 \n",
      "112.2969024181366\n",
      "validation loss decreased , saving model (-0.93585846 ==> -0.93621309)\n",
      "Epoch 27 : Training loss 0.00009648 and valid loss : -0.93649355 :  \n",
      "Epoch 27 : valid accuracy :93.70073700 with total prob : 97.53735352 \n",
      "109.02012777328491\n",
      "validation loss decreased , saving model (-0.93621309 ==> -0.93649355)\n",
      "Epoch 28 : Training loss 0.00009409 and valid loss : -0.93674800 :  \n",
      "Epoch 28 : valid accuracy :93.67179871 with total prob : 97.63470459 \n",
      "109.51047992706299\n",
      "validation loss decreased , saving model (-0.93649355 ==> -0.93674800)\n",
      "Epoch 29 : Training loss 0.00009214 and valid loss : -0.93700894 :  \n",
      "Epoch 29 : valid accuracy :93.66496277 with total prob : 97.72515106 \n",
      "111.74765110015869\n",
      "validation loss decreased , saving model (-0.93674800 ==> -0.93700894)\n",
      "Epoch 30 : Training loss 0.00009062 and valid loss : -0.93729681 :  \n",
      "Epoch 30 : valid accuracy :93.67362213 with total prob : 97.80906677 \n",
      "109.84644556045532\n",
      "validation loss decreased , saving model (-0.93700894 ==> -0.93729681)\n",
      "Epoch 31 : Training loss 0.00008948 and valid loss : -0.93762446 :  \n",
      "Epoch 31 : valid accuracy :93.66952515 with total prob : 97.88658142 \n",
      "110.73771667480469\n",
      "validation loss decreased , saving model (-0.93729681 ==> -0.93762446)\n",
      "Epoch 32 : Training loss 0.00008871 and valid loss : -0.93800166 :  \n",
      "Epoch 32 : valid accuracy :93.67362213 with total prob : 97.95715332 \n",
      "109.83105254173279\n",
      "validation loss decreased , saving model (-0.93762446 ==> -0.93800166)\n",
      "Epoch 33 : Training loss 0.00008828 and valid loss : -0.93843995 :  \n",
      "Epoch 33 : valid accuracy :93.70256042 with total prob : 98.01914978 \n",
      "109.46539807319641\n",
      "validation loss decreased , saving model (-0.93800166 ==> -0.93843995)\n",
      "Epoch 34 : Training loss 0.00008814 and valid loss : -0.93895819 :  \n",
      "Epoch 34 : valid accuracy :93.90695953 with total prob : 98.07231140 \n",
      "111.66657948493958\n",
      "validation loss decreased , saving model (-0.93843995 ==> -0.93895819)\n",
      "Epoch 35 : Training loss 0.00008821 and valid loss : -0.93958713 :  \n",
      "Epoch 35 : valid accuracy :94.19225311 with total prob : 98.11893463 \n",
      "109.38386106491089\n",
      "validation loss decreased , saving model (-0.93895819 ==> -0.93958713)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 : Training loss 0.00008833 and valid loss : -0.94036083 :  \n",
      "Epoch 36 : valid accuracy :94.53952789 with total prob : 98.16905975 \n",
      "111.01080083847046\n",
      "validation loss decreased , saving model (-0.93958713 ==> -0.94036083)\n",
      "Epoch 37 : Training loss 0.00008827 and valid loss : -0.94128389 :  \n",
      "Epoch 37 : valid accuracy :94.60857391 with total prob : 98.21931458 \n",
      "112.32580876350403\n",
      "validation loss decreased , saving model (-0.94036083 ==> -0.94128389)\n",
      "Epoch 38 : Training loss 0.00008787 and valid loss : -0.94231036 :  \n",
      "Epoch 38 : valid accuracy :94.70587158 with total prob : 98.26653290 \n",
      "109.68164467811584\n",
      "validation loss decreased , saving model (-0.94128389 ==> -0.94231036)\n",
      "Epoch 39 : Training loss 0.00008713 and valid loss : -0.94337001 :  \n",
      "Epoch 39 : valid accuracy :94.79565430 with total prob : 98.31124878 \n",
      "109.12617588043213\n",
      "validation loss decreased , saving model (-0.94231036 ==> -0.94337001)\n",
      "Epoch 40 : Training loss 0.00008619 and valid loss : -0.94440293 :  \n",
      "Epoch 40 : valid accuracy :94.86287689 with total prob : 98.35214996 \n",
      "112.83742189407349\n",
      "validation loss decreased , saving model (-0.94337001 ==> -0.94440293)\n",
      "Epoch 41 : Training loss 0.00008520 and valid loss : -0.94537097 :  \n",
      "Epoch 41 : valid accuracy :94.95607758 with total prob : 98.38846588 \n",
      "109.36249375343323\n",
      "validation loss decreased , saving model (-0.94440293 ==> -0.94537097)\n",
      "Epoch 42 : Training loss 0.00008423 and valid loss : -0.94625435 :  \n",
      "Epoch 42 : valid accuracy :95.03378296 with total prob : 98.41963196 \n",
      "109.53760433197021\n",
      "validation loss decreased , saving model (-0.94537097 ==> -0.94625435)\n",
      "Epoch 43 : Training loss 0.00008333 and valid loss : -0.94704540 :  \n",
      "Epoch 43 : valid accuracy :95.08937836 with total prob : 98.44584656 \n",
      "112.10423064231873\n",
      "validation loss decreased , saving model (-0.94625435 ==> -0.94704540)\n",
      "Epoch 44 : Training loss 0.00008251 and valid loss : -0.94774355 :  \n",
      "Epoch 44 : valid accuracy :95.15911102 with total prob : 98.46593475 \n",
      "110.22453260421753\n",
      "validation loss decreased , saving model (-0.94704540 ==> -0.94774355)\n",
      "Epoch 45 : Training loss 0.00008176 and valid loss : -0.94835216 :  \n",
      "Epoch 45 : valid accuracy :95.23886108 with total prob : 98.48223114 \n",
      "110.89079165458679\n",
      "validation loss decreased , saving model (-0.94774355 ==> -0.94835216)\n",
      "Epoch 46 : Training loss 0.00008110 and valid loss : -0.94887651 :  \n",
      "Epoch 46 : valid accuracy :95.31064606 with total prob : 98.49525452 \n",
      "109.82564163208008\n",
      "validation loss decreased , saving model (-0.94835216 ==> -0.94887651)\n",
      "Epoch 47 : Training loss 0.00008050 and valid loss : -0.94932274 :  \n",
      "Epoch 47 : valid accuracy :95.38515472 with total prob : 98.50541687 \n",
      "110.90297150611877\n",
      "validation loss decreased , saving model (-0.94887651 ==> -0.94932274)\n",
      "Epoch 48 : Training loss 0.00007996 and valid loss : -0.94969714 :  \n",
      "Epoch 48 : valid accuracy :95.42617798 with total prob : 98.51329041 \n",
      "111.9908697605133\n",
      "validation loss decreased , saving model (-0.94932274 ==> -0.94969714)\n",
      "Epoch 49 : Training loss 0.00007949 and valid loss : -0.95000587 :  \n",
      "Epoch 49 : valid accuracy :95.44828033 with total prob : 98.51658630 \n",
      "110.19440817832947\n",
      "validation loss decreased , saving model (-0.94969714 ==> -0.95000587)\n",
      "Epoch 50 : Training loss 0.00007906 and valid loss : -0.95025476 :  \n",
      "Epoch 50 : valid accuracy :95.51777649 with total prob : 98.51560974 \n",
      "119.30776381492615\n",
      "validation loss decreased , saving model (-0.95000587 ==> -0.95025476)\n",
      "Epoch 51 : Training loss 0.00007868 and valid loss : -0.95044925 :  \n",
      "Epoch 51 : valid accuracy :95.60596466 with total prob : 98.51195526 \n",
      "127.96313071250916\n",
      "validation loss decreased , saving model (-0.95025476 ==> -0.95044925)\n",
      "Epoch 52 : Training loss 0.00007834 and valid loss : -0.95059437 :  \n",
      "Epoch 52 : valid accuracy :95.67774200 with total prob : 98.50576782 \n",
      "126.00366163253784\n",
      "validation loss decreased , saving model (-0.95044925 ==> -0.95059437)\n",
      "Epoch 53 : Training loss 0.00007803 and valid loss : -0.95069477 :  \n",
      "Epoch 53 : valid accuracy :95.71670532 with total prob : 98.49699402 \n",
      "122.66025757789612\n",
      "validation loss decreased , saving model (-0.95059437 ==> -0.95069477)\n",
      "Epoch 54 : Training loss 0.00007776 and valid loss : -0.95075472 :  \n",
      "Epoch 54 : valid accuracy :95.72924042 with total prob : 98.48502350 \n",
      "119.46327948570251\n",
      "validation loss decreased , saving model (-0.95069477 ==> -0.95075472)\n",
      "Epoch 55 : Training loss 0.00007752 and valid loss : -0.95077818 :  \n",
      "Epoch 55 : valid accuracy :95.74770355 with total prob : 98.47029114 \n",
      "119.0593912601471\n",
      "validation loss decreased , saving model (-0.95075472 ==> -0.95077818)\n",
      "Epoch 56 : Training loss 0.00007731 and valid loss : -0.95076878 :  \n",
      "Epoch 56 : valid accuracy :95.77550507 with total prob : 98.45346832 \n",
      "113.02592897415161\n",
      "Epoch 57 : Training loss 0.00007712 and valid loss : -0.95072990 :  \n",
      "Epoch 57 : valid accuracy :95.80216217 with total prob : 98.43552399 \n",
      "114.94830775260925\n",
      "Epoch 58 : Training loss 0.00007696 and valid loss : -0.95066468 :  \n",
      "Epoch 58 : valid accuracy :95.82357788 with total prob : 98.41629791 \n",
      "112.07053852081299\n",
      "Epoch 59 : Training loss 0.00007682 and valid loss : -0.95057598 :  \n",
      "Epoch 59 : valid accuracy :95.83975983 with total prob : 98.39599609 \n",
      "117.57200264930725\n",
      "Epoch 60 : Training loss 0.00007669 and valid loss : -0.95046649 :  \n",
      "Epoch 60 : valid accuracy :95.86049652 with total prob : 98.37435913 \n",
      "111.20553994178772\n",
      "Epoch 61 : Training loss 0.00007659 and valid loss : -0.95033866 :  \n",
      "Epoch 61 : valid accuracy :95.88305664 with total prob : 98.35203552 \n",
      "115.79081153869629\n",
      "Epoch 62 : Training loss 0.00007650 and valid loss : -0.95019476 :  \n",
      "Epoch 62 : valid accuracy :95.89945984 with total prob : 98.32953644 \n",
      "111.27418088912964\n",
      "Epoch 63 : Training loss 0.00007643 and valid loss : -0.95003688 :  \n",
      "Epoch 63 : valid accuracy :95.92839813 with total prob : 98.30755615 \n",
      "110.07132887840271\n",
      "Epoch 64 : Training loss 0.00007636 and valid loss : -0.94986696 :  \n",
      "Epoch 64 : valid accuracy :95.95711517 with total prob : 98.28642273 \n",
      "117.0443766117096\n",
      "Epoch 65 : Training loss 0.00007630 and valid loss : -0.94968681 :  \n",
      "Epoch 65 : valid accuracy :95.28900146 with total prob : 98.27961731 \n",
      "112.02475833892822\n",
      "Epoch 66 : Training loss 0.00007624 and valid loss : -0.94949820 :  \n",
      "Epoch 66 : valid accuracy :94.48506927 with total prob : 98.31085205 \n",
      "116.57635283470154\n",
      "Epoch 67 : Training loss 0.00007616 and valid loss : -0.94930287 :  \n",
      "Epoch 67 : valid accuracy :94.48848724 with total prob : 98.35910034 \n",
      "119.87016367912292\n",
      "Epoch 68 : Training loss 0.00007606 and valid loss : -0.94910265 :  \n",
      "Epoch 68 : valid accuracy :94.47869110 with total prob : 98.40690613 \n",
      "115.53092432022095\n",
      "Epoch 69 : Training loss 0.00007593 and valid loss : -0.94889953 :  \n",
      "Epoch 69 : valid accuracy :94.48461914 with total prob : 98.45399475 \n",
      "128.96472787857056\n",
      "Epoch 70 : Training loss 0.00007576 and valid loss : -0.94869567 :  \n",
      "Epoch 70 : valid accuracy :94.48461914 with total prob : 98.49948120 \n",
      "127.93145823478699\n",
      "Epoch 71 : Training loss 0.00007552 and valid loss : -0.94849348 :  \n",
      "Epoch 71 : valid accuracy :94.48461914 with total prob : 98.54300690 \n",
      "124.63049983978271\n",
      "Epoch 72 : Training loss 0.00007522 and valid loss : -0.94829551 :  \n",
      "Epoch 72 : valid accuracy :94.48461914 with total prob : 98.58447266 \n",
      "132.1449978351593\n",
      "Epoch 73 : Training loss 0.00007486 and valid loss : -0.94810434 :  \n",
      "Epoch 73 : valid accuracy :94.48461914 with total prob : 98.62378693 \n",
      "133.4754877090454\n",
      "Epoch 74 : Training loss 0.00007443 and valid loss : -0.94792246 :  \n",
      "Epoch 74 : valid accuracy :94.48461914 with total prob : 98.66092682 \n",
      "127.11860990524292\n",
      "Epoch 75 : Training loss 0.00007394 and valid loss : -0.94775203 :  \n",
      "Epoch 75 : valid accuracy :94.48461914 with total prob : 98.69594574 \n",
      "146.90871286392212\n",
      "Epoch 76 : Training loss 0.00007341 and valid loss : -0.94759471 :  \n",
      "Epoch 76 : valid accuracy :94.48461914 with total prob : 98.72892761 \n",
      "189.32764601707458\n",
      "Epoch 77 : Training loss 0.00007287 and valid loss : -0.94745156 :  \n",
      "Epoch 77 : valid accuracy :94.48461914 with total prob : 98.76004028 \n",
      "581.5229184627533\n",
      "Epoch 78 : Training loss 0.00007232 and valid loss : -0.94732293 :  \n",
      "Epoch 78 : valid accuracy :94.48461914 with total prob : 98.78942871 \n",
      "155.29925537109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 : Training loss 0.00007178 and valid loss : -0.94720851 :  \n",
      "Epoch 79 : valid accuracy :94.48461914 with total prob : 98.81729126 \n",
      "120.45455503463745\n",
      "Epoch 80 : Training loss 0.00007127 and valid loss : -0.94710744 :  \n",
      "Epoch 80 : valid accuracy :94.48461914 with total prob : 98.84373474 \n",
      "152.01790952682495\n",
      "Epoch 81 : Training loss 0.00007079 and valid loss : -0.94701848 :  \n",
      "Epoch 81 : valid accuracy :94.48461914 with total prob : 98.86890411 \n",
      "120.26044917106628\n",
      "Epoch 82 : Training loss 0.00007035 and valid loss : -0.94694017 :  \n",
      "Epoch 82 : valid accuracy :94.48461914 with total prob : 98.89283752 \n",
      "121.34832811355591\n",
      "Epoch 83 : Training loss 0.00006996 and valid loss : -0.94687098 :  \n",
      "Epoch 83 : valid accuracy :94.48461914 with total prob : 98.91562653 \n",
      "134.37672424316406\n",
      "Epoch 84 : Training loss 0.00006960 and valid loss : -0.94680950 :  \n",
      "Epoch 84 : valid accuracy :94.48461914 with total prob : 98.93730927 \n",
      "167.629163980484\n",
      "Epoch 85 : Training loss 0.00006928 and valid loss : -0.94675442 :  \n",
      "Epoch 85 : valid accuracy :94.48461914 with total prob : 98.95789337 \n",
      "505.9485459327698\n",
      "Epoch 86 : Training loss 0.00006899 and valid loss : -0.94670463 :  \n",
      "Epoch 86 : valid accuracy :94.48461914 with total prob : 98.97744751 \n",
      "147.43655371665955\n",
      "Epoch 87 : Training loss 0.00006873 and valid loss : -0.94665918 :  \n",
      "Epoch 87 : valid accuracy :94.48461914 with total prob : 98.99599457 \n",
      "152.15164470672607\n",
      "Epoch 88 : Training loss 0.00006850 and valid loss : -0.94661731 :  \n",
      "Epoch 88 : valid accuracy :94.48461914 with total prob : 99.01357269 \n",
      "139.2877016067505\n",
      "Epoch 89 : Training loss 0.00006829 and valid loss : -0.94657841 :  \n",
      "Epoch 89 : valid accuracy :94.48461914 with total prob : 99.03023529 \n",
      "130.86496591567993\n",
      "Epoch 90 : Training loss 0.00006810 and valid loss : -0.94654197 :  \n",
      "Epoch 90 : valid accuracy :94.48461914 with total prob : 99.04603577 \n",
      "137.52215385437012\n",
      "Epoch 91 : Training loss 0.00006792 and valid loss : -0.94650760 :  \n",
      "Epoch 91 : valid accuracy :94.48461914 with total prob : 99.06099701 \n",
      "118.68038821220398\n",
      "Epoch 92 : Training loss 0.00006776 and valid loss : -0.94647499 :  \n",
      "Epoch 92 : valid accuracy :94.48461914 with total prob : 99.07521820 \n",
      "114.91291928291321\n",
      "Epoch 93 : Training loss 0.00006761 and valid loss : -0.94644388 :  \n",
      "Epoch 93 : valid accuracy :94.48461914 with total prob : 99.08869171 \n",
      "120.4138388633728\n",
      "Epoch 94 : Training loss 0.00006747 and valid loss : -0.94641408 :  \n",
      "Epoch 94 : valid accuracy :94.48461914 with total prob : 99.10147858 \n",
      "115.97581672668457\n",
      "Epoch 95 : Training loss 0.00006734 and valid loss : -0.94638544 :  \n",
      "Epoch 95 : valid accuracy :94.48461914 with total prob : 99.11363983 \n",
      "114.82943630218506\n",
      "Epoch 96 : Training loss 0.00006721 and valid loss : -0.94635780 :  \n",
      "Epoch 96 : valid accuracy :94.48461914 with total prob : 99.12520599 \n",
      "115.9495906829834\n",
      "Epoch 97 : Training loss 0.00006710 and valid loss : -0.94633108 :  \n",
      "Epoch 97 : valid accuracy :94.48461914 with total prob : 99.13619995 \n",
      "115.01050233840942\n",
      "Epoch 98 : Training loss 0.00006699 and valid loss : -0.94630518 :  \n",
      "Epoch 98 : valid accuracy :94.48461914 with total prob : 99.14667511 \n",
      "116.30603623390198\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f9a60b579414>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFonction_de_perte\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#labels1=labels[loc]\n",
    "a=time.time()\n",
    "#inputs=torch_tensor[:,:].to(device) \n",
    "##labels1=labels[:].to(device)\n",
    "#print(len(inputs))\n",
    "#print(len(labels1))\n",
    "#vinputs =vtorch_tensor[:,:].to(device)\n",
    "#vlabels1=vlabels[:].to(device)\n",
    "\n",
    "epochs = 400\n",
    "valid_loss=0\n",
    "accuracy=0\n",
    "valid_loss_min=np.Inf\n",
    "#batch_size = 100\n",
    "\n",
    "####33  !!!!!!!!!! This shit cost alot of time ----> do not use it \n",
    "#b=time.time()\n",
    "#with torch.no_grad():\n",
    "   # for i, x in enumerate(vinputs):\n",
    "       # x2=x[None,:]\n",
    "       # output = torch.exp(model(x2))\n",
    "       # l2=vlabels1[i][None]\n",
    "       # test_loss+=Fonction_de_perte(output, l2)\n",
    "     #   output = torch.exp(output)\n",
    "      #  top_p , top_c = output.topk(1, dim=1)\n",
    "     #   equals = top_c==vlabels1[i].view(*top_c.shape)\n",
    "    #    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "   # tain_losses.append(running_loss/len(inputs))\n",
    "   # test_losses.append(test_loss/len(vinputs))\n",
    "   # print ('test accuracy :{0}, test loss : {2} ,  time {1} '.format(accuracy*100/len(vinputs) , time.time()-b ,test_loss/len(vinputs) ))\n",
    "\n",
    "######## ---> use this :) \n",
    "b=time.time()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = torch.exp(model(vinputs))\n",
    "    valid_loss=Fonction_de_perte(output, vlabels1)\n",
    "top_p , top_c = output.topk(1, dim=1)\n",
    "del(output)\n",
    "propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "equals = top_c==vlabels1.view(*top_c.shape)\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "#print ('valid accuracy :{0:.8f} with total prob : {3:.8f} and  valid loss : {2:.8f} ,  time {1:.6f} '.format(accuracy*100 , time.time()-b ,test_loss,propabilities))\n",
    "print ('Epoch  : valid accuracy :{0:.8f} with total prob : {1:.8f} '.format(accuracy*100  ,propabilities))\n",
    "\n",
    "####\n",
    "#print(inputs.shape[0])\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    c=time.time()\n",
    "    running_loss = 0\n",
    "    #for i, x in enumerate(inputs):\n",
    "    for i in range(0,inputs.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #indices = [i:i+batch_size]\n",
    "        #print(i+batch_size)\n",
    "        batch_x, batch_y = inputs[i:i+batch_size], labels1[i:i+batch_size]\n",
    "        \n",
    "        #x2=x[None,:]\n",
    "        output = model.forward(batch_x)\n",
    "        \n",
    "        #output = model.forward(x2)\n",
    "        #l2=labels1[i][None]\n",
    "        #print(l2)\n",
    "        #loss = Fonction_de_perte(output, l2)\n",
    "        loss = Fonction_de_perte(output, batch_y)\n",
    "      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "        accuracy=0\n",
    "        #print(l2)\n",
    "        #print(l2)\n",
    "        \n",
    "        b=time.time()\n",
    "        with torch.no_grad():\n",
    "            ##\n",
    "            #with torch.no_grad():\n",
    "            #for i, x in enumerate(vinputs):\n",
    "             #   x2=x[None,:]\n",
    "              #  output = torch.exp(model(x2))\n",
    "               # l2=vlabels1[i][None]\n",
    "                #valid_loss1+=Fonction_de_perte(output, l2)\n",
    "            #test_loss=test_loss/len(vinputs)\n",
    "            ##\n",
    "                \n",
    "            model.eval()\n",
    "            output = torch.exp(model(vinputs))\n",
    "            valid_loss=Fonction_de_perte(output, vlabels1)\n",
    "        top_p , top_c = output.topk(1, dim=1)\n",
    "        del(output)\n",
    "        propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "        equals = top_c==vlabels1.view(*top_c.shape)\n",
    "        accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        print('Epoch {2} : Training loss {0:.8f} and valid loss : {1:.8f} :  '.format(running_loss/len(inputs),valid_loss, e))\n",
    "   \n",
    "        print ('Epoch {2} : valid accuracy :{0:.8f} with total prob : {1:.8f} '.format(accuracy*100  ,propabilities, e))\n",
    "        print(time.time()-c)\n",
    "    #print('validation loss with iterations {0}'.format(valid_loss1/len(vinputs) ) )   \n",
    "        train_losses.append(running_loss/len(inputs))\n",
    "        valid_losses.append(valid_loss)\n",
    "        if (valid_loss<valid_loss_min):\n",
    "            print('validation loss decreased , saving model ({:.8f} ==> {:.8f})'.format(valid_loss_min,valid_loss))\n",
    "            torch.save(model.state_dict(),'3mod_temp_400_lstm_b.pth')\n",
    "            valid_loss_min=valid_loss\n",
    "print(time.time()-a)            \n",
    "torch.save(model.state_dict(),'3mod_temp_400_lstm_b#2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "test accuracy :95.78359985 with total prob : 98.24584961 and  test loss : -0.94939091 ,  time 28.44861555 \n",
      "la précision de detection globale: 94.93909121791648 \n",
      "detection des communication normal: 95.50471940 with accuracy 96.34678628 ( 367088/381007 )\n",
      "details normal classed udp :3.65321, pluies 0.00000 , jam 0.00000 (ou 100.00000,0.00000,0.00000) \n",
      "detection du deni de service par udp flood 93.86542566 with accuracy 94.71458036 ( 190113/200722 )\n",
      "details udp classed normal :5.28542, pluies 0.00000 , jam 0.00000 (ou 100.00000,0.00000,0.00000) \n",
      "=====================\n",
      "<================================>\n",
      "Total_time\n",
      "413.7488970756531\n"
     ]
    }
   ],
   "source": [
    "a=time.time()\n",
    "summm=[]\n",
    "sum1=[]\n",
    "sum2=[]\n",
    "sum3=[]\n",
    "sum4=[]\n",
    "\n",
    "#tinputs =ttorch_tensor[:,:] \n",
    "#tlabels1=tlabels[:]\n",
    "\n",
    "\n",
    "#tinputs =torch_tensor[:,:] \n",
    "#tlabels1=labels[:]\n",
    "\n",
    "print('=====================')\n",
    "b=time.time()\n",
    "model.eval()\n",
    "output = torch.exp(model(tinputs))\n",
    "test_loss=Fonction_de_perte(output, tlabels1)\n",
    "top_p , top_c = output.topk(1, dim=1)\n",
    "del(output)\n",
    "propabilities= torch.mean(top_p.type(torch.FloatTensor))*100\n",
    "equals = top_c==tlabels1.view(*top_c.shape)\n",
    "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "model.train()\n",
    "print ('test accuracy :{0:.8f} with total prob : {3:.8f} and  test loss : {2:.8f} ,  time {1:.8f} '.format(accuracy*100 , time.time()-b ,test_loss ,propabilities))\n",
    "\n",
    "class_correct = list(0. for i in range (4))\n",
    "class_total = list(0. for i in range (4))\n",
    "\n",
    "C_n_udp=0\n",
    "C_n_pluies=0\n",
    "C_n_jam=0\n",
    "C_n_total=0\n",
    "\n",
    "\n",
    "C_u_normal=0\n",
    "C_u_jam=0\n",
    "C_u_pluies=0\n",
    "C_u_total=0\n",
    "\n",
    "C_p_normal=0\n",
    "C_p_udp=0\n",
    "C_p_jam=0\n",
    "C_p_total=0\n",
    "\n",
    "C_j_normal=0\n",
    "C_j_udp=0\n",
    "C_j_pluies=0\n",
    "C_j_total=0\n",
    "\n",
    "for i, x in enumerate(tinputs):\n",
    "    optimizer.zero_grad()\n",
    "    x2=x[None,:]\n",
    "    with torch.no_grad():\n",
    "        output = torch.exp(model(x2))\n",
    "    out=output.detach().numpy()*100\n",
    " \n",
    "    l3=tlabels1[i].item()\n",
    "  \n",
    "    if(l3==0):\n",
    "        summm.append(out[0][0])\n",
    "        sum1.append(out[0][0])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_n_udp+=1\n",
    "            #if(top_c[i][0]==2):\n",
    "            #    C_n_pluies+=1\n",
    "            #if(top_c[i][0]==3):\n",
    "            #    C_n_jam +=1\n",
    "            \n",
    "            C_n_total +=1\n",
    "            \n",
    "    if(l3==1):\n",
    "        summm.append(out[0][1])\n",
    "        sum2.append(out[0][1])\n",
    "        if(top_c[i][0]!=l3):\n",
    "           # if(top_c[i][0]==3):\n",
    "            #    C_u_jam+=1\n",
    "           # if(top_c[i][0]==2):\n",
    "           #     C_u_pluies+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_u_normal +=1\n",
    "            \n",
    "            C_u_total +=1\n",
    "\n",
    "    if(l3==2):\n",
    "        summm.append(out[0][2])\n",
    "        sum3.append(out[0][2])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_p_udp+=1\n",
    "            if(top_c[i][0]==3):\n",
    "                C_p_jam+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_p_normal +=1\n",
    "            \n",
    "            C_p_total +=1\n",
    "\n",
    "    if(l3==3):\n",
    "        summm.append(out[0][3])\n",
    "        sum4.append(out[0][3])\n",
    "        if(top_c[i][0]!=l3):\n",
    "            if(top_c[i][0]==1):\n",
    "                C_j_udp+=1\n",
    "            if(top_c[i][0]==2):\n",
    "                C_j_pluies+=1\n",
    "            if(top_c[i][0]==0):\n",
    "                C_j_normal +=1\n",
    "            \n",
    "            C_j_total +=1\n",
    "\n",
    "    class_total[l3]+=1\n",
    "    class_correct[l3]+=equals[i][0]\n",
    "    #print(output)\n",
    "print('la précision de detection globale: {0} '.format(mean(summm)))\n",
    "print('detection des communication normal: {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum1), class_correct[0]*100/class_total[0] ,class_correct[0] ,class_total[0]))\n",
    "if(C_n_total!=0):\n",
    "    print('details normal classed udp :{0:.5f}, pluies {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_n_udp*100/class_total[0],C_n_pluies*100/class_total[0],C_n_jam*100/class_total[0],C_n_udp*100/C_n_total,C_n_pluies*100/C_n_total,C_n_jam*100/C_n_total))\n",
    "else:\n",
    "    print('detection normal 100')\n",
    "\n",
    "print('detection du deni de service par udp flood {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum2), class_correct[1]*100/class_total[1] ,class_correct[1] ,class_total[1]))\n",
    "if(C_u_total!=0):\n",
    "    print('details udp classed normal :{0:.5f}, pluies {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_u_normal*100/class_total[1],C_u_pluies*100/class_total[1],C_u_jam*100/class_total[1],C_u_normal*100/C_u_total,C_u_pluies*100/C_u_total,C_u_jam*100/C_u_total))\n",
    "else:\n",
    "    print('detection udp flood 100')\n",
    "    \n",
    "#print('detection du deni naturel : pluies et orages : {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum3), class_correct[2]*100/class_total[2] ,class_correct[2] ,class_total[2]))\n",
    "#if(C_p_total!=0):\n",
    "#    print('details pluies classed udp :{0:.5f}, normal {1:.5f} , jam {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_p_udp*100/class_total[2],C_p_normal*100/class_total[2],C_p_jam*100/class_total[2],C_p_udp*100/C_p_total,C_p_normal*100/C_p_total,C_p_jam*100/C_p_total))\n",
    "#else:\n",
    " #   print('detection pluies et orages 100')\n",
    "\n",
    "#print('detection du deni naturel : jam : {0:.8f} with accuracy {1:.8f} ( {2:.0f}/{3:.0f} )'.format(mean(sum4), class_correct[3]*100/class_total[3] ,class_correct[3] ,class_total[3]))\n",
    "#if(C_j_total!=0):\n",
    "#    print('details jam classed udp :{0:.5f}, pluies {1:.5f} , normal {2:.5f} (ou {3:.5f},{4:.5f},{5:.5f}) '.format(C_j_udp*100/class_total[3],C_j_pluies*100/class_total[3],C_j_normal*100/class_total[3],C_j_udp*100/C_j_total,C_j_pluies*100/C_j_total,C_j_normal*100/C_j_total))\n",
    "#else:\n",
    "#    print('detection brouillage 100')\n",
    "print('=====================')\n",
    "\n",
    "print('<================================>')\n",
    "print('Total_time')\n",
    "print(time.time()-a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('3mod_temp_400_lstm_b.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2576ae8d490>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAsx0lEQVR4nO3deZxU5Z3v8c+vlq7em60bmgYEjRsCAnZwSxRUjFvENWJ0Iia5Xk2cTMwk0Zg7GSe5d2LMRDO5MeaaxGiiozFGlInEBWMEx0m0QVEQEEQQaKCBpvelqrqe+8c53RRNd7N0dVUv3/frVa+zPOdUPcelvv08zzlPmXMOEREZ2gKZroCIiGSewkBERBQGIiKiMBARERQGIiKCwkBEREhRGJjZQ2ZWZWaruik3M/uJmW0ws3fMbGZS2QVmts4vuyMV9RERkcOTqpbBw8AFPZRfCBzrv24CHgAwsyBwv18+GbjWzCanqE4iInKIUhIGzrmlQHUPh8wDfuM8fwWGmVkpMAvY4Jzb6JyLAk/4x4qISBqF0vQ5ZcCWpO2t/r6u9p/a1RuY2U14rQry8vJOOeGEE/qmpqm2/W12JYpwhWMpKYhkujYiMoQtX758t3OuuKuydIWBdbHP9bD/wJ3OPQg8CFBeXu4qKipSV7u+9P0J/Kb5dLaddhffuujETNdGRIYwM9vcXVm6wmArMD5pexxQCWR1s3/wCGdTEI3R0BrPdE1ERLqVrltLFwGf8+8qOg2odc5tB94EjjWzSWaWBcz3jx08wjnkBeI0RdsyXRMRkW6lpGVgZo8Ds4FRZrYV+GcgDOCc+zmwGLgI2AA0ATf6ZXEzuxV4AQgCDznnVqeiTv1GKIe8QFQtAxHp11ISBs65aw9S7oAvd1O2GC8sBqdwDrkWo1FhICL9mJ5A7mvhHHKsVWEgIv2awqCvhXPIJkajxgxEpB9TGPS1cA7ZqGUgIv1bum4tHbpCOWS5VhqiCgMR6b/UMuhrYS8MGlvj6PemRfq3PXv2MH36dKZPn86YMWMoKyvr2I5Goz2eW1FRwVe+8pXD+ryJEyeye/fu3lQ5ZdQy6GvhHMKJFhIOWuMJssPBTNdIRLoxcuRI3n77bQDuuusu8vPz+frXv95RHo/HCYW6/tosLy+nvLw8HdXsE2oZ9LVwDqFEK4CeNRAZgBYsWMDXvvY15syZw+23384bb7zBGWecwYwZMzjjjDNYt24dAH/5y1+45JJLAC9IPv/5zzN79myOPvpofvKTnxz0c+69916mTJnClClT+PGPfwxAY2MjF198MSeffDJTpkzhd7/7HQB33HEHkydPZtq0afuFVW+oZdDXQjkEXZwgbTS2xhmVr8nqRA7Fv/znat6rrEvpe04eW8g/f/qkwz7v/fffZ8mSJQSDQerq6li6dCmhUIglS5Zw55138oc//OGAc9auXcsrr7xCfX09xx9/PLfccgvhcLjL91++fDm//vWv+dvf/oZzjlNPPZWzzz6bjRs3MnbsWJ577jkAamtrqa6uZuHChaxduxYzo6am5rCvpytqGfS1cA4A2URpbNXtpSID0dVXX00w6HXx1tbWcvXVVzNlyhRuu+02Vq/uetKEiy++mEgkwqhRoygpKWHnzp3dvv9rr73G5ZdfTl5eHvn5+VxxxRUsW7aMqVOnsmTJEm6//XaWLVtGUVERhYWFZGdn88UvfpGnn36a3NzclFyjWgZ9zQ+DHKI06o4ikUN2JH/B95W8vLyO9X/6p39izpw5LFy4kE2bNjF79uwuz4lE9vUCBINB4vHu///v7uaS4447juXLl7N48WK+9a1vcf755/Od73yHN954g5dffpknnniCn/70p/z5z38+sgtLopZBX2tvGZjmJxIZDGpraykrKwPg4YcfTsl7nnXWWTzzzDM0NTXR2NjIwoUL+eQnP0llZSW5ublcf/31fP3rX2fFihU0NDRQW1vLRRddxI9//OOOAe/eUsugr3V0E+nBM5HB4Jvf/CY33HAD9957L+ecc05K3nPmzJksWLCAWbNmAfDFL36RGTNm8MILL/CNb3yDQCBAOBzmgQceoL6+nnnz5tHS0oJzjvvuuy8ldbCBeO/7gPpxm7WL4YlruaT1f/O5Ky7jMx8ff/BzRET6gJktd851ef+ruon6WtKYgbqJRKS/Uhj0tfYw0MylItKPKQz6mh8G+cG45icSkX5LYdDXQl4YFIViNOk5AxHppxQGfc1vGRSG2tRNJCL9VkrCwMwuMLN1ZrbBzO7oovwbZva2/1plZm1mNsIv22Rm7/plA+QWocMQ9p4OLArFNIAsIv1Wr8PAzILA/cCFwGTgWjObnHyMc+6HzrnpzrnpwLeAV51z1UmHzPHLB+6Uf90JZwOQH4jpCWSRQSg/Px+AyspKrrrqqi6PmT17Nl3dDt/d/kxIRctgFrDBObfRORcFngDm9XD8tcDjKfjcgSHUPoAc09xEIoPY2LFjeeqppzJdjSOWijAoA7YkbW/19x3AzHKBC4DkKf4c8KKZLTezm1JQn/4lEIBghLxAXGMGIv3c7bffzs9+9rOO7bvuuosf/ehHNDQ0cO655zJz5kymTp3Ks88+e8C5mzZtYsqUKQA0Nzczf/58pk2bxjXXXENzc/NBP/vxxx9n6tSpTJkyhdtvvx2AtrY2FixYwJQpU5g6dWrH08Y/+clPOqawnj9/fiouPSXTUVgX+7p7rPnTwH916iI60zlXaWYlwEtmttY5t/SAD/GC4iaACRMm9LbO6RXOIS8QpbFZYSByyP50B+x4N7XvOWYqXHh3t8Xz58/nq1/9Kl/60pcAePLJJ3n++efJzs5m4cKFFBYWsnv3bk477TQuvfRSzLr6+oMHHniA3Nxc3nnnHd555x1mzpzZY7UqKyu5/fbbWb58OcOHD+f888/nmWeeYfz48Wzbto1Vq1YBdExXfffdd/Phhx8SiUT61RTWW4HkORbGAZXdHDufTl1EzrlKf1kFLMTrdjqAc+5B51y5c668uLi415VOq3AOORalMapuIpH+bMaMGVRVVVFZWcnKlSsZPnw4EyZMwDnHnXfeybRp0zjvvPPYtm1bj1NSL126lOuvvx6AadOmMW3atB4/980332T27NkUFxcTCoW47rrrWLp0KUcffTQbN27k7//+73n++ecpLCzseM/rrruORx99tNtfXjtcqXiXN4FjzWwSsA3vC/+znQ8ysyLgbOD6pH15QMA5V++vnw98NwV16l/COeQSpb4lRlvCEQx0/deEiCTp4S/4vnTVVVfx1FNPsWPHjo4umMcee4xdu3axfPlywuEwEydOpKWlpcf36a7V0JXu5ogbPnw4K1eu5IUXXuD+++/nySef5KGHHuK5555j6dKlLFq0iO9973usXr2616HQ65aBcy4O3Aq8AKwBnnTOrTazm83s5qRDLwdedM41Ju0bDbxmZiuBN4DnnHPP97ZO/U44l7xAjISD6saef1RbRDJr/vz5PPHEEzz11FMddwfV1tZSUlJCOBzmlVdeYfPmzT2+x1lnncVjjz0GwKpVq3jnnXd6PP7UU0/l1VdfZffu3bS1tfH4449z9tlns3v3bhKJBFdeeSXf+973WLFiBYlEgi1btjBnzhzuueceampqaGho6PV1p6R94ZxbDCzutO/nnbYfBh7utG8jcHIq6tCvhbLJiXkhUFXfQnGBfvpSpL866aSTqK+vp6ysjNLSUgCuu+46Pv3pT1NeXs706dM54YQTenyPW265hRtvvJFp06Yxffr0jqmpu1NaWsr3v/995syZg3OOiy66iHnz5rFy5UpuvPFGEokEAN///vdpa2vj+uuvp7a2Fucct912G8OGDev1dWsK63R4+BLqm5uZuvlr/PrGjzPn+JJM10hEhiBNYZ1p4RwizmsZ7KprzXBlREQOpDBIh3AOoYQ32FRV3/Ogk4hIJigM0iGUQyDeQlFOmJ1qGYhIP6QwSIdwDsSaKSmIqGUgIv2SwiAdwjkQa6GkMEJVvVoGItL/KAzSIZwDsSZK8iNUqZtIRPohhUE6hHPAtTGmIMiu+tZunzYUEckUhUE6+NNYl+ZBtC1BbXMswxUSEdmfwiAd/J++HJPjPUWoO4pEpL9RGKSDHwYl2V73kO4oEpH+RmGQDn4YjMr2WgYaRBaR/kZhkA7+mMHILO/3DHR7qYj0NwqDdPBbBjkWJT8SUjeRiPQ7CoN0COd6y1iL/xSyWgYi0r8oDNIhnO0tY00UF0SoqlPLQET6F4VBOnS0DJopKcxWy0BE+h2FQTqE/JZB3J+srk5PIYtI/6IwSAd/AJlYM6MLIzTH2mhojWe2TiIiSVISBmZ2gZmtM7MNZnZHF+WzzazWzN72X9851HMHhaQwKCnwWgnqKhKR/iTU2zcwsyBwPzAX2Aq8aWaLnHPvdTp0mXPukiM8d2Br7ybyf9MAvAfPjinOz2ClRET2SUXLYBawwTm30TkXBZ4A5qXh3IHDzBtEjjdTUuiHgZ41EJF+JBVhUAZsSdre6u/r7HQzW2lmfzKzkw7zXMzsJjOrMLOKXbt2paDaaRbKhlgzxe3dRJqSQkT6kVSEgXWxr/OtMiuAo5xzJwP/F3jmMM71djr3oHOu3DlXXlxcfKR1zZxwLsRaKMwOEQkF1DIQkX4lFWGwFRiftD0OqEw+wDlX55xr8NcXA2EzG3Uo5w4a4WyINWFmjNazBiLSz6QiDN4EjjWzSWaWBcwHFiUfYGZjzMz89Vn+5+45lHMHjXAOxL3WQPuzBiIi/UWv7yZyzsXN7FbgBSAIPOScW21mN/vlPweuAm4xszjQDMx33lNXXZ7b2zr1SyHvd5ABSgojrNtRn+EKiYjs0+swgI6un8Wd9v08af2nwE8P9dxBKZwDsWYASgqyWfb+7gxXSERkHz2BnC5JYVBcEKG+NU5TVE8hi0j/oDBIl6QwmDgyD4APqhozWSMRkQ4Kg3QJ53YMIE8pKwRgVWVtJmskItJBYZAuoeyOAeQJI3IpyA7x7jaFgYj0DwqDdEnqJjIzpowtYrXCQET6CYVBuuQM91oGUW+cYOq4ItbsqCfWlshwxUREFAbpM+wob1m7FYCTxhYSjSdYv7Mhg5USEfEoDNJlmD/rRs1HAEwtKwJglbqKRKQfUBiky7AJ3tIPg4kj88iPhHRHkYj0CwqDdMkfA4FwRxgEAsbksYW6o0hE+gWFQboEAlA0Dmr3/XzD1LIi1myvI65BZBHJMIVBOg0b39EyAO/hs5ZYgg926UlkEckshUE6DZsANfu3DECDyCKSeQqDdBp2FDTsgJg3LcWkUfnkZgU1biAiGacwSKci//bSum0ABAPG5NJCVuuOIhHJMIVBOnXcXrq5Y9eUsiJWV9bRlujyp59FRNJCYZBOHQ+e7Rs3mFJWRFO0jQ1VehJZRDJHYZBOBWPBgvvdUXTGMSMB+Mu6qkzVSkQkNWFgZheY2Toz22Bmd3RRfp2ZveO/Xjezk5PKNpnZu2b2tplVpKI+/VYwBIVl+z1rMHZYDieNLeSl93ZmsGIiMtT1OgzMLAjcD1wITAauNbPJnQ77EDjbOTcN+B7wYKfyOc656c658t7Wp98bNmG/lgHAeSeOZvlHe9nT0JqhSonIUJeKlsEsYINzbqNzLgo8AcxLPsA597pzbq+/+VdgXAo+d2AaNn6/MQOAuZNH4xz8ea26ikQkM1IRBmVA8rfbVn9fd74A/Clp2wEvmtlyM7upu5PM7CYzqzCzil27dvWqwhk1bALUV0JbrGPXSWMLKS3KVleRiGRMKsLAutjX5X2SZjYHLwxuT9p9pnNuJl4305fN7KyuznXOPeicK3fOlRcXF/e2zplTNB5couNZA/B++ey8E0ezbP1uWmJtGayciAxVqQiDrcD4pO1xQGXng8xsGvBLYJ5zbk/7fudcpb+sAhbidTsNXp2msm533uTRNMfaeP2D3RmolIgMdakIgzeBY81skpllAfOBRckHmNkE4Gng75xz7yftzzOzgvZ14HxgVQrq1H918awBwGlHjyA/EuKl9zRuICLp1+swcM7FgVuBF4A1wJPOudVmdrOZ3ewf9h1gJPCzTreQjgZeM7OVwBvAc86553tbp36tcBxgB7QMIqEgZx9XzJI1O0noaWQRSbNQKt7EObcYWNxp38+T1r8IfLGL8zYCJ3feP6iFsqCgdL9nDdqdN7mE597dzttba5g5YXgGKiciQ5WeQM6ELp41ADjnhNFEQgEWrtjWxUkiIn1HYZAJw8bvN1ldu6KcMJ86aQyLVlbqriIRSSuFQSYMmwB1ldAWP6Do6vJx1DbHWLJGzxyISPooDDJh1HGQiEPV6gOKzjhmFKVF2fy+YmsGKiYiQ5XCIBOOORcweP+FA4qCAePKmeNYtn4XO2pb0l83ERmSFAaZkF8M48ph3eIui686ZRwJB39YodaBiKSHwiBTjrsAKt+Cuu0HFE0clcesiSN4avlWnNMzByLS9xQGmXL8hd5y/YFdRQBXlY/jw92NLN+8t8tyEZFUUhhkSslk766idV0/cH3x1FIKIiF+898H3oIqIpJqCoNMMYPjLoSNf4Fo0wHFeZEQn/n4eBa/u53ttc3pr5+IDCkKg0w6/gKIN8OHr3ZZvOCMiSSc47dqHYhIH1MYZNJRn4CsAlj3py6Lx4/IZe7k0fzHGx/RHNUTySLSdxQGmRTKgo+d4z1vkEh0ecjnz5xETVOMhW9pviIR6TsKg0w7/mJo2AEf/qXL4lmTRnDS2EIe+q8PdZupiPQZhUGmnXQZFE2AJXd12TowMz5/5iQ2VDWwdL1+BU1E+obCINNCETjn27B9Jax+ustDLjm5lNGFEe5/ZYNaByLSJxQG/cHUz8DoqfDydyHeekBxJBTklrOP4Y0Pq/nvD/Z08QYiIr2jMOgPAgGYe5f3GwcVD3V5yPxZExhTmM29L72v1oGIpFxKwsDMLjCzdWa2wczu6KLczOwnfvk7ZjbzUM8dMo45FyadDa/eAy21BxRnh4N8ec4xVGzey2sbNHYgIqnV6zAwsyBwP3AhMBm41swmdzrsQuBY/3UT8MBhnDs0mMHc70JLDTz3j9DFX/+f+fh4xhZlc59aByKSYqloGcwCNjjnNjrnosATwLxOx8wDfuM8fwWGmVnpIZ47dIydDrPvhHd/D2/99oDiSCjIl8/5GCs+quHV93elv34iMmilIgzKgC1J21v9fYdyzKGcC4CZ3WRmFWZWsWvXIP4i/OTXvO6ixd+Ene8dUHz1KeMZNzyHu/+0lnhb1w+qiYgcrlSEgXWxr3MfRnfHHMq53k7nHnTOlTvnyouLiw+zigNIIAhX/AIiBfD7BRBt3K84KxTgzotOZO2Oeh5/c0vX7yEicphSEQZbgfFJ2+OAykM85lDOHXoKRsOVv4Dd78OzXz5g/ODCKWM47egR/OjFddQ0RTNUSREZTFIRBm8Cx5rZJDPLAuYDizodswj4nH9X0WlArXNu+yGeOzQdPRvOuwtWL4RXf7BfkZlx16UnUdcc496X3s9I9URkcOl1GDjn4sCtwAvAGuBJ59xqM7vZzG72D1sMbAQ2AL8AvtTTub2t06Bx5j/AydfCX77vhUKSE8YU8nenHcWjf93Mmu11GaqgiAwWNhBvUSwvL3cVFRWZrkZ6xFvh4Utgx7tw42Io63hEg5qmKHP+7S8cU5zPk//zdAKBroZgREQ8ZrbcOVfeVZmeQO7vQhGY/xjkjYLHr4XarR1Fw3Kz+F8XT6Zi815+/fqmzNVRRAY8hcFAkF8Cn30SYk3wH9dAa31H0RUzyzj3hBLueX4tG3c1ZLCSIjKQKQwGitGT4eqHoWoN/P5GaIsD3mDyv14xlexwkG889Q5tiYHX7ScimacwGEg+di5cci9seAn+9M2OW05HF2Zz16WTWb55L796bWOGKykiA5HCYKA5ZQGc+VWo+BW8dm/H7suml3H+5NH88IV1rNxSk6naicgApTAYiM79Z+83EF7+Lrz9OOB1F/3gymmUFGTzpcdWUNsUy3AlRWQgURgMRIEAzLsfJp0Fi26FDS8DMDwvi59+dgZV9S384+/fJqHxAxE5RAqDgSqUBdc8CsUnwO+uh4/+CsCMCcO586ITWbKmigeXafxARA6NwmAgyy6C65+GglJ47GqofBuABWdM5OKppdzz/FpeWVuV2TqKyICgMBjoCkbDDYsgexj89nKoWoOZcc9V0zixtJBb/2MF71VqugoR6ZnCYDAoGgc3PAvBLHjk07DjXfIiIX51w8cpyA7zhUfepKquJdO1FJF+TGEwWIw4Ghb80QuEhy+GLW8ypiibXy0op7Y5xhceqaC+RXcYiUjXFAaDyahj4fPPQ84I+M082PgqJ40t4qefncF72+v4wsMVNEfbMl1LEemHFAaDzbAJXiAMPwoevRKWP8I5J4zmvmum8+bmam76bQWtcQWCiOxPYTAYFYzxprue9En4z6/Ac1/n0inF/ODKaSxbv5svP/YW0bh+P1lE9lEYDFY5w+Gzv4fTb4U3fwG/uYzPfAy+O+8klqzZyf/4jbqMRGQfhcFgFgzBp/4PXP7/oPIt+NnpfC6ylB9cMYVl63fxd7/6G7XNGlQWEYXB0HDyfPjS6zB2Oiz6e65ZdxsPX1LIyq01XPP//psdtbrtVGSo61UYmNkIM3vJzNb7y+FdHDPezF4xszVmttrM/iGp7C4z22Zmb/uvi3pTH+nB8InwuUVw4Q9hyxucteRSXjvuSRLVm7j0p6/xtmY6FRnSevUbyGZ2D1DtnLvbzO4Ahjvnbu90TClQ6pxbYWYFwHLgMufce2Z2F9DgnPu3w/ncIfUbyH2hcQ/8133wxi9wiTivWjmPRs/m4suv4/KZR2W6diLSR/ryN5DnAY/4648Al3U+wDm33Tm3wl+vB9YAZb38XOmNvJFw/v+Gr7yFnXozZ2Wt55fBH3Das7P52/9dQHTVImipzXQtRSSNetsyqHHODUva3uucO6CrKKl8IrAUmOKcq/NbBguAOqAC+Efn3N5uzr0JuAlgwoQJp2zevPmI6y2dxKPE1y7mgyW/omzvG+RbC86CWMmJMGYalE6DUcd5TzkXjfcGpkVkwOmpZXDQMDCzJcCYLoq+DTxyqGFgZvnAq8D/cc497e8bDewGHPA9vO6kzx/sgtRN1HeWrdnGb596ipNjb/HpUTsZ37oBa0ya+TQQgsKxUFjmLQtKIX+092xDwRhvu2AMRAoydxEi0qWewuCgf+I5587r4Y13mlmpc267PzbQ5XzJZhYG/gA81h4E/nvvTDrmF8AfD1Yf6VufPLGME2+7mTuffpcfvreTE8YUcM/80UzL3g3VG2Hvh1C7FeoqYdsKqN8B8eYD3yirwAuLojIvOIYd5T0dPfwoGHEM5I0Cs/RfoIh0qbft/UXADcDd/vLZzgeYmQG/AtY45+7tVFbqnNvub14OrOplfSQFRuVHePBz5bywegd3LVrNvEc2cNXMcdw29yrGDsvZ/2DnoLUO6ndC/XZo8Jd1lVC3zVvuXO3tTxYphJHHeD/OU3IilEz2uqQKRqfvQkWkQ2/HDEYCTwITgI+Aq51z1WY2Fvilc+4iM/sEsAx4F2ifA+FO59xiM/stMB2vm2gT8D+TwqFb6iZKn4bWOP++5H0eeX0zGNxw+lHcMvtjjMjLOrw3ijVDzRbYuwmqP4A9H8Ce9bBrnRce7QpKoXQ6jJ8FEz/hrYcO87NEpEu9GjPojxQG6bd1bxP3vbSep9/aSiQU4OpTxvOFT0xi4qi83r95UzVUrYHtK2H721730571XlkoByaeCcddAMdf6P12g4gcEYWBpMz6nfX8YtlGnnmrklgiwbknlHDVKeM554QSskIpfKC9YRd89Dps+i9Y/6I3VgEwdgZMuwamXAn5Jan7PJEhQGEgKVdV38JvXt/MkxVbqKpvZXhumIunlTJ38hhOO3oEkVAwdR/mHOxeD+sWw+qnvRaEBeGYc+CUBXDcpyAYTt3niQxSCgPpM/G2BK9t2M1Ty7fy8poqmmNt5GUFOfNjo5g1aQSzJo1gcmkhoWAKWw1Va+Gd38HKJ6C+EvLHwIzrvWAYNj51nyMyyCgMJC1aYm389wd7eGnNTpat38WWau+W05xwkOPGFHDC6AKOG1PA+OE5lA3PoWxYDkU5YexIbzFti8OGl6Di194S4NhPwce/4LUaAilsnYgMAgoDyYgdtS28samatz7ay7od9azdUU91Y3S/YwIGRTlhinLCREJBQkHraEU452hLeK9YW4J4wvnnGGYQCQUpzA5RmBPm6FA15zYtZlrVs2RHq4kXjCNQvoDAjOuhsDTt1y7SHykMpF9wzlHdGGXr3ma21TRTWdNMTVOM2mbv1RpvI97miLZ5dyAHA0bQjGDACAcDhIKGAQkHCedoibVR1xKnrjnGnsYouxtaCbk4cwMVfDb4Mp8IrqaNAO/lzWJj2WW4Yz/FxDEjOLo4j8JsjTHI0KMwkCEhGk+ws66FLdVNbNrTRM3WNUz86Glm1b3IKFfNXpfPn9pm8Z+J0/kg52SOKi7gqJF5TByZy/gRuYwbnsv4ETkU50eOvOtKpB9TGMjQlmgjtv7PtFQ8Ss7GFwm1NVEXGsHfsk7juegM/tRwLK3se7AtEgowdlgOpUXZjCnKZkyhtywpyKa4IEJJQYRR+RFysjQmMWA4B21RiLd6y45XLGk97i0TcUjEvO329USbt94W8/fFwSX2rSfikPC3nX9soi3pmDZ/f/sykbTdaT15337b/vqn/hXGf/yI/jH0am4ikQEvECR8/FzCx8+FaBOsf5HC1U8zd8PLzI0v5r6CPBrHnkHlyFNZnVPOmuhoKmtb2FbTzF8/2ENVfWvHeEWynHCQEXlZjMzPYlhuFsNzwwzPzaLQHwMpyglTkB2iIDtEYba3nh8JkRcJEQkF1ProinPQWu9NcdJSCy113nr7vtYGiDZAtHHfK9bkv5r3LeMtEGvxv/xbve10sYB363Mg6E3saEEIJO2zgL8/4G8H918PBPZ/j/b1YBhCEa+8L6qtloEMWbEW2LTMe37hgz97U2WAd6vqhNPgqDNg3MdJFE9mdyvsqm/teO1uiFLd2Mqehih7GqPUNEXZ2xRjb1OU+pb4QT86FDBys4LkRULkZAXJzQqSGw6RnRUkJxwgOxwkOxQkOxwgEg4SCQXICgaIhL1lVihIVihAOGhkBQOE/DGVcMBfBo1gIEAoYISC+8ZeggEj0NW6GYGANzjfPkDfvt+Mww8u57wv6qY9/qsamqv3X2/em/SqgZYaLwBc4mDvDuFcyMrzXuE8yMqFcI73xHo421uGIv6+CAQj3rJjPQuC/isQSlr3v3QDYW+q9kDYKw+E/O1Or/Yv9kAo6cu//4a8uolEDkX1h7DxFdj8Onz0V6jd4u0PhGH0Sd7vOpRM9l7FJ3hPQHfxP35bwlHf4g2K17fEqWuJUdccp7E1TmM0Tn1LnKZonMbWNhpb4zTF2miOttEUjdMcS9ASbaM51kZrvI2WWILWeBut8QSZ/F8126IUB5oYEahnVKCBkVbPCGtgRKCe4TQwwuoYTh1Frp4iV0dhopYsYl2+l8OIhQuJZw8nkT0MyxlBMG8YWXkjCOQMg+wiaF9GCiDSviyASL4XBLpt+IgoDESORM0W2LYcKt+CyhWwY5X3F227rAIYebT/oz/joGiCN2V3wRivdZFfkrIno51zxBOO1niCaPKrLUE8kSAW9+7Caks44m0JYglHWyJBvM07ry3hcG1RrLWBQLSWULSeYKyOcGsdoWgtoXg9kWgtWbFaIrE6smK1ZMdqyI7Xkh2rJZzovpulKVBAQ7CQ+kARdYFC6qyAvRRR7fLZ7QrYFc9jRzyP7dFcdifyqSOPRDc/sliQHWJEXpbX/da+zI8wMi+LUfkRRubvW47IzUrtw4xDgMJAJBWcg4YqqHoPdr/vzbxa/YHXoqjd6vVNdxYpgtzhkDMCsgu9qbsjhfu6NcK5fj9weF83hbX3GRsdzYHkgciEP+gZj3qfGWvxflMi1gKxRq/PPNoE0fp9feyt9QfvNw+EIGe498oeBrkjvHrnjvD2tW/njYLckfvKDjHwnPPCrL4lTn1LjLqWODVNUWqbY9T4XWw1Td5twtWNrVQ3xtjT0Ep1Y7TLMRvwnlFpD432V8f4TV4Ww9rHb3K9ZWF2mNys4JAdr9EAskgqmHm/t1AwGo6Zs39ZIgFNu72upYYq70d/Gqr27ydvrfP2t9TtG+jsKkAORyDs94tne33lHf3nuVA4zutWycrbv7slu8h/FXpf+u3dMln5fdrfbWbeWEg4SHFB5JDPc85R1xxnlx8Mexpa2d0Ypbohyh5/3Ka6McrmPU2s+KiGmqbuwwO8sZD2wfyC7DD5EW/sJi8SIi8rSG5WyBvDyQqSkxUiJxwkJyvgjeFkJY3lhIJEwgEioX3rWUHvFQgMvLBRGIikQiDgdQsd7kyqibb9b3NMvq2wg3lf0oHwvkHKUMRrSQyBv3DNzPvLPvfQWyANrfGOBxprmmLUNEepa24fv4nR0BqnoSVOvb+sbozyUXWTN4YTbaMp2kZbD4FyMO0D+2F/4D8cDHQM+If9Af+soBHqGPD3BvvDoQDhgPcUfvvgf8i/ESAYNEIB48qZ4zi6OP+I69YdhYFIJgWCEMjx/rqXlDAzCrLDFGSHOdJpC53zxmBaogmaY97gfkssQUu8jZaYN6DfGvMG+KPxfYP8rUljOTF/GY0niLV5U6p4696YTizujfc0x1zHuE+sfZzHn34l7k/FkkjsG/s5ddJIhYGISDqYmdf1EwpSxNCYukRD8SIi0rswMLMRZvaSma33l8O7OW6Tmb1rZm+bWcXhni8iIn2rty2DO4CXnXPHAi/7292Z45yb3um2psM5X0RE+khvw2Ae8Ii//ghwWZrPFxGRFOhtGIx2zm0H8Jfd3VfngBfNbLmZ3XQE52NmN5lZhZlV7Nq1q5fVFhGRZAe9m8jMlgBjuij69mF8zpnOuUozKwFeMrO1zrmlh3E+zrkHgQfBewL5cM4VEZGeHTQMnHPndVdmZjvNrNQ5t93MSoGqbt6j0l9WmdlCYBawFDik80VEpG/1tptoEXCDv34D8GznA8wsz8wK2teB84FVh3q+iIj0vd6Gwd3AXDNbD8z1tzGzsWa22D9mNPCama0E3gCec84939P5IiKSXr16Atk5twc4t4v9lcBF/vpG4OTDOV9ERNJLTyCLiIjCQEREFAYiIoLCQEREUBiIiAgKAxERQWEgIiIoDEREBIWBiIigMBARERQGIiKCwkBERFAYiIgICgMREUFhICIiKAxERASFgYiIoDAQERF6GQZmNsLMXjKz9f5yeBfHHG9mbye96szsq37ZXWa2Lansot7UR0REjkxvWwZ3AC87544FXva39+OcW+ecm+6cmw6cAjQBC5MOua+93Dm3uJf1ERGRI9DbMJgHPOKvPwJcdpDjzwU+cM5t7uXniohICvU2DEY757YD+MuSgxw/H3i8075bzewdM3uoq24mERHpewcNAzNbYmarunjNO5wPMrMs4FLg90m7HwCOAaYD24Ef9XD+TWZWYWYVu3btOpyPFhGRgwgd7ADn3HndlZnZTjMrdc5tN7NSoKqHt7oQWOGc25n03h3rZvYL4I891ONB4EGA8vJyd7B6i4jIoettN9Ei4AZ//Qbg2R6OvZZOXUR+gLS7HFjVy/qIiMgR6G0Y3A3MNbP1wFx/GzMba2YddwaZWa5f/nSn8+8xs3fN7B1gDnBbL+sjIiJH4KDdRD1xzu3Bu0Oo8/5K4KKk7SZgZBfH/V1vPl9ERFJDTyCLiIjCQEREFAYiIoLCQEREUBiIiAgKAxERQWEgIiIoDEREBIWBiIigMBARERQGIiKCwkBERFAYiIgICgMREUFhICIiKAxERASFgYiIoDAQEREUBiIiQi/DwMyuNrPVZpYws/IejrvAzNaZ2QYzuyNp/wgze8nM1vvL4b2pj4iIHJnetgxWAVcAS7s7wMyCwP3AhcBk4Fozm+wX3wG87Jw7FnjZ3xYRkTTrVRg459Y459Yd5LBZwAbn3EbnXBR4Apjnl80DHvHXHwEu6019RETkyITS8BllwJak7a3Aqf76aOfcdgDn3HYzK+nuTczsJuAmf7PBzA4WQt0ZBew+wnMHMl330DNUr13X3b2juis4aBiY2RJgTBdF33bOPXuw8wHrYp87hPP2P8G5B4EHD/e8AypjVuGc63Z8Y7DSdQ89Q/Xadd1H5qBh4Jw770jf3LcVGJ+0PQ6o9Nd3mlmp3yooBap6+VkiInIE0nFr6ZvAsWY2ycyygPnAIr9sEXCDv34DcCgtDRERSbHe3lp6uZltBU4HnjOzF/z9Y81sMYBzLg7cCrwArAGedM6t9t/ibmCuma0H5vrbfa3XXU0DlK576Bmq167rPgLm3GF334uIyCCjJ5BFRERhICIiQywMupsWY7Axs/Fm9oqZrfGnC/kHf/+gn/7DzIJm9paZ/dHfHvTXDGBmw8zsKTNb6/97P30oXLuZ3eb/N77KzB43s+zBeN1m9pCZVZnZqqR93V6nmX3L/55bZ2afOpTPGDJhcJBpMQabOPCPzrkTgdOAL/vXOhSm//gHvBsV2g2Fawb4d+B559wJwMl4/wwG9bWbWRnwFaDcOTcFCOLdrTgYr/th4IJO+7q8Tv//9fnASf45P/O//3o0ZMKAnqfFGFScc9udcyv89Xq8L4YyBvn0H2Y2DrgY+GXS7kF9zQBmVgicBfwKwDkXdc7VMASuHe9ZqRwzCwG5eM8wDbrrds4tBao77e7uOucBTzjnWp1zHwIb8L7/ejSUwqCraTHKMlSXtDGzicAM4G90mv4D6Hb6jwHqx8A3gUTSvsF+zQBHA7uAX/tdZL80szwG+bU757YB/wZ8BGwHap1zLzLIrztJd9d5RN91QykMUjItxkBiZvnAH4CvOufqMl2fvmRmlwBVzrnlma5LBoSAmcADzrkZQCODo2ukR34f+TxgEjAWyDOz6zNbq37hiL7rhlIY9DQtxqBjZmG8IHjMOfe0v3unP+0Hg3D6jzOBS81sE14X4Dlm9iiD+5rbbQW2Ouf+5m8/hRcOg/3azwM+dM7tcs7FgKeBMxj8192uu+s8ou+6oRQGPU2LMaiYmeH1H69xzt2bVDRop/9wzn3LOTfOOTcR79/tn51z1zOIr7mdc24HsMXMjvd3nQu8x+C/9o+A08ws1/9v/ly88bHBft3turvORcB8M4uY2STgWOCNg76bc27IvICLgPeBD/BmXc14nfroOj+B1yx8B3jbf10EjMS762C9vxyR6br20fXPBv7orw+Va54OVPj/zp8Bhg+Fawf+BViL90NbvwUig/G6gcfxxkVieH/5f6Gn6wS+7X/PrQMuPJTP0HQUIiIypLqJRESkGwoDERFRGIiIiMJARERQGIiICAoDERFBYSAiIsD/B/wWCuObqf+SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "#test_losses_scaled = scaler.fit_transform(test_losses)\n",
    "\n",
    "\n",
    "\n",
    "Tr=np.array(train_losses)\n",
    "Te=np.array(valid_losses)\n",
    "\n",
    "Tr=np.reshape(Tr, (len(Tr),1))\n",
    "Te=np.reshape(Te, (len(Te),1))\n",
    "\n",
    "# fit on training data column\n",
    "scale = StandardScaler().fit(Tr)\n",
    "tain_losses = scale.transform(Tr)\n",
    "scale = StandardScaler().fit(Te)\n",
    "test_losses = scale.transform(Te)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "#test_losses_scaled = scaler.fit_transform(test_losses)\n",
    "plt.ylim([-1,1])\n",
    "\n",
    "plt.plot(tain_losses, label='Train loss')\n",
    "plt.plot(test_losses, label='valid loss')\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
